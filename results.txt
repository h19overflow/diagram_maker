and as such is a hybrid human-generated dataset. The underlying documents are human-written and
come from C4 and Wikipedia while the instructions are generated visa LLMs. The dataset is extended
with additional structured corpora examples such as Stack Exchange and WikiHow and task examples
such as question answering, email writing, grammar error correction, story/poem generation, and text
summarization. The dataset contains 23,700 examples.
To partially alleviate this, we here perform some qualitative analysis, in two sections. First, in §6.1
10
instructions on 1600+ tasks. In EMNLP, 2022.
[61] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S.
Dhanasekaran, A. Arunkumar, D. Stap, et al. Super-naturalinstructions: Generalization via
declarative instructions on 1600+ nlp tasks. InProceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing, pages 5085–5109, 2022.
[62] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V . Le.
arXiv:1804.07461, 2018.
[59] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct:
Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560,
2022.
[60] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S.
Dhanasekaran, A. Naik, D. Stap, et al. Super-naturalinstructions:generalization via declarative
instructions on 1600+ tasks. In EMNLP, 2022.
fairness, accountability, and transparency, pages 610–623, 2021.
[7] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O’Brien, E. Hallahan, M. A. Khan,
S. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models
across training and scaling. arXiv preprint arXiv:2304.01373, 2023.
[8] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,
J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models.
have discussed and our interpretation of them.
6.1 Qualitative Analysis of Example Generations
To find examples, we first go through data generated for the Vicuna benchmark and the OpenAssistant
benchmark, and look for patterns in the answers Guanaco generates. When we notice a pattern we
attempt to setup a question or prompt that will induce the pattern even though it is the incorrect
solution, e.g., if we observe that the model tends to give long-winded answers we prompt the model
Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn.
Res., 21(1), jan 2020. ISSN 1532-4435.
[50] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler,
T. L. Scao, A. Raja, et al. Multitask prompted training enables zero-shot task generalization.
arXiv preprint arXiv:2110.08207, 2021.
[51] M. Sap, R. LeBras, D. Fried, and Y . Choi. Neural theory-of-mind? on the limits of social
[23] J. Henderson, S. Ruder, et al. Compacter: Efficient low-rank hypercomplex adapter layers. In
Advances in Neural Information Processing Systems, 2021.
[24] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Mea-
suring massive multitask language understanding. In International Conference on Learning
Representations, 2020.
[25] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi. The curious case of neural text
Model Elo Rank Elo Rank Elo Rank
GPT-4 1176 1 1348 1 1294 1 1
Guanaco-65B 1023 2 1022 2 1008 3 2
Guanaco-33B 1009 4 992 3 1002 4 4
ChatGPT-3.5 Turbo 916 7 966 5 1015 2 5
Vicuna-13B 984 5 974 4 936 5 5
Guanaco-13B 975 6 913 6 885 6 6
Guanaco-7B 1010 3 879 8 860 7 7
Bard 909 8 902 7 - - 8
we show some examples that we believe are representative of some observed patterns in the text
generated by our 65b Guanaco model. Second, §6.2 we detail considerations about the results we
arXiv:1907.11692, 2019.
[39] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y . Tay, D. Zhou, Q. V . Le, B. Zoph, J. Wei,
et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv
preprint arXiv:2301.13688, 2023.
[40] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context.
arXiv preprint arXiv:2110.15943, 2021.
[41] A. Nematzadeh, K. Burns, E. Grant, A. Gopnik, and T. Griffiths. Evaluating theory of mind in
