A Comprehensive Survey of Small Language Models in the Era of Large
Language Models: Techniques, Enhancements, Applications, Collaboration with
LLMs, and Trustworthiness
FALI WANG, ZHIWEI ZHANG, XIANREN ZHANG, and ZONGYU WU, The Pennsylvania State
University, USA
TZUHAO MO, University of Pennsylvania, USA
QIUHAO LU, WANJING WANG, and RUI LI,UTHealth Houston, USA
JUNJIE XU, The Pennsylvania State University, USA
XIANFENG TANG and QI HE, Amazon, USA
YAO MA,Rensselaer Polytechnic Institute, USA
YAO MA,Rensselaer Polytechnic Institute, USA
MING HUANG, UTHealth Houston, USA
SUHANG WANG‚àó, The Pennsylvania State University, USA
Large language models (LLMs) have demonstrated emergent abilities in text generation, question answering, and reasoning, facilitating
various tasks and domains. Despite their proficiency in various tasks, LLMs like PaLM 540B and Llama-3.1 405B face limitations due
to large parameter sizes and computational demands, often requiring cloud API use which raises privacy concerns, limits real-time
applications on edge devices, and increases fine-tuning costs. Additionally, LLMs often underperform in specialized domains such as
healthcare and law due to insufficient domain-specific knowledge, necessitating specialized models. Therefore, Small Language Models
(SLMs) are increasingly favored for their low inference latency, cost-effectiveness, efficient development, and easy customization
and adaptability. These models are particularly well-suited for resource-limited environments and domain knowledge acquisition,
addressing LLMs‚Äô challenges and proving ideal for applications that require localized data handling for privacy, minimal inference
latency for efficiency, and domain knowledge acquisition through lightweight fine-tuning. The rising demand for SLMs has spurred
extensive research and development. However, a comprehensive survey investigating issues related to the definition, acquisition,
application, enhancement, and reliability of SLM remains lacking, prompting us to conduct a detailed survey on these topics. The
definition of SLMs varies widely, thus to standardize, we propose defining SLMs by their capability to perform specialized tasks and
suitability for resource-constrained settings, setting boundaries based on the minimal size for emergent abilities and the maximum size
sustainable under resource constraints. For other aspects, we provide a taxonomy of relevant models/methods and develop general
frameworks for each category to enhance and utilize SLMs effectively. We have compiled the collected SLM models and related
methods on GitHub: https://github.com/FairyFali/SLMs-Survey.
‚àóCorresponding author.
Authors‚Äô addresses: Fali Wang, fqw5095@psu.edu; Zhiwei Zhang; Xianren Zhang; Zongyu Wu, The Pennsylvania State University, University Park,
USA; TzuHao Mo, University of Pennsylvania, Philadelphia, USA; Qiuhao Lu; Wanjing Wang; Rui Li, UTHealth Houston, Houston, USA; Junjie Xu, The
Pennsylvania State University, University Park, USA; Xianfeng Tang; Qi He, Amazon, Palo Alto, USA; Yao Ma, Rensselaer Polytechnic Institute, Troy,
USA; Ming Huang, UTHealth Houston, Houston, USA; Suhang Wang, The Pennsylvania State University, University Park, USA, szw494@psu.edu.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components
of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on
servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
¬© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
Manuscript submitted to ACM
Manuscript submitted to ACM 1
arXiv:2411.03350v2  [cs.CL]  28 Dec 2024
2 Fali Wang, et al.
CCS Concepts: ‚Ä¢ Computing methodologies ‚ÜíNatural language generation .
ACM Reference Format:
Fali Wang, Zhiwei Zhang, Xianren Zhang, Zongyu Wu, TzuHao Mo, Qiuhao Lu, Wanjing Wang, Rui Li, Junjie Xu, Xianfeng Tang,
Qi He, Yao Ma, Ming Huang, and Suhang Wang. 2018. A Comprehensive Survey of Small Language Models in the Era of Large
Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness. J. ACM 37, 4, Article 111
(August 2018), 78 pages. https://doi.org/XXXXXXX.XXXXXXX
1 INTRODUCTION
The evolution of neural language models (LMs) from BERT‚Äôs [83] pre-training and fine-tuning paradigm to T5‚Äôs [284]
pre-training plus prompting approach, and finally to GPT-3‚Äôs [37] pre-training plus in-context learning, has greatly
enhanced natural language processing (NLP). These advancements have broadened NLP‚Äôs application across various
fields, including language understanding [350], programming [255, 331], recommendation systems [369], information
retrieval [44, 152, 228, 317], mobile-device control [87], scientific discovery [311, 436], medical question answering
[34, 366], and legal question answering [11]. In particular, the recent emergence of proprietary commercial models
including ChatGPT, Bard, and Claude, and open-sourced models such as Llama [94, 338, 339] has led to rapid growth
in the development of large language models (LLMs). Even though neural networks consistently improve on various
tasks with longer training times, larger datasets, and increased model sizes‚Äîa phenomenon known as a neural scaling
law [166], these models unpredictably exhibit a sudden acquisition of versatile abilities, termed " emergent ability , "
once they reach a critical scale threshold, thereby supporting the "larger is better" trend. This ability is not present in
small-scale models. For instance, the latest Llama-3.1 model with 405 billion parameters performs better in dialogue,
logical reasoning, and programming compared to the smaller 7B counterpart [94].
Despite their prowess in complex tasks, LLMs‚Äô huge parameters and computational needs impose significant
limitations, hindering their adoption in many real-world applications. For example, the LLaMa 3.1 model with 405 billion
parameters [94], trained on 16K H100 GPUs for 54 days, requires about 202.5 GB of GPU memory using int4 precision
and has large inference latency. These issues present several challenges in specific contexts: (1) LLMs are generally
hosted in the cloud and used via cloud-based APIs due to the large GPU memory and computational cost. Users need to
upload their data to query LLMs, raising data leakage and privacy concerns, especially in high-stake scenarios such as
healthcare, finance, and e-commerce; (2) Driven by personal agents, on-device deployment is a critical requirement.
Several factors, including cloud costs, latency, and privacy concerns, hinder the on-device processing of cloud-based
LLMs, and direct deployment is impractical due to their high parameter and cache requirements, which often exceed the
capabilities of devices such as mobile phones; (3) Their large parameter count can cause inference delays from seconds
to minutes, unsuitable for real-time applications. For instance, Llama 2 7B takes approximately 84 seconds to process
100 tokens on benchmarks including HellaSwag, TruthfulQA, MMLU, and Arc_C when run on a smartphone equipped
with a Snapdragon 685 processor [336]; (4) To boost performance in specialized domains such as healthcare and law,
where generic LLMs underperform, LLMs are often fine-tuned. However, this process is computationally expensive due
to their large size. (5) Though general-purpose LLMs are powerful, many real-world applications require only specific
abilities and domain knowledge, deploying general-purpose LLMs would be a waste of resources and such LLMs often
cannot match the performance of models tailored for specific tasks [52, 125, 156, 275, 369].
Recently, small language models (SLMs) have shown great potential in alleviating these issues while achieving
performance comparable to LLMs for domain-specific problems [1, 26, 116, 143, 223, 274, 333, 336, 402, 439]. Owing to
fewer parameters, SLMs excel in efficiency, cost, flexibility, and customization. They provide significant computational
Manuscript submitted to ACM
A Survey of Small Language Models 3
Small Language Models
Introduction
(¬ß1)
Concepts in
Building LMs
(¬ß2)
Architecture (¬ß2.1)
Training techniques (¬ß2.2)
Obtain SLMs from LLMs (¬ß2.3)
Pruning (¬ß2.3.1)
Unstructured Pruning [76, 102, 201, 300, 321, 438, 442];
Structured Pruning [13, 19, 51, 124, 188, 196, 232, 243,
385, 410, 446]
Knowledge Distillation (¬ß2.3.2)
White-Box KD [6, 119, 157, 169, 173, 265, 434]; Black-
Box KD [48, 271, 360]
Quantization (¬ß2.3.3)
SqueezeLLM [170]; JSQ [122]; FrameQuant [4]; OneBit
[400]; BiLLM [147]; LQER [432]; I-LLM [144]; PV-Tuning
[239]; BitNet [357]; BitNet b1.58 [231]; PEQA [168];
QLoRA [81]
Enhancement
of SLMs (¬ß3)
Training Methods for SLMs from
Scratch (¬ß3.1)
 MobiLlama [336]; MobileLLMs [223]; MindLLMs [412]; Tang et al. [328]
Supervised Fine-Tuning (¬ß3.2)
Alpaca [329]; UltraChat [86]; WizardLM [392]; SlimOrca [203]; ShareGPT [356]; Capybara
[72]; Deita [216]; MetaMathQA [420]; MobileBERT [322]; StableLM [26, 340]; RLHF [264];
DPO [283]
Data quality in Knowledge Distil-
DPO [283]
Data quality in Knowledge Distil-
lation (¬ß3.3)
TinyStory [96]; Self-Amplify [29]; AS-ES Learning [384]; Huang et al. [145]; Bhan et al. [29];
Tian et al. [337]
Distillation techniques for en-
hancing SLM (¬ß3.4)
 GKD [6]; DistiLLM [173]; Adapt-and-Distill [414]; Bit-level Inference Scaling Laws [82]
Performance improvement
through quantization (¬ß3.5)
Bit-level Inference Scaling Laws [82]; BiLLM [147]; LLM.int8() [80]; PB-LLM [299];
OneBit [400]; BitNet [357]; LLM-QAT [221]
OneBit [400]; BitNet [357]; LLM-QAT [221]
Techniques in LLMs contributing
SLMs (¬ß3.6)
 RAG for SLMs [215, 415]; MoE for SLMs [172, 194]
Applications of
SLMs (¬ß4)
Task-specific SLM applications
(¬ß4.1)
SLM applications in QA (¬ß4.1.1)
Phi-series [1, 120]; Orca 2 [247]; BioGPT-Large [125];
Stable Beluga 7B [238]; Phogat et al. [275]; Hartill et al.
[129]; Gichamba et al. [112]; Jeong et al. [156]; Rationale
Ranking [128]
SLM applications in Coding
(¬ß4.1.2)
DeepSeek-Coder [121]; Phi-1 [120]; Phi-3.5-mini [1];
CodeGemma [331]; CodeLlama [292]
SLM applications in Recom-
mender Systems (¬ß4.1.3)
PromptRec [382]; SLIM [369]; BiLLP [309]; RecLoRA
[455]; Wu et al. [379]
SLM applications in Web Search
(¬ß4.1.4)
Content encoder [44, 152, 228]; H-ERNIE [63]; Zou et al.
[462]; Peng et al. [272]; CoCondenser [109]; Implicit
Interaction (ùêº3) [89]; InPars [36]; rewrite-retrieve-read
[233]
SLM applications in Mobile-
device (¬ß4.1.5)
Octopus [52]; MobileAgent [87]; Carreira et al. [42];
AutoDroid [376]; Qin et al. [278]; Zhu et al. [458]
Techniques during SLM Deploy-
ment (¬ß4.2)
Memory efficiency (¬ß4.2.1)
MoE-I2 [404]; MobileAIBench [253]; MobileLLM [223];
EdgeMoE [416]; GEAR [165]; HETLORA [62]; MobiL-
lama [336]
Computing efficiency (¬ß4.2.2)
COST-EFF [305]; MobileLLM [223]; MobiLlama [336];
Merino [451]; EdgeMoE [416]; Nawrot et al. [256];
LLMCadLLM-Cad[394]; LinguaLinked [447]
Models (¬ß5)
Generic-domain SLMs (¬ß5.1)
Models (¬ß5)
Generic-domain SLMs (¬ß5.1)
PhoneLM [417]; Llama 3.2; Qwen [23, 402]; Gemma [332, 333]; StableLM [26, 340]; TinyL-
lama [439]; OLMo [116]; H2O-Danube3 [274]; Fox-1 [334]; MiniCPM [143]; Phi [1, 1, 120,
155, 200]; BLOOM and BLOOMZ [181]; Galactica [330]; OPT [440]; XGLM [209]; Megatron-
gpt2 [310]; MINITRON [252]; LaMini-LM [171]; FlanT5 [64] ...
Specific-domain SLMs (¬ß5.2)
 Hippocrates [3]; BioMedLM [34]; MentaLLaMA [406]; ChemLLM [436]; SciGLM [435]; As-
troLLaMA [258]; MindLLM [412]
SLMs for
troLLaMA [258]; MindLLM [412]
SLMs for
LLMs (¬ß6)
SLM for reliable LLM generation
(¬ß6.1)
POLAR [450]; SAPLMA [21]; SuperICL [393]; SuperContext [408]; Self-RAG [18]; SKR [365];
SlimPLM [325]; CRAG [401]; Feng et al. [101]; Liu et al. [219]; HaluAgent [58]; Toolformer
[295]
SLM for extracting LLM prompts
(¬ß6.2)
 Prompt Stealing Attacks [297]; Output2prompt [433]; Model Purifying [197]; [443]
SLM for fine-tuning LLMs (¬ß6.3)
SLM for fine-tuning LLMs (¬ß6.3)
Weak-to-Strong Search [453]; Emulated Fine-tuning [246]; CROSSLM [79]; Swayamdipta et al.
[323]; Mekala et al. [242]; Proxy-tuning [212]
SLM for LLM applications (¬ß6.4)
SLCoLM [327]; HEF [413]; BLADE [189]; Contrastive Decoding [198]; ‚ÄúTrain-Guide-Predict‚Äù
[327]; Sennrich et al. [296]
SLM for LLM safety (¬ß6.5)
 Llama Guard [153]; Kwon et al. [177]
SLM for LLM evaluation (¬ß6.6)
SLIDE [449]; Semantic uncertainty [175]; Selfcheckgpt [240]; Proxylm [16]; Factscore [151]
Synergy be-
tween SLMs
and LLMs (¬ß7)
Cloud-edge synergy (¬ß7.1)
 CoGenesis [437]; Hao et al. [127]; Xu et al. [397]; LLM-to-SLM [28]; Synergy of Thoughts
[298]; CROSSLM [79]
Task-centric synergy (¬ß7.2)
 ùõº-UMi [307]; SynCID [204]; Filter-then-rerank Framework [234]; LLMCad [394]
Trustworthiness
in SLMs (¬ß8)
HELM [205]; Do-Not-Answer [364]; PromptRobust [456]; GLUE-X [407]; HaluEval [191]; PrivLM-Bench [190]; FFT [71]; ROB-
BIE [98]; TrustLLM [320]; RAmBLA [35]; JailbreakBench [45]; Xie et al. [390]; OR-Bench [70]; SORRY-Bench [387]; BeHonest [59];
Hong et al. [138]; Nakka et al. [254]; RUPBench [370]
Future Direc-
tions (¬ß9)
Fig. 1. Overview of Small Language Models.
Manuscript submitted to ACM
4 Fali Wang, et al.
125M 1.3B 6.7B 30B 120B
Model Size
0
2000
4000
6000Downloads
2306
6355
1119 1245 771
GALACTICA Models
1.6B 3B 12B
Model Size
0
2000
4000
6000
1860
5270
341
StableLM-2 Models
0.5B1.5B 3B 7B 14B 32B 72B
Model Size
0
1
2
3
1e6
157K
3M
187K186K90K 66K127K
Qwen Models
3.8B 7B 14B
Model Size
0.0
0.5
1.0
1.5
2.0
1e6
2M
44K 75K
Phi-3 Models
Fig. 2. Download Statistics Last Month in Huggingface for LLMs with Various Model Sizes, obtained on October 7, 2024.
savings in pre-training and inference with reduced memory and storage needs, which is vital for applications requiring
efficient resource use. These small models are especially effective in resource-limited settings, performing well on
low-power devices such as edge devices. Besides, SLMs improve on-device processing by enhancing privacy, security,
response times, and personalization. This supports advanced personal assistants and cloud-independent applications,
boosting energy efficiency and reducing carbon emissions. For example, the Llama 3.2 models (1B & 3B) demonstrate
that local processing enables immediate execution of prompts and responses [7]. This approach protects privacy by
keeping sensitive data such as patient health information (PHI), business data, personal messages, and calendar details
local, enhancing confidentiality. It also allows for precise control over which queries are processed on-device versus
those requiring cloud-based models. Therefore, small language models are gaining increasing attention as alternatives
to LLMs, as indicated in Figure 2, which shows that SLMs are downloaded more frequently than larger models in the
Hugging Face community, and Figure 3, which illustrates the growing popularity of SLM releases over time.
Typically, LMs that exhibit emergent abilities are classified as LLMs. However, the categorization of SLMs remains
unclear. Studies vary in their contexts: some define SLMs as models with fewer than one billion parameters [223], while
others consider the term ‚Äúsmall language model‚Äù relative to the larger counterparts [183, 327, 369], with no consensus on
a unified definition in the current landscape of LLMs. Research suggests SLMs for mobile devices, typically possessing
around 6GB of memory, consist of sub-billion parameter models [ 223], whereas others classify models with up to
10 billion parameters as small, noting their lack of emergent abilities [105]. Given their use in resource-constrained
environments and for specific tasks, we propose a generalized definition: Given specific tasks and resource constraints,
we define SLMs as falling within a range where the lower bound is the minimum size at which the model exhibits emergent
abilities for a specialized task, and the upper bound is the largest size manageable within limited resource conditions. This
definition integrates various perspectives and addresses factors related to mobile computing and capability thresholds.
Due to the growing demand for SLMs, extensive literature has emerged on various aspects of SLMs. For example,
several resource-efficient techniques [398] and training methods optimized for SLMs, such as quantization-aware training
[221, 357, 400] and selective architectural component choices [223, 280, 336], aim to enhance performance in specific
applications [36, 52, 275, 292, 382]. These methods have led to the development of numerous open-source, general-
purpose, and domain-specific SLMs [3, 26, 34, 333, 402, 435]. Beyond their inherent capabilities, SLMs can enhance LLMs
by serving as modules or effective proxies [246, 297, 383, 399, 413, 450]. Furthermore, the complementary advantages of
SLMs and LLMs can be leveraged collectively to better complete tasks [79, 204, 234, 307, 437]. Despite the commendable
performance of SLMs, it is crucial not to overlook their credibility issues, such as the risks of adversarial attacks,
producing hallucinations, and privacy breaches [88, 95, 138, 176, 176, 222, 248, 254, 273, 351, 354, 370, 371, 425, 445].
However, currently, there is no comprehensive survey thoroughly exploring these works on SLMs in the era of LLMs.
Therefore, this paper offers the first comprehensive survey that analyzes various aspects of SLMs in the LLM era and
their future directions. The overview structure of our paper is shown in Figure 1. To summarize, our major contributions
are:
Manuscript submitted to ACM
A Survey of Small Language Models 5
< 2022
2024
2025+
Phi-1.5
Phi-2 Phi-3
TinyLlama
MobileLLM
Bloom
Cerebras-GPT
Pythia
OPT
Galactica
GPT-Neo
Megatron-gpt2
Orca 1
Orca 2
XGLM
Lamini-LM
StableLM
Qwen 1
Qwen 2.5
Llama 3.2
OpenELM
OLMo
Minitron
Qwen 1.5
BioMedLM
AdaLM
T5
Flan-T5
2023
1-4
2022
5-8
Domain-specific SLMs
Dolly v2
9-12
AstroLLaMA
MindLLM
1-2
MobiLlama
3-4
Gemma
MiniCPM
Phi-3.5
ChemLLM
SciGLM
Llemma
Hippocrates
5-6
Qwen 2
Rho-1
Me-LLaMA
Rene 
7-8
Gemma 2
H2O-Danube3
BioMistral
9-10
Phi-1
StableLM 2
H2O-Danube3
BioMistral
9-10
Phi-1
StableLM 2
OceanGPT
MobileBERT
Developed by Diverse Groups
Fox-1 
11-12
PhoneLM
SmolLM
CT-LLM
Fig. 3. A timeline of existing small language models.
‚Ä¢In Section 3, we examine various techniques for improving the performance of SLMs, including training from scratch,
fine-tuning, knowledge distillation, quantization, and leveraging LLM-enhancing technologies to optimize SLMs.
‚Ä¢In Section 4, we discuss the tasks that SLMs can enhance and the deployment strategies that enable models to fit
within the resource constraints of edge devices while maintaining acceptable inference speed.
‚Ä¢In Section 5, we collect SLMs with fewer than 7 billion parameters across both general-purpose and domain-specific
applications, reviewing common architectural choices, training techniques, and datasets, and providing a comparative
summary of performance across different model sizes. Recent SLMs are listed.
‚Ä¢In Section 6, we explore how SLMs can address key challenges faced by LLMs, such as high inference latency,
labor-intensive fine-tuning, susceptibility to knowledge noise, and risks of copyright infringement.
‚Ä¢In Section 7, we survey two kinds of synergies between LLMs and SLMs: one involves cloud-based LLMs and local
SLMs, while the other leverages the unique advantages of both to more effectively solve tasks.
‚Ä¢In Section 8, we investigate the trustworthiness issues of SLMs, including hallucination and privacy concerns, by
providing a taxonomic summary of current evaluation methods.
Concurrently with our survey, Lu et al. [229] evaluate open-source SLMs, focusing on their architectures, datasets,
algorithms, and on-device performance metrics such as inference latency and memory usage. Van Nguyen et al .
[345] delve into optimization strategies for SLMs, including model compression, pruning, and quantization. Chen and
Varoquaux [49] investigate how SLMs enhance LLMs and vice versa. In contrast, our survey offers a more comprehensive
review with the following differences: (1) we present a detailed taxonomy of recent advancements in SLMs in the era of
LLMs; (2) we define SLMs based on emergent capabilities and device specifications, which refines previous unclear
definitions related to LLMs; (3) we discuss SLM applications, especially in on-device tasks and deployment, topics
previously unexplored; (4) we examine domain-specific SLMs previously overlooked; and (5) we additionally consider
the synergy between SLMs and LLMs.
Manuscript submitted to ACM
6 Fali Wang, et al.
2 FOUNDATIONAL CONCEPTS IN BUILDING LANGUAGE MODELS
This section will introduce foundational concepts and background knowledge for language models, including the
concepts of architecture and the training process, as well as methods for obtaining SLMs from LLMs. The advanced
training strategy to improve SLM performance will be introduced in Section 3.
2.1 Architecture of SLMs
2.1 Architecture of SLMs
SLMs commonly employ the Transformer architecture [346] (see Figure 4), which utilizes self-attention mechanisms to
manage long-range text dependencies, essential for maintaining performance with constrained resources. However, due
to the attention mechanism, Transformers have large inference cost. Hence, to alleviate the issue, several subquadratic-
time architectures such as Mamba [ 117], Hymba [90], and xLSTM [ 25] are proposed. Next, we will give details of
Transformer due to its popularity and briefly introduce newly emerged models.
Multi-Head 
Attention
Add & Norm
Add & Norm
Feed Forward
N√ó
Input 
Embedding
Positional 
Encoding
Masked 
Multi-Head 
Attention
Add & Norm
Add & Norm
Feed Forward
N√ó
Output 
Embedding
Positional 
Encoding
Masked 
Multi-Head 
Attention
Add & Norm
Linear
Softmax
Output 
Probabilities
Inputs Outputs
Fig. 4. Transformer architecture [346].
2.1.1 Transformer. The Transformer‚Äôs self-attention mechanism [346] allows
language models to efficiently capture contextual information across longer
sequences, even with limited resources. The Transformer generally adopts an
encoder-decoder structure featuring self-attention mechanisms, feedforward
networks, positional embeddings, and layer normalization. The Transformer
architecture design tailored for SLMs is detailed in Section 5; this section will
provide only foundational concepts.
Self-Attention Mechanism enables the model to evaluate the importance
of tokens relative to each other. The self-attention mechanism is written as
Attention(Q,K,V)= softmax
 
QK‚ä§
‚àöÔ∏Å
ùëëùëò
!
V
where Q, K, and V are query, key, and value matrices, scaled by
‚àöÔ∏Å
ùëëùëò for stability
where ùëëùëò is the dimension of key matrices. The dot product QK‚ä§reflects the
similarity between the query and key vectors.
Multi-Head Attention (MHA) [ 346] is the first method that uses multiple
heads to capture diverse information. MHA allows the model to attend to different
parts of the input sequence using multiple attention heads as
MultiHead(Q,K,V)= Concat(head1,head2,..., head‚Ñé)WùëÇ, with headùëñ = Attention(QWùëÑ
ùëñ ,KWùêæ
ùëñ ,VWùëâ
ùëñ ) (1)
Each head in the Multi-Head Attention mechanism operates independently, allowing the model to capture diverse
aspects of the data. The outputs are combined using learned projection matrices WùëÑ
ùëñ , Wùêæ
ùëñ , and Wùëâ
ùëñ , concatenated, and
passed through the output projection matrix WùëÇ.
passed through the output projection matrix WùëÇ.
Building on this foundation, several modifications have been introduced to further optimize self-attention mechanisms
for specific challenges such as memory efficiency and computational speed. To address the KV-cache bottleneck in
MHA, Multi-Query Attention (MQA) [ 303] proposes that all attention heads share the same set of keys and values,
which reduces the memory and computational overhead associated with storing and managing multiple key-value pairs.
Grouped Query Attention (GQA) [ 8] serves as a middle ground between MHA and MQA. It introduces subgroups of
query heads (fewer than the total number of attention heads), where each subgroup shares a single key and value head.
Manuscript submitted to ACM
A Survey of Small Language Models 7
Unlike MQA and GQA, which reduce the number of key and value heads,Multi-Head Latent Attention (MLA) [ 211]
compresses the keys and values into a joint latent vector. This compression allows for efficient handling of key-value
pairs while maintaining high performance, significantly reducing the KV-cache and improving inference efficiency.
Flash Attention [73, 74] accelerates the self-attention mechanism by minimizing the memory overhead typical of
standard attention calculations. This optimization allows SLMs to process longer sequences more efficiently, enhancing
their functionality under strict hardware constraints.
Feedforward Network (FFN) comprises two linear transformations separated by a non-linearity, typically modeled
as FFN(x) = ùúé(xW1 +ùëè1)W2 +ùëè2. where W1 and W2 are the weight matrices, and ùëè1 and ùëè2 are bias terms. ùúé
is the activation function, which introduces non-linearity, allowing models to learn complex patterns. Generally,
ReLU is used as the activation function. In addition to ReLU, activation functions such as GeLU and SiLU are also
used in SLMs to improve performance. We give the details here: (i)ReLU (Rectified Linear Unit) [ 5] is defined as
ùúé(ùë•)= max(0,ùë•), which is commonly used for its simplicity and effectiveness. (ii) GELU (Gaussian Error Linear
Unit) [135] is defined as GELU(ùë•)= ùë• ¬∑Œ¶(ùë•)= ùë• ¬∑1
2
h
1 +erf

ùë•‚àö
2
i
, where Œ¶(ùë•)is the standard Gaussian CDF and
2
i
, where Œ¶(ùë•)is the standard Gaussian CDF and
erf is the error function. It is smoother than ReLU and widely used in models such as BERT [ 83] and GPT [ 282]
for better gradient flow control. Since calculating the Gaussian error function for each neuron is computationally
expensive and time-consuming, there are approximations using tanh and sigmoid functions, corresponding toGELUtanh
and SiLU: (iii) GELU with tanh is defined as GELUtanh (ùë•) = 0.5 ¬∑ùë• ¬∑

1 +tanh
‚àöÔ∏É
2
ùúã ¬∑(ùë•+0.044715 ¬∑ùë•3)

. This

1 +tanh
‚àöÔ∏É
2
ùúã ¬∑(ùë•+0.044715 ¬∑ùë•3)

. This
approximation uses the Tanh function to simplify computations. (iv) SiLU (Sigmoid Linear Unit) [ 97] is calculated as
SiLU(ùë•)= ùë• ¬∑sigmoid(ùë•)= ùë• ¬∑ 1
1+ùëí‚àíùë• . It effectively combines the sigmoid function with its input, enhancing modeling
capabilities. (v) SwiGLU (Swish-Gated Linear Units) [304] integrates the Swish activation function with Gated Linear
Units, defined as SwiGLU(ùë•)= Swish(ùë• ¬∑ùëä +ùëè)‚äô( ùë• ¬∑ùëâ +ùëê)where ùëä,ùëâ are the weight matrix and ùëè,ùëê are the bias
terms. The Swish function is expressed as Swish(ùë•)= ùë• ¬∑sigmoid(ùë•). This combination enhances expressiveness and
computational efficiency, making it a preferred choice in advanced models such as the Qwen series [402].
Positional Embeddings in Transformer models [346] are essential for capturing token order, providing context
about relative positions within a sequence. Traditional positional embeddings in the Transformer architecture utilize
a sinusoidal function, defined as: ùëÉùê∏(ùëùùëúùë†,2ùëñ)= sin
 ùëùùëúùë†
 ùëùùëúùë†
100002ùëñ/ùëëmodel

, ùëÉùê∏ (ùëùùëúùë†,2ùëñ+1)= cos
 ùëùùëúùë†
100002ùëñ/ùëëmodel

where ùëùùëúùë†
represents the position within the sequence, ùëñ is the dimension index, and ùëëmodel is the dimensionality of the model. To
improve the model‚Äôs capacity for understanding the relative positions of tokens within a sequence, Rotary Positional
Embedding (RoPE) [ 318] introduces a rotational matrix to the embeddings. RoPE significantly enhances the positional
encoding by maintaining the relative distances through rotational transformations, thus optimizing the model‚Äôs
interpretative ability regarding sequence dynamics.
Layer Normalization [ 185] stabilizes the training process by normalizing layer outputs, accelerating convergence.
Two types of layer normalization are commonly used [185]: (i) Non-Parametric Layer Norm normalizes inputs using
the mean and variance calculated across the layer‚Äôs dimensions without learnable parameters asLN(ùë•)= ùë•‚àíùúá
ùúé where ùúáis
ùúé where ùúáis
the mean andùúéis the standard deviation of the inputs. Its simplicity makes it ideal for SLMs. (ii)Parametric Layer Norm
includes learnable parameters ùõæ and ùõΩfor adaptive scaling and bias, enhancing model flexibility: PLN(ùë•)= ùõæ
ùë•‚àíùúá
ùúé

+ùõΩ
Additionally, RMS Norm (Root Mean Square Layer Normalization) [ 431] simplifies the calculation by using the
root mean square of inputs, reducing computational demands: RMSNorm(ùë•)= ùõæ ùë•‚àöÔ∏É
1
ùëÅ
√çùëÅ
ùëñ=1 ùë•2
ùëñ +ùúñ
+ùõΩ where ùëÅ is the
1
ùëÅ
√çùëÅ
ùëñ=1 ùë•2
ùëñ +ùúñ
+ùõΩ where ùëÅ is the
number of inputs, ùë•ùëñ is the ùëñ-th input, and ùúñ is a small constant to prevent division by zero.
Manuscript submitted to ACM
8 Fali Wang, et al.
ùêªùêªùëòùëò
ùë•ùë•ùëòùëò
ùë¶ùë¶ùëòùëò
ÔøΩùêµùêµ
ùê∂ùê∂
ÃÖùê¥ùê¥
Inputs
Linear
Conv
ùúéùúé
Selective 
SSM
ùë•ùë•
Linear
Outputs
Linear
ùúéùúé
Fig. 5. Mamba 1 architecture [117].
2.1.2 Mamba. The attention mechanism in Transformer suffers from
a drawback: it requires recalculating attention scores with every pre-
vious token for each new token generated during inference, leading
to quadratic time complexity. This increases the inference cost as
sequence lengths grow. In contrast, Mamba [75, 117], based on state
space models (SSMs) [164] which are a superclass of recurrent neural
networks (RNNs), rely only on the last hidden state for generating the
next token, enabling faster inference speeds, as shown in Figure 5. To
address the Linear Time Invariant nature of traditional SSMs, which
hinders their ability to focus on or ignore specific inputs, Mamba
improves SSMs with a dynamic selection mechanism. This mechanism selectively filters out irrelevant information
while retaining essential data, tailored to the content of the input. Leveraging this selective SSM foundation, Mamba
adeptly captures complex global relationships within sequence data. Due to its focus on the immediate previous hidden
state, as opposed to Transformer which requires access to all previous hidden states, Mamba achieves a higher utilization
rate of model parameters. This makes it more suitable for SLMs. However, we identify two drawbacks of Mamba: (i)
its focus on selectively capturing global information may compromise performance on tasks that require nuanced
understanding, such as detailed sentiment analysis or complex entity recognition; and (ii) to balance inference speed,
Mamba‚Äôs recurrent structure primarily encodes static global information, which limits its effectiveness in handling
multi-round tasks within a single query, such as interactive dialogue systems or iterative problem-solving scenarios.
In language modeling, Mamba 1 [117] is pre-trained on the Pile dataset [108] using the training recipe from [37] and
ranges from 125M to 1.3B parameters. It outperforms comparable models such as Pythia [31] and RWKV [270] in various
tasks; for instance, Mamba-1.4B achieves a 32.8% accuracy on the Arc-Challenge [65] dataset, surpassing Pythia-1.4B‚Äôs
28.5% and RWKV-1.5B‚Äôs 29.4%. Mamba 2 [75] develops a theoretical framework linking SSMs with attention mechanisms
through structured semi-separable matrices, enhancing the selective SSM to achieve 2-8x faster speeds while competing
with Transformer models. Training and configuration for Mamba 2 align with Mamba 1. Additionally, Mamba-series
models are applied widely across different fields [160, 280, 281, 463]. Other follow-up Mamba-based language models
such as Falcon Mamba 7B [463] and Mistral 7B [160] also demonstrate the efficiency of the architecture for NLP tasks.
Falcon Mamba 7B scales Mamba‚Äôs long-sequence processing capabilities to extensive language data, reducing memory
overhead and making it ideal for long-form NLP tasks. Similarly, Mistral 7B incorporates Mamba‚Äôs efficient sequence
handling ability to improve processing speed and computational efficiency in long-context NLP tasks, showcasing
Mamba‚Äôs scalability and practicality for large-scale language modeling.
Meta Tokens
Input Tokens
Input 
Proj
Latent Feature
SSM Feat.
Attn Feat.
SSM Head
Attn Head
Gate
SSM Feat.
Attn Feat.
SSM Head
Attn Head
Gate 
Norm.
Gate 
Norm.
Mean Output 
Proj
Split
Fig. 6. Hymba [90] architecture.
2.1.3 Hymba. Attention heads in the Trans-
former facilitate high-resolution recall, while
SSM heads in Mamba efficiently summarize
context. To balance performance and effi-
ciency for SLMs, Hymba [90] integrates both
attention and SSM heads within the same
layer, allowing for parallel and complemen-
tary processing of inputs, as depicted in Fig-
tary processing of inputs, as depicted in Fig-
ure 6. This hybrid-head approach enables each layer to simultaneously leverage the high-resolution recall of attention
Manuscript submitted to ACM
A Survey of Small Language Models 9
heads and the contextual summarization of SSMs, increasing the model‚Äôs flexibility and expressiveness in managing
diverse information flows and memory access patterns.
Hymba has been developed in models of varying sizes‚Äî125M, 350M, and 1.5B, trained on a combination of the
DCLM-Baseline-1.0 [192], SmolLM-Corpus [27], and a proprietary high-quality dataset, with token counts of 1 trillion,
250 billion, and 50 billion, respectively. The models incorporate the Warmup-Stable-Decay (WSD) learning rate scheduler
[143] and the data annealing technique [94] to ensure stable pretraining, conducted on 128 NVIDIA A100 GPUs. The
1.5B base model was post-trained using full finetuning (FFT), followed by direct preference optimization (DPO) [283] to
develop the Hymba-1.5B-Instruct model. In commonsense reasoning tasks, the Hymba 1.5B model surpasses Llama-
3.2-3B [7] by achieving 1.32% higher average accuracy, requiring an 11.67√ósmaller cache size, and delivering a 3.49√ó
increase in processing speed.
2.1.4 xLSTM. Long Short-Term Memory (LSTM) [ 137] shares a conceptual similarity with Mamba that achieves
success in language modeling through the introduction of time-dependent weights. This similarity raises an intriguing
question: how effective would LSTMs be at language modeling if scaled to billions of parameters, incorporating
advanced techniques from modern LLMs while addressing known LSTM limitations? Inspired by this question, Beck
et al. [25] propose xLSTM architecture, which performs favorably compared to state-of-the-art Transformers and SSMs
in empirical evaluations. To address the limitations of LSTM, xLSTM designs exponential gates to enhance effectiveness
with long sequences, expand memory cells from scalars to matrices to increase storage capacity, and remove memory
mixing to enable parallel processing.
mixing to enable parallel processing.
To test the language modeling capabilities of xLSTM scaled to billions of parameters, it is trained on a large dataset
comprising 300 billion tokens from SlimPajama [ 314] across various model sizes (125M, 350M, 760M, 1.3B). The
performance of pre-trained xLSTM is compared against RWKV-4 [270], Llama [153], and Mamba [117] across 571 text
domains of the PALOMA benchmark [237] and various downstream tasks. Across all model sizes and the majority of
tasks, xLSTM consistently outperforms the others, suggesting that larger xLSTM models could become formidable
competitors to existing Large Language Models that utilize Transformer technology.
2.2 Training SLMs from Scratch
Training SLMs from scratch entails several critical steps: (i) Pre-training, focused on acquiring general features and
knowledge from the corpus; (ii) Fine-tuning, targeted at boosting the model‚Äôs abilities and performance for specific tasks;
(iii) Decoding strategies, which involve the methods used for iteratively selecting the next token during generation.
2.2.1 Pre-training. Typically, the pre-training paradigm for language models is divided into encoder-based and decoder-
based approaches. Encoder-based models, such as BERT [83], utilize Masked Language Modeling (MLM) tasks where
the goal is to predict masked tokens within a sentence. This is achieved by maximizing:
ùëÉ(masked token |context)= softmax(W ¬∑hmask +ùëè),
ùëÉ(masked token |context)= softmax(W ¬∑hmask +ùëè),
where masked token is the original token that has been masked, context represents the other unmasked tokens in the
sentence, W and ùëèare trainable parameters of a linear output layer, hmask is the output from the transformer encoder
for the masked position, and softmax is the activation function that converts logits to probabilities over the vocabulary.
This process enhances the model‚Äôs language encoding capabilities. Decoder-based models, such as GPT [282], employ
Manuscript submitted to ACM
10 Fali Wang, et al.
Next Token Prediction (NTP) tasks, aiming to model the distribution of the next token by maximizing:
ùëÉ(next token |context)= softmax(W ¬∑hlast +ùëè),
where next token is the token that the model aims to predict, context represents the sequence of tokens preceding the
token to be predicted, and hlast is the output from the transformer encoder for the last token in the context. Effective
data preprocessing, crucial for optimizing the performance of SLMs trained from scratch, involves meticulous data
cleaning and strategic tokenization.
Data Cleaning involves techniques such as filtering, deduplication, and noise reduction, which improve data quality
and help the model generalize better. Filtering noisy or irrelevant data, addressing outliers, and handling imbalances in
the dataset ensure that the training data is both representative and efficient. Deduplication, in particular, helps prevent
overfitting by removing repeated instances, making the model more robust with efficient parameter usage.
Tokenization plays a vital role in handling diverse vocabularies without increasing model size. Advanced methods
such as Byte-Pair Encoding (BPE) [ 106] and WordPiece [316] break text into subwords [83], allowing the model to
manage rare and compound words efficiently. These strategies ensure that SLMs maintain a balance between vocabulary
coverage and model compactness, crucial for improving generalization while minimizing computational demands.
2.2.2 Fine-Tuning. After the initial training, SLMs are fine-tuned on specific tasks using task-specific data and loss
functions. Parameter-efficient fine-tuning methods [118, 140, 142, 199], such as Low-Rank Adaptation (LoRA), prefix-
tuning, and adapter modules, are particularly effective for SLMs. Low-Rank Adaptation (LoRA) [ 142] modifies
Transformer weights by introducing trainable low-rank matrices A and B for efficient fine-tuning, avoiding significant
alterations to pre-trained weights. The update is represented as: ŒîW = AB‚ä§The fine-tuned weight matrix used
in Transformer operations then becomes: Wft = W +ùõºŒîW where ùõº is a scaling factor adjusting the adaptation‚Äôs
impact, allowing fine-tuning on a smaller set of parameters while retaining the model‚Äôs foundational capabilities.
Prefix-Tuning [199] prepends learnable prefixes to the input sequence, guiding the model‚Äôs attention without altering
core model parameters. It is especially useful for generative tasks. Adapter Modules [ 140] are small, trainable layers
inserted into the pre-trained model. These layers are fine-tuned on task-specific data, allowing the base model to
remain fixed while the adapters learn the necessary adjustments. The typical structure of an adapter module includes a
down-projection, a non-linearity, and an up-projection: Adapter(h)= h +Wup ¬∑ùúé(Wdown ¬∑h +bdown)+bup where
h is the input hidden state, Wdown and Wup are the projection matrices, bdown and bup are the bias terms, and ùúé is a
non-linear activation function.
2.2.3 Decoding Strategies. After pre-training or fine-tuning, employing an effective decoding strategy is crucial for
generating output from language models. Decoding, the process of text generation from SLMs, involves iteratively
selecting the next word. A fundamental method is the greedy search, which predicts the most likely token at each step.
This is formally modeled as: ùë•ùëñ = arg maxùë• ùëÉ(ùë• |ùë•<ùëñ), where ùë•ùëñ is the token with the highest probability at the ùëñ-th
step, conditioned on the preceding context ùë•<ùëñ. Other decoding strategies, such as beam search or top-k sampling, are
crucial for generating high-quality outputs. Beam search balances exploration and exploitation by considering multiple
possible sequences simultaneously, while top-k sampling introduces diversity and creativity in text generation. These
strategies collectively ensure that SLMs are efficient and capable of delivering high performance across various natural
language processing tasks.
Manuscript submitted to ACM
A Survey of Small Language Models 11
2.3 Obtain SLMs from LLMs
Obtaining an SLM from an LLM is crucial for deploying in resource-constrained environments. Instead of training from
scratch, leveraging an LLM allows for knowledge transfer, enabling SLMs to retain much of the LLM‚Äôs linguistic and
domain knowledge with reduced training time and data. To obtain SLMs from LLMs, three primary techniques are used:
pruning, knowledge distillation, and quantization. Pruning removes less critical parameters, reducing model size while
aiming to maintain performance. Knowledge distillation transfers knowledge from a large teacher model to a smaller
student model, preserving much of the original model‚Äôs understanding. Quantization decreases parameter precision,
significantly lowering memory and computation needs with minimal impact on accuracy. These methods balance size
reduction, efficiency, and performance retention.
reduction, efficiency, and performance retention.
(b) Structured Pruning(a) Unstructured Pruning
Active Neurons Pruned Neurons
Fig. 7. Unstructured and structured pruning.
2.3.1 Pruning. Pruning is a technique used to reduce a model‚Äôs size and
computational requirements (e.g., LLMs) without significantly sacrificing
its performance [126]. This process involves identifying and removing less
important or redundant parameters and components from the model. The
primary goal of LLM pruning is to make the model more efficient, faster, and
suitable for deployment in resource-constrained environments. Typically,
pruning can be categorized into two main types: unstructured pruning and
structured pruning . An illustration of unstructured pruning and structured
pruning is shown in Figure 7.
Unstructured Pruning [76, 102, 201, 300, 321, 438, 442] prunes an LLM by removing weights individually without
considering its internal structure. The least significant parameters are pruned according to specific criteria (e.g. magnitude
or impact on the output). This method can achieve significant compression while maintaining performance. However,
it can also lead to irregular memory access patterns and reduced hardware efficiency because the pruned model lacks a
regular structure. SparseGPT [102] is a representative unstructured pruning method that can reduce large-scale GPT
models like OPT-175B [440] and BLOOM-176B [181] to up to 60% sparsity using a novel sparse regression solver. Wanda
[321] combines weight magnitudes with input activations to efficiently identify and discard less impactful parameters.
It operates in a single forward pass, rapidly achieving high sparsity without retraining. It is also worth noting that
recent studies specifically address the compatibility issues between pruning and Low-rank Adaptation (LoRA) [142],
such as LoRAPrune [438].
such as LoRAPrune [438].
Structured Pruning [13, 19, 51, 110, 124, 188, 196, 232, 243, 305, 306, 385, 410, 446], which prunes an LLM by
targeting entire structural components‚Äîsuch as neurons, channels, or layers‚Äîrather. This approach allows for a direct
reduction in dimensionality, thus efficiently reducing model complexity and memory usage. Although structured pruning
may lead to higher accuracy degradation than unstructured pruning, it simplifies implementation without requiring
specialized hardware. ShortGPT [ 243] proposes the Block Influence (BI) metric, which measures the significance
of each layer based on its transformation of hidden states. Essentially, a transformer block‚Äôs influence is measured
by how much it alters the hidden states. By calculating BI scores, ShortGPT determines which layers contribute
minimally to the overall performance and removes these low-importance layers. This simple yet effective layer removal
strategy significantly reduces the model‚Äôs parameters and computational requirements. LLM Pruner [ 232] offers a
method to efficiently prune LLMs without access to the original training dataset. It employs a three-step compression
pipeline: Discovery (identifying interdependent structures), Estimation (evaluating the performance impact of each
group), and Recovery (post-training to address performance loss). NutePrune [196] enhances structured pruning with a
Manuscript submitted to ACM
12 Fali Wang, et al.
Training 
Data
White-box KD
Teacher 
Model
Student 
Model
LLM API
Balck-box KD
Training 
Data
Student 
Model
Fig. 8. Illustration of white-box and black-box knowledge distillation [245].
Numerous-teacher method, employing variable sparsity masks and LoRA modules to guide the pruning process. This
approach effectively reduces model size and complexity. COST-EFF [305] introduces a slenderized backbone‚Äîa form
of structured pruning‚Äîand a multi-exit model that employs task-specific calibration through knowledge distillation.
This slenderization reduces the model‚Äôs spatial footprint, while the multi-exit strategy effectively balances utility and
runtime costs. To enhance the flexibility of structural pruning, DISP-LLM [110] breaks the structural dependencies in
regular methods by allowing different layers to have different subsets of features along the embedding dimension.
2.3.2 Knowledge Distillation. Knowledge distillation (KD) compresses a larger teacher model into a smaller student
model by training the student to mimic the teacher‚Äôs outputs [ 136]. This enables the student to retain much of the
teacher‚Äôs capabilities with fewer parameters, making it ideal for scaling down LLMs for resource-limited environments
while maintaining performance. KD can be categorized into white-box and black-box approaches [361, 403, 457] as
shown in Figure 8. In White-Box KD, the student has access to the teacher‚Äôs internal states or output distributions
[6, 119, 157, 169, 173, 265, 381, 434]. Generalized Knowledge Distillation (GKD) [173] introduces skew KL divergence to
stabilize gradients and enhance performance, using an adaptive off-policy approach to minimize noisy feedback and
improve efficiency.Black-Box KD relies only on teacher outputs without having access to model internals [48, 182, 271,
360]. Methods like Distilling Step-by-Step [141] use teacher-generated rationales to train smaller models, improving
performance with fewer examples. LaMini-LM [380] creates a diverse instruction dataset with GPT-3.5 Turbo responses,
enabling robust performance in smaller models.
2.1
5.8
‚Ä¶
3.5
5.3
1.2 0.2 ‚Ä¶ 3.1 2.3
0.9 1.0 ‚Ä¶ 0.3 2.5
Decomposition
Finetune for 
downstream task
1 0 ‚Ä¶ 3 2
1 1 ‚Ä¶ 0 3
1 0 ‚Ä¶ 3 2
1 1 ‚Ä¶ 0 3
2.5
4.8
‚Ä¶
3.8
3.4
Full-Precision Low-Precision Low-Precision. .
1.2 0.2 ‚Ä¶ 3.1 2.3
1.2 0.2 ‚Ä¶ 3.1 2.3
0.9 1.0 ‚Ä¶ 0.3 2.5
1 0 ‚Ä¶ 3 2
1 1 ‚Ä¶ 0 3
Full-Precision Low-Precision
(Optional)
QAT
Trainable Frozen
PTQ
Calibration Data
Training Data
Fig. 9. Illustration of quantization-aware training (QAT) and post-training quantization (PTQ).
Manuscript submitted to ACM
A Survey of Small Language Models 13
Table 1. Representative quantization methods.
Methods Bit Type Technical Contribution Problems
SqueezeLLM [170] 3-bit PTQ Sensitivity-based non-uniform quantization,
dense and sparse decomposition
ultra-low bit quantization
JSQ [122] Flexible PTQ Joint Sparsification and Quantization better compression-accuracy trade-offs.
FrameQuant [4] Fractional bit PTQ Fractional bit widths better compression-accuracy trade-offs.
OneBit [400] 1-bit PTQ Quantization-aware knowledge distillation 1-bit quantization
BiLLM [147] 1-bit PTQ Crucial Weights Selection, Block-based error
compensation
1-bit quantization
LQER [432] Flexible PTQ Quantization Error Minimization better compression-accuracy trade-offs
I-LLM [144] Flexible PTQ Fully-Smooth Block-Reconstruction, Dy-
namic Integer-only MatMul and Integer-only
Non-linear Operators
Integer-only Quantization
Non-linear Operators
Integer-only Quantization
PV-Tuning [239] 1-bit/2-bit PTQ PV algorithm better compression-accuracy trade-offs.
BitNet [357] 1-bit QAT 1-bit Transformer Architecture 1-bit quantization
BitNet b1.58 [231] {-1, 0, 1} QAT Ternary Parameters 1-bit quantization
PEQA [168] Flexible QAT Quantization Scales Optimization Parameter-Efficient Finetuning
QLoRA [81] NF4 QAT 4-bit NormalFloat and Double Quantization Parameter-Efficient Finetuning
2.3.3 Quantization. Quantization reduces the storage and computational demands of LLMs by converting floating-
point representations into lower-precision formats, significantly cutting both storage requirements and computational
complexity. Existing methods fall into two categories:Post-Training Quantization (PTQ) and Quantization-Aware Training
(QAT). Figure 9 illustrates the two quantization methods.Post-Training Quantization, applied after training, simplifies
model compression without altering the architecture or requiring retraining, though it may result in precision loss.
Consider a group or block of weights w; the linear operation can be expressed asùë¶ = wx, while the quantized version is
given byùë¶ = ùëÑ(w)x. Generally, the quantization functionùëÑis defined as [207]: ùëÑ(w)= Œî¬∑Round  w
Œî
 , Œî = max(|w|)
2ùëÅ ‚àí1 ,
where ùëÅ is the number of quantization bits, and Œî is the quantization scale factor determined by the absolute maximum
value of w. Quantization-Aware Training (QAT) enhances LLM efficiency by directly incorporating quantization
into the training process, often resulting in higher accuracy than PTQ. During QAT, the forward pass utilizes quantized
weights ùëÑ(W)and activations ùëÑ(X), while retaining full-precision values during the backward pass and for updating
gradients to ensure stable learning dynamics. The comparisons of the post-training quantization methods are summarized
in Table 1, detailing precision, addressed problems, and technical contributions of each method.
2.3.4 Low-Rank Techniques. Low-rank techniques compress LLMs by approximating a high-dimensional weight matrix
with two lower-dimensional matrices, reducing computational and memory requirements. A matrix W ‚ààRùëö√óùëõ is
approximated as W ‚âàA √óB, where A ‚ààRùëö√óùëü and B ‚ààRùëü√óùëõ, with ùëü much smaller than ùëöor ùëõ, reducing the number
of parameters. Building on this concept, Ji et al. [158] propose a low-rank method tailored for LLMs, leveraging the
observation that while LLMs have high-rank weights, their feature interactions tend to exhibit low-rank properties. The
method estimates feature distributions using pooled covariance matrices and allocates distinct compression ratios to
layers based on their sensitivity to low-rank compression. A Bayesian optimization strategy, using a Gaussian process
as the surrogate model, optimizes the allocation of low-rank dimensions, ensuring the model maintains performance
while achieving significant compression. Transitioning from model compression to fine-tuning, Cho et al. [62] tackles
system and data heterogeneity with the HETLORA method, which uses heterogeneous low-rank approximations to
accommodate the diverse capabilities of clients and data complexities. By combining local rank self-pruning with sparsity-
weighted aggregation, it balances high and low-rank LoRA modules, improving convergence speed and performance
compared to uniform approaches. LLM-Neo [409] combines knowledge distillation with low-rank adaptation (LoRA) to
improve the efficiency of transferring knowledge from a teacher LLM to a compact student model.
Manuscript submitted to ACM
14 Fali Wang, et al.
Table 2. Advanced enhancement methods for SLM.
Topic Method Main Contribution
Training
from
Scratch
MindLLM [412] Bilingual models with advanced features.
MobiLlama [336] On-device SLM with dual objectives for efficiency and capability.
MobileLLM [223] Optimizes LLM deployment on mobile with advanced architecture.
Supervised
Fine-tuning
MobileBERT [322] Compact BERT for efficient fine-tuning.
Alpaca 7B [329] Uses ChatGPT-generated tasks to tune Llama 7B.
RLHF [264] Trains using human-preferred data and reinforcement learning.
DPO [283] Dynamically adjusts log probabilities to prevent model degradation.
Data
Quality
in KD
TinyStory [96] Enhances narrative coherence in child-friendly datasets.
AS-ES [384] Improves CoT by categorizing reasoning steps.
Self-Amplify [29] Automates CoT data annotation for small models.
Distillation
for SLM
GKD [6] Aligns training and inference distributions using on-policy sequences.
DistiLLM [173] Uses skew KL divergence and adaptive off-policy for output utilization.
Adapt-and-Distill [414] Domain adapts both teacher and student models before distillation.
Quantization
SmoothQuant [386] Balances quantization difficulty using per-channel scaling.
BiLLM [147] Applies Hessian-based metrics for binary residual approximation.
LLM-QAT [221] Uses data-free knowledge distillation and logit distillation for fine-tuning.
PB-LLM [299] Binarizes non-salient weights while preserving others in higher precision.
OneBit [400] Achieves near 1-bit quantization with minimal performance loss.
BitNet [357] Introduces 1-bit Transformer architecture with BitLinear layers.
BitNet b1.58 [231] Implements a ternary weight system in enhanced BitNet.
LLM techniques
for SLM
Ma et al. [234] Combines filtering and re-ranking to improve Information Extraction tasks.
MoQE [172] Applies quantization to expert weights to outperform dense models.
SLM-RAG [215] Shows that SLMs with RAG can match LLM performance.
3 ADVANCED ENHANCEMENT STRATEGIES FOR SMALL LANGUAGE MODELS
With the foundational concepts introduced in Section 2, this section explores various advanced techniques that enhance
the performance of SLMs, including innovative training methods for training SLMs from scratch, supervised fine-tuning
(SFT) to align SLMs to adhere to instructions, advanced knowledge distillation and quantization techniques, and
techniques frequently used in LLMs such as mixture-of-experts to enhance SLM for specific applications. A summary of
enhancement techniques is also summarized in Table 2.
3.1 Innovative Training Methods for Small Language Models from Scratch
In scenarios with limited resources, we aim to train small language models to provide efficient, cost-effective solutions
tailored for specific domains, while still maintaining competitive performance with larger models. Training small
language models (SLMs) from scratch involves unique strategies that diverge significantly from those used for large
language models (LLMs). This section synthesizes cutting-edge techniques tailored to optimize the inherent capabilities
of SLMs, underscoring their potential to match or surpass larger counterparts in efficiency and effectiveness. As shown
in Figure 10, the methods for training SLMs from scratch can be categorized into three primary categories: Architecture
Design, Data Construction , and Optimization Strategy . Next, we introduce each category in detail.
Architecture Design for SLMs When designing SLM architectures, parameter-sharing techniques are employed to
minimize space usage and reduce the model‚Äôs size. As shown in the first part of Figure 10, parameter sharing is achieved
by two approaches: (i) a single Feed-Forward Network (FFN) module is shared by every transformer layer. As shown in
Figure 10 (1) middle, FFN layer sharing/reusing can maintain a smaller size while still benefiting from the depth and
complexity gained through repeated processing of input data. This technique is firstly applied in MobiLlama [336] which
surpasses the performance of existing SLMs of comparable size. (ii) Entire transformer blocks are shared. As shown in
Manuscript submitted to ACM
A Survey of Small Language Models 15
1. Architecture Design: Parameter Sharing
FFN 1
MHSA 1
Block 1
FFN 2
MHSA 2
Block 2
FFN i
MHSA i
Block i
Shared FFN
MHSA 1
Block 1
Shared FFN
MHSA 2
Block 2
Shared FFN
MHSA i
Block i
Block1
Block1
Block2
Block2
2. Data Construction: Data Filtering
Filtering methods: Format Cleaning, 
Deduplication, Sensitive Information 
Exclusion, Self-Repeating Content Filter‚Ä¶.
High quality Datasets
3. Optimization Strategy: Multiple-round training
SLM
Training loss
Hard samples
SLM
Training loss
Hard samples
Reuglar LLM Shared FFN
Immediate 
Block Sharing
Block i
Datasets
Datasets
Fig. 10. Innovative Training Methods for Small Language Models from Scratch
Figure 10 (1) right, Transformer Block-wise Sharing is another parameter-sharing approach that maintains depth
and complexity. There are different transformer block-wise sharing strategies such as repeating the transformer blocks
all over again or repeating the immediate transformer block. This technique is applied in MobileLLMs [ 223] which
has 125M and 350M parameters. MobileLLMs demonstrate performance improvements of 2.7% and 4.3%, respectively,
compared to previous models with equivalent parameters. Moreover, they exhibit accuracy comparable to LLaMa-2-7B
on API call tasks, highlighting the capabilities of smaller models in mobile environments.
Data Construction For SLMs, the emphasis on data quality surpasses that of quantity and diversity [412]. Experi-
ments demonstrate that using a quality filtering approach to remove low-quality data can lead to improved performance
in SLMs [412]. Unlike large models, which can handle diverse and large datasets, SLMs benefit more from cleaner,
high-quality data probably due to their limited capacity against noise. Generally, data processing has several steps:
(i) Remove HTML, CSS, JS, and non-text elements for clean text; (ii) Filter low text-to-content ratio web pages; (iii)
Deduplicate using SimHash [77, 293]; (iv) Exclude sensitive/offensive content with heuristics and token replacements; (v)
Remove self-repeating phrases of advertisements to enhance dataset informativeness [47, 412]. These steps collectively
ensure that training data has high-quality, informative texts. SLMs also significantly benefit from these techniques. For
example, MindLLMs [412], which are bilingual lightweight language models (available in 1.3B and 3B versions), adopt
these data processing techniques and achieve improved capability acquisition.
Training Strategy for SLMs For LLMs, due to the large model size and data volume, LLMs are usually trained with
one round. For SLMs, multiple-round training can be applied [328]. Considering some examples are hard to fit, hard
examples can be trained with a high probability [ 328]. For each round of training, the data sampling probability is
updated according to the overall loss of that sample. Experiments results show that two rounds of training and a 50%
sampling rate are a good trade-off between performance and training efficiency. Tang et al. [328] show that a deep and
thin neural architecture and multiple-round training can enhance the performance of the trained Pangu 1.5B pro model.
This model outperforms the conventionally trained Pangu 1.5B and a series of other comparable large language models
with similar model sizes on multiple benchmark datasets, achieving an average performance increase of 8.87%.
Manuscript submitted to ACM
16 Fali Wang, et al.
Unlabeled 
instructions
LLM-annotated 
instructions
Annotate/
Self-instruct
SFT
FT SLM
UFT SLM
(b) Fine-tuning on instructions 
(instruction tuning).
FT SLM
UFT SLM
(a) Fine-tuning on 
downstream tasks.
Task-specific 
data
Preference data
>
ùë¶ùë¶ùë§ùë§ ùë¶ùë¶ùë°ùë°
Reward 
model
LM Policy
Label 
reward
Sample 
completions
(c) Preference Optimization.
SFT
Fig. 11. Fine-tuning for Enhancing SLMs
3.2 Supervised Fine-Tuning (SFT) for Enhancing SLM performance
Supervised Fine-Tuning (SFT) employs a training methodology similar to pre-training but is specifically tailored to
align models to adhere to the instructions encapsulated within various instructional datasets. This approach is designed
to refine the model‚Äôs responsiveness and appropriateness to given contexts as dictated by the training data. For example,
various models, such as Alpaca [ 329], UltraChat [86], WizardLM [392], SlimOrca [203], ShareGPT [356], Capybara
[72], Deita [216], and MetaMathQA [420], incorporates a suite of conversational datasets to enhance their capabilities
in context-aware dialogue and instruction adherence. Usually, as shown in Figure 11, existing SFT methods can be
categorized into three categories:
‚Ä¢(i) Classical fine-tuning with downstream data [83, 282] trains SLMs on task-specific annotated data, transferring
general language representations to specific tasks such as sentiment analysis. In the LLM era, this approach remains
effective, such as enhancing LLMs by calibrating responses or assigning risk scores with smaller models such as
BERT [450], or optimizing for mobile devices with MobileBERT [322].
‚Ä¢(ii) Instruction tuning with LLM-generated data [86, 203, 329] or human-generated questions with LLM annotations
[356] aims to align generative models with specific instructions, enhancing their instruction-following and reasoning
capabilities. For example, Alpaca 7B [329] uses 52k ChatGPT-generated instruction-following examples from 175
self-instructed seed tasks to tune Llama 7B [338]. Meanwhile, StableLM [26, 340] is trained on the Restruct-v1 dataset,
which includes summarization, question-answering, and sentiment analysis tasks, using instruction data from [226].
‚Ä¢(iii) Preference optimization with human feedback [264, 283, 356] aims to better align language models with
human preferences. Reinforcement Learning from Human Feedback (RLHF) [264] gathers human-preferred data,
trains a reward model, and fine-tunes the LM using reinforcement learning. Direct Preference Optimization (DPO)
[283] provides a simpler alternative to RLHF. Unlike RLHF, DPO avoids explicit reward modeling and reinforcement
learning techniques. Instead, it adjusts the log probabilities of preferred versus non-preferred responses using a
dynamic weighting mechanism, preventing model degradation issues typical of methods relying on probability ratios.
For instance, Llama 3.2 1B & 3B apply SFT and DPO in post-training to enhance alignment with instructions and
human preferences.
Manuscript submitted to ACM
A Survey of Small Language Models 17
LLM (e.g., GPT4)
Instruction: Create 
words that 3-year-old 
child would likely 
understand.
LLM (e.g., GPT4)
Q S1 S2 S4S3
Q Complex Chain of Thoughts
Q S1
S2
S1Q S2
S3Q S1
1. Augment data from other models (e.g., GPT4)
Question: Which type of activity would 
most likely be included on a weather map
A: satellite, B: seismic, C: hurricane 
SLM
Rationale Generation Methods
Answer: C ‚àö
Keywords: which, type, weather, map
2. Generate data by itself
2. Generate data by itself
CoT: Answer is C because of keywords: ‚Ä¶‚Ä¶
Simple Story 
Datasets
Fig. 12. Data Quality in Knowledge Distillation (KD)
3.3 Data Quality in Knowledge Distillation (KD)
Transitioning from the discussion on training SLMs from scratch, this section delves into the critical role of data quality
in Knowledge Distillation (KD). The motivation here is to highlight how high-quality data generated from LLMs can
significantly enhance the learning efficiency and performance of SLMs. The central idea is that meticulously crafted
datasets when used in KD, enable SLMs to more effectively mimic the advanced capabilities of their larger counterparts.
As shown in Figure 12, the data can come either from (1) other strong LLMs (e.g., GPT-4 [2]) which are much larger and
more powerful than the target SLM, or (2) the target SLM itself.
Augment Data from LLMs. LLM-generated data could be categorized as pre-training data and fine-tuning data .
Firstly, due to the limitations of model size, studies have shown that training SLMs requires simple and comprehensible
data [96, 183, 187, 384]. As shown in Figure 12 (1) left, TinyStory [96] shows that small models (tens of millions
of parameters) can generate coherent stories for 3-4-year-olds. GPT-3.5 or GPT-4 [2] prompts create simple stories
from three keywords chosen from a 1,500-word vocabulary, which are then used to train SLMs for similar outputs.
This approach shows that simple and comprehensible data can help smaller models exhibit behaviors similar to those
of larger language models, such as obeying scaling laws and achieving enhanced performance. On the other hand,
many efforts to enhance the Chain-of-Thought (CoT) capabilities of small models involve using LLMs to generate
high-quality fine-tuning CoT data. As shown in Figure 12 (1) right, these data train small models end-to-end to mimic
CoT reasoning [235, 384]. AS-ES Learning [384] highlights that small models struggle with complex reasoning, even
when provided detailed steps, as these require nuanced extraction and abstraction. Therefore, the study introduces a
paradigm splitting reasoning into extractive segments (context reminders) and abstractive segments (inferred insights).
Augment Data from Itself. Besides distilling data from other LLMs, language models can also train on their own
outputs [29, 145, 337]. Since voting strategies can improve the performance of LLMs, reasoning paths that lead to the
majority answer can be further utilized to fine-tune LLMs [145]. Similarly, SLMs can generate their training data with
the aid of existing rationale generation methods. Self-Amplify [29] notes that human annotation of Chain-of-Thought
(CoT) data is very time-consuming; thus, automated rationale generation methods have been proposed. These methods
Manuscript submitted to ACM
18 Fali Wang, et al.
1. Distribution Mismatch
2. Domain Gap
Student ùëûùëû Teacher ùëùùëù
ùê∑ùê∑(ùëûùëû, ùëùùëù)
(ùë•ùë•, ùë¶ùë¶)
Student ùëûùëû Teacher ùëùùëù
ùê∑ùê∑(ùëûùëû, ùëùùëù)
ùë•ùë•
ùë¶ùë¶‚Ä≤
(x, y‚Äô)
Student ùëûùëû Teacher ùëùùëù
ùê∑ùê∑(ùëûùëû, ùëùùëù)
(ùë•ùë•, ùë¶ùë¶)
Replay Buffer
(ùë•ùë•, ùë¶ùë¶‚Ä≤)
Large 
model
Distill Small model
(a) Domain Adaptation
Model in 
wiki 
domain
Model in 
Pubmed 
domain
Large, wiki 
model
Large,  
pubmed 
model
Pubmed
Small, pubmed 
model
Small, pubmed 
model
Small, wiki 
model
Pubmed
(a) Original KD (b) On-policy Approach (c) Off-policy Approach
(b) Knowledge Distillation
(c) Adapt-and-Distill
Dataset Dataset Dataset
Pubmed
Fig. 13. Distillation Techniques for Enhancing SLM Performance. On-policy means learning only use data from the current student
(policy), while off-policy permits the use of previously gathered data.
involve three main steps: (1) Selection of samples (ùë•,ùë¶)that the model predicts correctly as few-shot examples; (2)
Rationale generation, where rationales are produced using post hoc explanation methods; (3) Prompt design for SLMs,
where the final prompt is crafted based on the previously generated rationales.
3.4 Distillation Techniques for Enhancing SLM Performance
Following the discussion on data quality in KD, this section reviews specialized KD training strategies designed to
enhance the performance of SLMs. The motivation is to address the unique challenges and constraints involved in
distilling knowledge from LLMs to SLMs, ensuring that the smaller models can maximize their performance gains. As
shown in Figure 13, two main gaps between LLMs and SLMs lead to challenges in distillation:distribution mismatch and
domain gap . Distribution mismatch [6, 173] occurs when the distribution of output sequences during training does not
align with the distribution of sequences that SLMs produce during inference, leading to suboptimal performance of the
student model. The domain gap [414] arises when there is a discrepancy between the domains or tasks on which the
LLMs and SLMs are trained and applied. This gap can cause significant degradation in the performance of the student
model if not properly addressed during the distillation process. To address these issues, specialized strategies involve
first aligning the teacher and student models with the target domain before proceeding with knowledge distillation. To
explore these challenges further, we now delve into the details of these two branches of methods.
Distribution Mismatch In original knowledge distillation, illustrated in Figure 13 Distribution Mismatch (a),
the teacher and student are provided with the same input sequences ùë• and output labels ùë¶, producing probability
distributions for the next token ( ùëû and ùëù). The loss is calculated as the difference between these two distributions,
ùê∑(ùëû,ùëù). However, a key challenge arises due to distribution mismatch: the output sequences during training (ùë¶) differ in
distribution from those the SLMs produce during inference (ùë¶‚Ä≤). To address this challenge, various techniques have been
proposed. As shown in Figure 13 Distribution Mismatch (b), one approach trains the student model using on-policy
sequences‚Äîsequences generated by the student itself‚Äîguided by the teacher model‚Äôs feedback. Specifically, both the
student and teacher take the same input (ùë•) and the student-generated output (ùë¶‚Ä≤), producing probability distributions
Manuscript submitted to ACM
A Survey of Small Language Models 19
for the next token ( ùëû and ùëù, respectively). The loss is calculated as the difference between these two distributions,
ùê∑(ùëû,ùëù). This approach helps the student model reduce the distribution gap between training and inference by learning
from the teacher‚Äôs feedback on its own generated sequences. Generalized Knowledge Distillation ( GKD) [6] is the
first work using this technique and improves distillation outcomes. However, a drawback of this technique is that
it requires the student to constantly produce new training sequences, which can be computationally expensive. To
improve efficiency, as shown in Figure 13 Distribution Mismatch (c), an adaptive off-policy approach can be used to
efficiently manage student-generated outputs by storing them in a replay buffer, thereby reducing computational costs.
DistiLLM [173] employs this off-policy approach and improves the efficiency of KD.
Domain Gap When training an SLM in a specific domain that differs from the domain of the LLMs, the gap between
the two domains becomes problematic. As illustrated in Figure 13 Domain Gap (a), domain adaptation fine-tunes a
language model, initially trained on a general corpus, using a specialized dataset such as PubMed to enhance performance
in that specific domain. As illustrated in Figure 13 Domain Gap (b), Knowledge distillation transfers knowledge from
the larger model to the smaller one. However, because the teacher model may not produce high-quality outputs on
specialized datasets, domain adaptation is needed prior to knowledge distillation. As illustrated in Figure 13 Domain
Gap (c), Adapt-and-Distill [414] tackles the domain gap by distilling general large models into smaller ones. This
paper introduces AdaLM and demonstrates that the ‚ÄúAdapt-and-Distill‚Äù strategy‚Äîfirst involving domain adaptation
of both the large teacher model and the small student model, followed by distillation‚Äîis the most effective compared
to three other strategies: training directly from scratch, distillation followed by adaptation, and adapting the teacher
model before distillation into a general small student model. These innovative techniques are crucial for enhancing the
capabilities of SLMs, making them more efficient and effective for various applications. However, adapting both the
teacher (LLMs) and the student (SLMs) models to the target domain can be time-consuming. Future research could
focus on efficiently solving the domain gap problem.
Insights: Here are some insights from distillation techniques:
‚Ä¢Sampling SLM outputs during the training process is the main approach to resolving distribution mismatch.
‚Ä¢Techniques like Adapt-and-Distill address the domain gap by first adapting both the teacher (LLMs) and the
student (SLMs) models to the target domain before proceeding with distillation.
3.5 Performance Improvement through Quantization
As mentioned in Section 2, quantization is one of the most effective methods for adapting LLMs to SLMs. However, com-
pression to smaller sizes often compromises performance. To address the performance drop associated with quantization,
various methods have been proposed. This section examines how these quantization methods specifically enhance
the performance of SLMs. While the general introduction to compression methods is discussed in the compression
section, the focus here is on detailing those approaches that boost the efficiency and effectiveness of SLMs. As shown in
Figure 9, we categorize these quantization methods into two main approaches: Post-Training Quantization (PTQ), where
quantization is conducted on a well-trained fixed model, and Quantization-Aware Training (QAT), where quantization
is integrated into the training process. This section introduces advanced techniques in PTQ and QAT respectively.
Post-Training Quantization (PTQ) primarily includes weight quantization and activation quantization. Weight
quantization aims to quantize model parameters while preserving performance.GPTQ [103] compresses LLMs to 4-bit or
2-bit by quantizing weights layer-by-layer to minimize layer-wise quantization errors.PB-LLM [299], applicable to both
Manuscript submitted to ACM
20 Fali Wang, et al.
PTQ and QAT, retains the most salient weights while binarizing the rest based on magnitudes.BiLLM [147], another
PTQ method, uses a Hessian-based metric to identify salient and non-salient weights. Salient weights undergo binary
residual approximation to minimize loss, while non-salient weights are divided into sparse and concentrated groups for
separate binarization, reducing quantization errors. Activation quantization faces challenges with outliers that can
stretch the quantization range, causing most values to cluster at few bits and introducing significant errors. To address
this, LLM.int8() [80] isolates outlier features for 16-bit processing and handles the rest in 8-bit. SmoothQuant [386]
circumvents per-channel quantization issues by employing a "smoothing" technique that shifts the quantization challenge
from activations to weights through a per-channel scaling transformation. This balance between activating and weight
quantization allows effective 8-bit quantization (W8A8), preserving accuracy while significantly reducing memory and
computational costs. SmoothQuant thus enhances the efficiency of SLMs in resource-constrained environments.
Quantization-Aware Training (QAT) differs from PTQ in that it includes a training phase after the model has been
quantized. When models are quantized to extremes, such as 2-bit or 1-bit, performance typically drops significantly,
but further training can help the model retain its capabilities. For instance, to mitigate performance degradation from
binarization, PB-LLM [299] selectively binarizes only non-salient weights, preserving the most salient ones at higher
precision. This method effectively reduces the model size without significantly impacting performance. Salient weights
are chosen based on their magnitude, ensuring that the most influential weights maintain higher precision to preserve
the model‚Äôs reasoning capabilities. The paper explores both post-training quantization (PTQ) and quantization-aware
training (QAT) to fine-tune and recover the performance of partially binarized models, achieving a balance between
compression and accuracy. OneBit [400] and BitNet [357] address the severe performance degradation associated
with 1-bit quantization by decomposing floating-point matrices and employing mixed-precision strategies. Specifically,
OneBit introduces Sign-Value-Independent Decomposition (SVID), which decomposes a floating-point matrix into a
1-bit matrix and two floating-point vectors. This method allows LLMs to be quantized to a 1-bit level while minimizing
performance loss. By retaining critical information with the floating-point vectors, OneBit effectively balances extreme
compression with maintaining model accuracy. BitNet b1.58 [231] improves on the original BitNet by introducing a
ternary matrix weight system -1, 0, 1, resulting in a 1.58-bit model. BitNet b1.58 matches the performance of full-precision
models starting from a 3 billion parameter size while further reducing memory and latency costs. LLM-QAT [221]
employs data-free knowledge distillation, where the pre-trained model itself generates data for fine-tuning the quantized
model (student) using logit distillation from the full-precision model (teacher). This method incorporates quantization
of weights, activations, and key-value cache, achieving accurate 4-bit quantization for weights and key-value caches,
and 6-bit for activations, demonstrating substantial improvements over existing post-training quantization methods.
Insights: Insights drawn from quantization strategies include:
‚Ä¢Post-Training Quantization techniques primarily focus on quantizing model weights, where selecting salient
weights is crucial. Beyond weight quantization, handling outliers in activation signals is a significant challenge
in quantizing activations.
‚Ä¢Quantization-Aware Training methods show that low-bit quantization (e.g., 1-bit models) requires additional
tuning to maintain performance. Knowledge can be distilled from the model before quantization to the
quantized model.
Manuscript submitted to ACM
A Survey of Small Language Models 21
3.6 Techniques in LLMs Contributing to SLMs
This subsection explores the potential of advanced techniques such as RAG and MoE, which enhance LLM performance,
to also maintain or boost SLM performance within constrained computational budgets. However, effectively integrating
these techniques into SLMs, which inherently possess limited capabilities, remains an unresolved challenge.
Retrieval Augmented Generation (RAG) enhances the capabilities of language models in knowledge-intensive
tasks by incorporating a retrieval mechanism. This approach allows models to access relevant contextual information
from a data repository in response to user queries. By integrating this retrieved data, RAG-equipped models better
understand specific topics, enabling more informed and accurate outputs. For SLMs, a significant concern is whether
they possess the capacity for long-context reasoning. A recent study [215] compares SLMs at the 7B level with RAG to
larger models such as GPT-3.5 and GPT-4, suggesting that SLMs equipped with RAG can sometimes perform comparably
or even better than LLMs. These findings indicate that RAG for SLMs is effective and represents a promising direction
for future research.
Mixture-of-Experts (MoE) [ 39] has emerged as an effective method for substantially scaling up model capacity
with minimal computation overhead in LLMs. The MoE framework is founded on a straightforward yet potent concept:
distinct components of a model, referred to as ‚Äúexperts‚Äù, specialize in different tasks or data facets. In this paradigm,
only the relevant experts are activated for a specific input, which manages computational costs while leveraging a
vast pool of specialized knowledge. This scalable and adaptable approach enables increased model capacity without
proportionally escalating computational demands. We argue that MoE is particularly suitable for SLM architectures
[161] as it minimizes both computational load and memory overhead. However, research on MoE for SLMs remains
sparse. Future studies could investigate how large LLM MoE architectures can be effectively compressed into small
ones or how to develop an SLM with MoE tailored for specific devices from scratch.
4 APPLICATIONS OF SMALL LANGUAGE MODELS
4 APPLICATIONS OF SMALL LANGUAGE MODELS
In this section, we delve into the applications of small language models (SLMs) across various NLP tasks and their
deployment strategies. Due to benefits such as enhanced privacy, faster inference, and lower memory requirements,
many NLP applications are now leveraging SLMs over LLMs. Additionally, deploying SLMs often involves considerations
of memory and runtime efficiency, which are crucial for optimizing resource use on budget-constrained edge devices,
particularly mobile phones. Then, we will discuss task-specific applications of SLMs and their deployment methods on
mobile and edge devices.
4.1 Task-specific SLM Applications
This subsection explores the diverse NLP tasks to which SLMs can contribute. Question-answering and coding represent
generative tasks, while recommender systems and web search (though not strictly within the NLP domain) typically
leverage the encoding capabilities of SLMs. Additionally, the application of SLMs on mobile devices is particularly
well-suited due to constraints in memory and computing resources. The representative works are systematically
organized in Table 3.
4.1.1 SLM Applications in Question-Answering. Question-answering (QA) is a fundamental task in the NLP field,
demanding language models to exhibit abilities in understanding language, reasoning, common sense, and recalling
specialized knowledge. Typically, larger language models yield better QA performance. However, the substantial size
of these models introduces challenges such as immense computational requirements, privacy concerns when using
Manuscript submitted to ACM
22 Fali Wang, et al.
Table 3. Task-specific SLM Applications
Aspect Representative work Key point
SLM in QA
Alpaca [329] Tune Llama 7B [338] using 52k ChatGPT-generated examples.
Stable Beluga 7B [238] Employ explanation tuning to Llama-2 7B [339] on an Orca-style dataset.
Fine-tuned BioGPT [125] Fine-tuning BioGPT (1.6B) [230] on PubMedQA.
Financial SLMs [275] Transfer financial knowledge from GPT-4 [2] to multiple SLMs.
ColBERT [112] Fetch retrieval documents for SLMs to answer domain-specific questions.
Rationale Ranking [128] For unseen questions, combine retrieval with LLM-generated rationales.
T-SAS [156] Enhance SLMs adaptability with self-generated pseudo labels.
SLM in
Coding
Phi-3.5-mini [1] New addition to the Phi-3 series and focus on high-quality data.
TinyLlama [439] 1.1B Transformer model is trained on 3T corpus.
CodeLlama [292] A derivative of Llama 2 fine-tuned on domain-specific datasets.
CodeGemma [331] Fine-tuning Gemma to enhancing coding capabilities.
SLM in
Recommen-
dation
SLM in
Recommen-
dation
PromptRec [382] Training on prompt templates
SLIM [369] Step-by-step Knowledge Distillation
BiLLP [309] LLaMa-2-7B as planner and reflector
ONCE [214] LLaMa-2-7B as Content Encoder
RecLoRA [455] Personalized low-rank adaptation
SLM in Web
Search
Content encoder [44, 152, 228] Encode concatenated queries and documents.
Ranker [63, 260] Re-rank retrieved documents using a specially SLM.
Rewriter [233] Bridge the gap between queries and needed knowledge by rewriting inputs.
SLM in
SLM in
Mobile-
device
Octopus [52] Calling software APIs via learning in documents
MobileAgent [87] Standard Operating Procedure (SOP)
ùõº-UMI [307] SLMs serve as Multi-agents in tool uses.
Mobile Interaction [42] Text-to-action control and tests on 6GB and 4GB Android devices
AutoDroid [376] Interaction based on GUI and APP knowledge injection
M4 [423] a foundation model handling all mobile AI tasks.
Agent for Text Rewriting [458] Data Knowledge Distillation from LLMs
proprietary LLMs, and difficulties in customization. These issues lead researchers and developers to favor SLMs in
scenarios that demand efficiency, privacy, and customization. Therefore, we explore methods to enhance the capabilities
of SLMs in QA across three key areas: (i) Instruction Tuning of Generic SLMs for QA, (ii) Instruction Tuning of
Domain-Specific SLMs for QA, and (iii) Enhancing SLMs for Out-of-Domain Questions.
Instruction Tuning Generic SLMs for QA. Despite the Phi series‚Äô high question-answering capability, its training
cost with over 3.4T tokens on 512 H100 GPUs for 10 days [ 1] is prohibitive for many researchers and developers.
Instruction tuning [372] offers a cost-effective alternative, enhancing small models by fine-tuning on large model
outputs. Alpaca 7B [329] tunes Llama 7B [338] with 52k ChatGPT-generated examples from 175 seed tasks. This behavior
cloning mimics teacher models effectively but struggles in reasoning-intensive QA tasks where accuracy is key, not style
[60]. To counter it, explanation tuning [238] enhances Llama-2 7B [339] using explanatory LLM answers to improve
reasoning. However, its effectiveness varies with system instructions, and those effective for larger models like GPT-4
may not suit smaller ones. SLMs also struggle to identify optimal system instructions for different tasks. Therefore, Orca
2 [247] addresses this by promoting cautious reasoning, deciding which solution strategy to choose for a given task
among direct answer generation, or ‚ÄúSlow Thinking‚Äù strategies (step-by-step, guess and check or explain-then-answer,
etc.) and erasing specific system instructions during training. This involves (1) solution strategy is guided by the
performance of Orca 1 [ 251], (2) writing task-specific system instructions corresponding to the chosen strategy to
obtain teacher responses for each task, and (3) at training time, employing Prompt Erasing to replace student‚Äôs system
Manuscript submitted to ACM
A Survey of Small Language Models 23
Table 4. Comparison of instruction-tuned domain SLMs for QA and LLMs on FinQA [56] and PubMedQA [163].
Model Size Instruction tuned? Task Name Shot Type Accuracy (%)
GPT-4 [2] - √ó FinQA Zero-shot 77.5
Phi-3-Mini [1] 2.7B ‚úì FinQA Zero-shot 77.6
Meditron-70B [55] 70B √ó PubMedQA Zero-shot 81.6
RankRAG-llama3-70B [421] 70B √ó PubMedQA Zero-shot 79.8
Flan-PaLM [313] 540B √ó PubMedQA Few-shot 79.0
GAL 120B [330] 120B √ó PubMedQA Zero-shot 77.6
GAL 120B [330] 120B √ó PubMedQA Zero-shot 77.6
Flan-PaLM [313] 62B √ó PubMedQA Few-shot 77.2
BioGPT [230] 345M ‚úì PubMedQA Zero-shot 78.2
BioGPT-Large [230] 1.5B ‚úì PubMedQA Zero-shot 81.0
instructions with generic ones vacated of details of how to approach the task, encouraging students learn not just task
solutions but also deeper reasoning abilities.
Instruction Tuning Domain SLMs for QA. Beyond instruction tuning for generic SLMs, tuning domain-specific
SLMs is also crucial, as they provide specialized assistance where generic SLMs may underperform. Instruction-tuning
generic SLMs can derive domain SLMs. We summarize some representatives in several domains. (1) In finance, Phogat
et al. [275] transfer financial QA abilities from teacher LLMs such as GPT-4 [2] to specialized SLMs such as Phi-3-Mini [1],
using datasets such as FinQA [56], ConvFinQA [57], and TATQA [454]. They train SLMs with Python programs created
by the teacher model, which detail steps for financial reasoning, including concept comprehension, formula identification,
entity extraction, and calculations. During inference, SLMs generate Python code that an external interpreter executes.
(2) In the medical field, Guo et al . [125] enhance student SLMs, including domain-specific BioGPT (1.6B) [ 230] and
general Llama 7B [338], by fine-tuning on enriched PubMedQA [163] data. This enhancement is achieved by generating
new samples or rewriting existing ones using teacher LLMs, which include the highly knowledgeable GPT-4 and the
relatively weaker ChatGPT. The best SLM, with under 1.6 billion parameters, achieves 75.4% accuracy, surpassing
GPT-4‚Äôs 74.4% in few-shot settings on the PubmedQA test sets. It demonstrates that LLMs effectively refine and diversify
question-answer pairs, leading to enhanced performance in a significantly smaller model after fine-tuning. We report
the detailed results of comparisons of instruction-tuned domain-specific language models for QA and larger language
models on FinQA [56] and PubMedQA [163], as shown in Table 4.
Enhancing SLMs for Out-of-Domain Questions. One of the major advantages of LLMs is their strong compre-
hension and logical reasoning abilities, which SLMs often struggle to match due to their limited parameters, especially
when handling unseen or out-of-domain questions. Various methods have been developed to address this limitation,
including Retrieval-Augmented Generation (RAG) and self-adaptive techniques.
(1) Retrieval-Augmented Generation (RAG): Incorporating External Knowledge for Domain-Specific QA. RAG ad-
dresses OOD questions by integrating external knowledge during inference, allowing models to access information
beyond their pre-trained parameters. By retrieving relevant documents in real time, RAG enables small language
models to provide accurate answers on specialized topics. In the telecommunications domain, Gichamba et al .
[112] use ColBERT as a dense retrieval system to fetch documents from technical datasets. By encoding queries and
documents separately, ColBERT computes relevance scores, helping small models like Phi-2 and Falcon-7B retrieve
precise technical information to answer complex telecom-related queries. Rationale Ranking [128] addresses
answering unseen questions using smaller language models by integrating external explanatory contexts from
retrieval systems with reasoning rationales from LLMs. This method involves ranking both the retrieved explanatory
Manuscript submitted to ACM
24 Fali Wang, et al.
contexts and LLM-generated rationales using a scoring module, which then combines them to form a cohesive
context. Consequently, this integrated approach enhances the SLMs‚Äô performance on unseen questions.
(2) Self-Adaptive Techniques: Enhancing Model Adaptability with Self-Generated Pseudo Labels. Fine-tuning, while
effective in adapting domain knowledge, can be impractical in realistic scenarios where labeled datasets are scarce.
To overcome this, self-adaptive techniques employ self-generated pseudo labels to activate specific aspects of the
target tasks, thereby enhancing model adaptability [312, 347]. Test-time Self-Adaptive Small LMs (T-SAS) [156] first
stochastically generates multiple answers for an unlabeled question. The most plausible answer is then selected via
majority voting to enhance pseudo-label accuracy, serving as a pseudo-label for training during test-time.
Comparison between LLMs and SLMs for QA. When comparing LLMs such as GPT-4 [2] or BLOOM-175B [181]
with fine-tuned SLMs in QA tasks, the benefits of SLMs are clear. LLMs, while versatile across multiple domains due to
extensive pre-training, are computationally demanding, making them less ideal for resource-limited settings. SLMs,
however, when fine-tuned for specific domains, often match or exceed the performance of larger models within those
specialties. The trade-off is between large-scale models‚Äô generalization and small-scale model‚Äôs specialization: LLMs
handle diverse domains but may need additional techniques such as knowledge injection for domain-specific queries.
In contrast, domain-specific SLMs, though less flexible, provide higher accuracy and more relevant responses, making
them ideal for edge deployments where computational resources are scarce but domain precision is crucial.
4.1.2 SLM Applications in Coding. The adoption of SLMs for coding offers an alternative to LLMs due to their lower
computational needs and potential for domain-specific tuning. Despite LLMs‚Äô proficiency in code generation and
programming support, SLMs are advantageous for their faster inference, reduced operational costs, and suitability
for real-time environments where rapid responses are crucial. Representative works are discussed next. The Phi
series [1, 155, 200] showcase SLMs‚Äô evolution in coding tasks. For instance, Phi-1 [ 120], a Transformer with 1.3B
parameters, specializes in basic Python coding and achieves notable scores in benchmarks such as HumanEval [120],
which includes 164 programming problems. Subsequent models, Phi-1.5 and Phi-2, have enhanced these capabilities,
while Phi-3 demonstrated SLMs‚Äô potential to rival larger models [1]. The latest model, Phi-3.5-mini, with 3.8B parameters,
excels in long context tasks using advanced fine-tuning and optimization techniques, performing comparably to larger
models such as Llama-3.1-8B-instruct [94] and surpassing smaller ones like Gemma-2 [333].
Another avenue of development is the fine-tuning of general-purpose SLMs for coding tasks [ 23, 121, 227, 292,
331]. For instance, CodeLlama models [292], derivatives of Llama 2 [339], undergo a rigorous fine-tuning process on
domain-specific datasets, enhancing their proficiency in specific programming languages such as Python. They are
trained to handle tasks such as syntax error detection, code suggestion, and infilling, where they learn to predict and
complete missing parts of the code. This specialized fine-tuning improves their ability to interpret and execute detailed
programming instructions, making them highly effective in real-time code editing environments [292]. CodeGemma
models [331], stemming from Google DeepMind‚Äôs Gemma framework, also exhibit a focused approach to enhancing
coding capabilities through fine-tuning. These models are specifically engineered for high-performance code generation
and infilling, underpinned by extensive training on a vast corpus of over 500 billion to 1 trillion tokens, predominantly
consisting of code. This comprehensive dataset enables CodeGemma models to excel in mathematical reasoning and
complex problem-solving within code contexts, setting new benchmarks in latency-sensitive applications such as
real-time IDE support and automated code reviews [331].
Manuscript submitted to ACM
A Survey of Small Language Models 25
Table 5. Performance comparison between
SLMs and LLMs in coding benchmarks. All
models listed are chat or instruct versions, and
performance are sourced from respective re-
search papers or technical reports [94, 121, 292,
331, 342].
Model Size HumanEval MBPP
DeepSeek-Coder [121] 1.3B 65.2 49.4
CodeGemma [331] 2B 37.8 49.2
Gemma 2 [333] 2B 17.7 40.2
Phi-3.5-mini [342] 3.8B 62.8 69.6
DeepSeek-Coder [121] 6.7B 78.6 65.4
CodeGemma [331] 7B 60.4 55.2
CodeGemma [331] 7B 60.4 55.2
Llama 3.1 [94] 8B 66.5 69.4
Gemma 2 [333] 9B 61.0 69.3
GPT-3.5 Turbo - 68.0 71.2
DeepSeek-Coder [121] 33B 79.3 70.0
Llama 3.1 [94] 70B 80.5 75.4
Llama 3.1 [94] 405B 89.0 78.8
GPT-4o OpenAI [263] - 90.2 81.4
Claude 3.5 Sonnet [15] - 92.0 76.6
Comparison between SLMs and LLMs on Coding . Table 5 provides a
comparative analysis of SLMs and LLMs on coding benchmarks HumanEval
[50] and MBPP [20]. Insights include: (i) Small SLMs (1.3B - 3.8B Parameters)
like Phi-3.5-mini [342] achieve high scores, demonstrating the efficacy of
small models. Mid-sized SLMs (6.7B - 9B Parameters), such as DeepSeek-
Coder 6.7B [ 121] and Llama 3.1 8B [ 94], show improved performance,
indicating that larger model sizes and enhanced training contribute to
better accuracy. Large models (33B and above) like Llama 3.1 405B [ 94],
GPT-4o [263], and Claude 3.5 Sonnet [15] excel, supporting the idea that
bigger models generalize better across diverse coding tasks; ( ii) There‚Äôs
a notable trade-off between computational efficiency and performance,
with larger models requiring more resources, impacting their practical
deployment in constrained environments; ( iii) Specialized training and
fine-tuning, as used in models like DeepSeek-Coder [121], are crucial for
excelling in coding tasks, though such models may not handle complex
requests as effectively, highlighting the versatility of general SLMs for
broader applications.
broader applications.
4.1.3 SLM Applications in Recommender Systems. Recommender systems are essential in various online services,
helping to manage information overload and meet users‚Äô personal needs. SLMs enhance recommendation systems by
(1) addressing the cold start problem; (2) reducing popularity bias; (3) improving long-term planning; (4) serving as
personalized recommenders; and (5) acting as content encoders. These applications show the versatility and effectiveness
of SLMs in boosting performance and personalization in recommendation. Next, we introduce the details.
SLM for System Cold Start Problem. Traditional recommendation systems, which utilize historical user-item
interactions such as clicks, purchases, and ratings to learn representations and match items to users, fail in scenarios
lacking any user-item interactions, known as the cold-start recommendation problem, often occurring in start-up
businesses [289]. Although LLMs address this with in-context learning, their slow and costly inference restricts real-time
use. Thus, PromptRec [382] explores using SLMs as in-context recommenders for recommendation system cold-start
problems. However, SLMs often struggle without emergent context-learning abilities. To overcome this, SLMs are
enhanced by pre-training on relevant corpora, using a improved C4 corpus subset [284], and by developing training
prompts for different domains, enhancing cold-start performance. Results show that enhanced SLMs like BERT-mini
[83], with 11.3M parameters, achieve BERT-large‚Äôs performance in cold-start scenarios, with only 17% of BERT-large‚Äôs
inference time. Similarly, many studies have addressed the cold-start problem by leveraging BERT [133, 261, 441, 459].
For example, ADLRS [133] employs BERT to convert web-crawled item profiles into vectors that highlight key aspects,
aiding recommender systems in acquiring essential initial information.
SLM for Mitigating Popularity Bias. Popularity bias in recommender systems, marked by discrepancies between
item popularity in training datasets and the real world, often stems from using closed-loop datasets with limited
information. Recent LLMs leverage their broad open-world knowledge to better reason about user-item interactions
[206, 214], reducing this bias by providing recommenders with more extensive item details. Using the chain-of-thought
(CoT) prompting, LLMs decompose complex tasks into intermediate reasoning steps, enhancing understanding of
user behavior and interests. However, LLMs‚Äô high resource demands limit their practical use. To overcome this, the
Manuscript submitted to ACM
26 Fali Wang, et al.
Step-by-step Knowledge Distillation Framework for Recommendation (SLIM) [369] distills LLM reasoning capabilities
into SLMs, keeping just 4% of the original parameters, transitioning from ChatGPT to Llama 7B [ 338]. SLIM uses
detailed LLM templates to extract reasoning steps and streamlined templates for fine-tuning, enabling SLMs to improve
recommender systems by better reasoning on richer item information.
SLM for Long-term Planning. Traditional recommender systems focus on optimizing immediate user responses,
often maximizing short-term gains but overlooking long-term engagement. This can trap users in echo chambers and
filter bubbles [107, 368]. To tackle this, integrating planning capabilities into recommendations to balance immediate
and long-term outcomes is vital. LMs, with their extensive knowledge and reasoning abilities, are expected to enhance
planning capabilities. BiLLP [309] adopts a hierarchical learning approach with macro and micro-learning phases. In
macro-learning, a Planner and a Reflector, both as SLM instances like Llama-2-7B [339], operate; the Planner forms
long-term plans using high-level experiences, while the Reflector updates plans based on past actions. Micro-learning
uses an SLM-based Actor-Critic mechanism for personalized planning, with the Actor implementing plans and the
Critic assessing actions for long-term benefits. The use of SLMs for long-term planning, similar to their use in cold-start
scenarios, remains underexplored and merits further research.
(a) Lifelong Behavior Sequence
has bought...
. . . . . .
Large Language Models
Large Language Models
ùê¥ùê¥
ùêµùêµ
ùêµùêµ2
ùêµùêµ2
ùêµùêµùëòùëò
ùêµùêµùëòùëò
ùê¥ùê¥1
ùêµùêµ1
. . .
(b) Non-personalized LoRA Finetune (c) Personalized LoRA Finetune
Fig. 14. The illustration of lifelong behavior sequence
and personalized low-rank adaption (LoRA) for rec-
ommendation [455].
SLMs as a Personalized Recommender. Generative language
model-based recommender systems require integrating user knowl-
edge, typically achieved through fine-tuning. Fine-tuning techniques
like LoRA [142] can incorporate extensive knowledge across all users
by training an external module with a small number of parameters A
and B, but this approach often overlooks individual user preferences.
To address this, RecLoRA [455] utilizes Vicuna-7B [61] to integrate
personalized knowledge into SLMs/LLMs tailored for recommenda-
tion tasks, as illustrated in Figure 14. Specifically, RecLoRA maintains
a set of parallel, independent LoRA weights (Aùëñ,Bùëñ), allowing for
the customization of language model parameters to match individual
user preferences more effectively.
SLM as a Content Encoder. Language models, particularly when deep, provide an effective starting point for fine-
tuning on downstream tasks. In news recommendation systems, the representational capability of a model significantly
impacts performance. Consequently, many news recommender systems now employ language models fine-tuned on
specific datasets as text encoders. For example, Wu et al . [379] conducts pioneering work using a pre-trained language
model to enhance large-scale news recommender systems by substituting traditional news encoders with a BERT
model [83]. However, BERT may struggle to capture content as it is pre-trained on limited data. Therefore,ONCE [214]
propose using Llama-2-7B [339] as an encoder to overcome the limitations of BERT in content-based recommendations.
Additionally, the study explores the synergistic use of LLMs in recommendation systems, finding that SLMs optimized
with LoRA [142] outperform the recommendation results of systems assisted by generic LLMs such as ChatGPT.
4.1.4 SLM Applications in Web Search. Web search systems, involving retrieval and ranking, face challenges due to
the diverse web documents and search queries. Traditional keyword-matching methods often fall short because of
phrasing variations and the long-tail distribution of queries and content, complicating accurate semantic inference.
Effective integration of retrieval and ranking models is also crucial. Language models, serving as content encoders, help
overcome semantic challenges through their language understanding from pre-training [63, 89, 359]. Joint training of
Manuscript submitted to ACM
A Survey of Small Language Models 27
Passage Query
SLM as a Content Encoder
Features
Scoring head
Relevance 
score
(a) SLM as a Content Encoder (b) SLM as a Ranker
PassagesPassagesPassages
Retriever
index
Query
query
Top-k 
passages
SLM as a Ranker
Ranked Top-k 
passages
(c) SLM as a Rewriter
Query to LLM
SLM as a Rewriter
Modified Query
Retriever
Top-k 
passages
LLM
Generation
Fig. 15. Roles of SLM in Web Search.
Generation
Fig. 15. Roles of SLM in Web Search.
retrieval and ranking models addresses integration, with SLMs ranking retrieved documents and acting as re-rankers.
Additionally, SLMs serve as rewriters in scenarios requiring enhanced query understanding. Thus, in web search, SLMs
fulfill three roles: (1) content encoder , (2) ranker, and (3) rewriter, as depicted in Figure 15. Next, we give details.
SLM as a Content Encoder. Text embeddings are vector representations that encode semantic information, widely
used in retrieval; SLM-based dense retrieval utilizes pre-trained deep language understanding to effectively tackle
semantic challenges. H-ERNIE [63] employs a hierarchical model that encodes queries and documents at multiple
granularity‚Äîcharacter, word, and phrase‚Äîto improve specificity and relevance in web search results by aggregating
finer details into coarser layers, addressing issues like ambiguous queries.Implicit Interaction ( ùêº3) [89] uses BERT [83]
as a content encoder, generating implicit pseudo-queries from passages to enable high online efficiency with offline
caching of passage vectors. However, ERNIE and BERT-style models overlook advancements in SLMs such as context
length extension [292]. Thus, Peng et al . [272] employs LLaMa-7B [338] and Vicuna-7B [61] as semantic encoders for
embedding retrieval, demonstrating improved performance through soft prompt tuning. CoCondenser [109] addresses
sensitivity to noisy data and large batch requirements during dense retriever training. Using the Condenser architecture
with Transformer blocks, the model condenses information into dense vectors effectively.
SLM as a Ranker. The reranking task improves the order of multiple candidates to enhance retrieval quality because
rerankers are more accurate than embedding retrievers. InPars (Inquisitive Parrots for Search) [36] employs the T5
base 220M [284] as a re-ranker to enhance the BM25 retriever [291]. Initially, BM25 selects 1K candidates, re-ranked
by a fine-tuned T5 model (monoT5) adapted as a binary classifier to assess document-query relevance. Training data,
generated by GPT-3 [37], formulates queries and selects random negative examples. Experiments show the monoT5-
enhanced retriever significantly outperforms GPT-3; for example, it achieves a 0.3599 MAP score on the TREC-DL 2020
dataset [69], surpassing GPT-3‚Äôs 0.3163.
SLM as a Rewriter. Queries to the retriever, typically just a few keywords, may reveal a knowledge gap between
the query and the knowledge needed for effective retrieval, thus limiting performance. To address this, the ‚Äúrewrite-
retrieve-read‚Äù framework [233] uses T5-large [284] to bridge the knowledge gap in queries by rewriting them for
more effective retrieval. This rewriter, trained via reinforcement learning with downstream LLM performance as a
reward, outperforms general LLM rewrites. For example, it achieves a 45.97 F1 score on HotpotQA, surpassing the
generic LLM‚Äôs 43.85 F1 score.
Manuscript submitted to ACM
28 Fali Wang, et al.
4.1.5 SLM Applications in Mobile-device. The use of cloud-based LLMs on devices raises privacy concerns and their
large size limits real-time responses in urgent scenarios such as medical emergencies. To overcome these issues,
researchers are creating smaller, domain-specific models (SLMs) that offer accurate results and suit mobile use. This
subsection discusses SLM applications on mobile devices, focusing on three aspects: (1) software API calls, (2) mobile
control, and (3) basic NLP applications.
SLM for Tool Learning. Integrating LLMs with APIs enhances capabilities but incurs high training costs, prompting
a shift to smaller, task-specific models that cut costs but risk errors. In response,Octopus [52] uses a diverse dataset from
over 30K APIs and curriculum learning [217] to improve API function accuracy. This method boosts API performance
in models like Codellama-7b [ 292] and Google‚Äôs Gemma series [ 332]. PhoneLM-1.5B-Call [417] is fine-tuned on
DroidCall [389] datasets and achieves comparable performance compared to GPT-4o-mini [262]. ùõº-UMI [307] employs
SLMs as planners, callers, and summarizers within multi-agent systems, outperforming a single LLM in tool uses.
Goal: Book an appointment for the dentistry department 
tomorrow.
Role: Given a mobile screen and a question, provide the action 
based on the screen information.
Previous Actions: 
step_id:0 action_type:TYPE typed_textHospital
step_id:1 action_type:DUAL_POINT ui_textappointment
ui_type: TEXT
‚Ä¶
Environment: 
id:0 ui_text:<ui_type
id:1 ui_textPayment ui_type
‚Ä¶
AI Agent
Action_type:
DUAL_POINT_ui_text: Outpatient appointment. ui_typeid:16
Location:
Position: [100, 252, 300, 500] Xpath: view[0]/view[2]/text[0]
Generate 
Prompt
Execute 
Action
Input
Output
Fig. 16. An example workflow for an automated execution tool [ 87]. The
screenshot in the left is taken from [87].
SLM for Mobile Control. LM agents facilitate
user-device interactions through taps, gestures, and
text, automating tasks and enhancing user hands-
free convenience. Unlike traditional developer-
based approaches that require extensive developer
effort to design interfaces and translate commands
into API calls, LMs offer scalable automation via
GUI-based text contents. MobileAgent [87] inte-
grates instructions and Standard Operating Proce-
dures (SOP) to improve SLMs for mobile control. As
shown in Figure 16, it processes goals (e.g., book-
ing a dental appointment) by analyzing screens,
ing a dental appointment) by analyzing screens,
queries, prior actions, and UI elements, forming
prompts to generate outputs and execute actions (e.g., selecting text, XPath). Fine-tuning Qwen-7B [23] on AIA medical
data, it outperforms GPT-4 [2] on AitW [290], a key mobile benchmark, without extra inference costs. Carreira et al .
[42] run a small offline model on mobile devices, fine-tuned with ChatGPT-3.5 data, enabling tasks like calls and web
searches. RedPajama-INCITE-Chat-3B-v1 Computer [67], selected for its size and chat features, uses native code and
quantization, performing well on 6GB and 4GB Android devices.
Fig. 17. An illustration of Vicuna-7B-powered mobile task automation
[42] shows a user asking to be reminded about doing laundry on Aug 17.
The figure is taken from [42].
AutoDroid [376] improves Android app inter-
actions via GUI automation. Figure 17 shows LLM-
powered task automation (e.g., setting a laundry
powered task automation (e.g., setting a laundry
reminder for Aug 17) in four steps: (1) click ‚ÄôNew
Event‚Äô, (2) enter ‚Äôlaundry‚Äô in ‚ÄôTitle‚Äô, (3) click ‚ÄôSave‚Äô,
(4) finish. Using Vicuna-7B and app-specific knowl-
edge, AutoDroid generates privacy-filtered prompts
for tasks. On its DroidTask benchmark, it outper-
forms GPT-3.5 (34.7%) and GPT-4 (54.5%) with 57.7%
accuracy. M4 (composable mobile foundation
model) [423] introduces a 9.2B parameter model
Manuscript submitted to ACM
A Survey of Small Language Models 29
(7.5GB memory) for mobile AI tasks, managed by the OS and hardware. Currently limited to high-end devices, its applica-
bility will expand with increasing mobile memory/storage. These works highlight customizing smaller, domain-specific
SLMs to address memory limits while preserving functionality in mobile environments.
Data Synthesis
Response: "Based on your symptoms, 
you may..."
LLM
New Input: Doctor, I have been 
experiencing some discomfort and
experiencing some discomfort and 
curvature of my spine. Could you 
recommend some medications that 
could help alleviate my symptoms?
A prompt asks 
LLM to generate a 
semantically 
similar text based 
on input
Response: "Based on your symptoms, 
you may..."
Input: "Doctor, I have been diagnosed 
with scoliosis. What medication do I 
need to take?
Keep Same
Generated dialogue sets
Original dialogue set
For each selected data
Collect Synthesized Data
Prompt "Feedback"
Response
User
Large Language Models
Response
User
Large Language Models
Fine-tune User 
Annotation
Data Buffer Domain Data. Embedding Domain Data. Embedding...
Stop 
update
User-Generated 
Data
Get Text Domain Tag
Calculate EOE, DSS, IDD
Domain Data (EOE, DSS, IDD)
Forward
Data Selector
User-Generated Data
Input
Fig. 18. Overview of fine-tuning SLMs with synthesized and user
data [278]. Data Synthesis generates semantically similar text via
prompts, creating dialogue sets. Data Selection processes user data,
tags domains, and calculates metrics (EOE, DSS, IDD). Selected data
fine-tunes SLMs with user annotations. The iterative framework
refines SLMs through continuous data generation and selection.
SLM for Basic NLP Applications on Devices Per-
forming basic NLP tasks such as text rewriting directly on
the device can enable personalization while ensuring pri-
vacy. The sparse annotation on devices is a challenge.Qin
et al . [278] utilizes self-supervised data selection and
synthesis for on-device fine-tuning, leveraging sparse an-
notations and limited storage effectively. This approach,
demonstrated in Figure 18, employs the Llama-3B model
[338] and the LoRA fine-tuning method [142], enhancing
personalization by efficiently managing data through met-
rics including embedding entropy and domain-specific
scores. In mobile text rewriting, Zhu et al . [458] train
the compact Palm 2-XXS model [ 14] using data gener-
ated by the larger Palm 2-L to ensure user privacy and
accommodate device constraints. On its new benchmark,
MESSAGEREWRITEEVAL, Palm 2-XXS achieves a BLEU
score of 34.59, outperforming LLaMa-7B (16.65) [ 338].
Tests on the Samsung S23 show lower latency (29 tokens/s) than a 4-bit LLaMa-7B on a MacBook M1 Pro (18-22
tokens/s), proving its mobile efficiency for text rewriting.
Insights: We draw several key insights from the development of task-specific SLMs:
‚Ä¢There is considerable potential in enhancing the efficiency and effectiveness of small models by integrating self-adaptive
techniques with further fine-tuning and optimization of RAG-based methods.
‚Ä¢The growing relevance of SLMs in coding highlights their cost-effectiveness and efficiency as alternatives to LLMs,
providing quick processing and easy fine-tuning for specialized tasks; while LLMs handle complex tasks well, SLMs,
optimized and fine-tuned on specific data, are increasingly essential in resource-limited settings.
‚Ä¢SLMs significantly enhance recommendation systems due to their robust generalization, reasoning abilities, and in-context
learning, addressing key challenges such as cold-start problems and distribution biases. They support long-term planning,
replace traditional encoders, and use parallel low-rank parameters to inject personalized user knowledge effectively.
‚Ä¢SLMs play a crucial role in web search such as document encoding, text reordering, and query rewriting, often outper-
forming LLMs through techniques such as supervised fine-tuning, soft prompt tuning, unsupervised contrastive loss, and
reinforcement learning, thereby enhancing adaptability and efficiency.
‚Ä¢SLMs are utilized on mobile devices primarily for privacy and memory constraints, with applications in API calls and
mobile control; they are typically developed by generating data with LLMs and fine-tuning with SLMs, or by using local
SLMs to handle privacy with LLMs boosting performance, and their training involves innovative techniques like learning
from data streams and managing non-IID time series data.
Manuscript submitted to ACM
30 Fali Wang, et al.
Table 6. On-device Deployment Optimization Techniques
Aspect Representative Work Key Point
Memory
Efficiency
Optimization
EDGE-LLM [422] Edge LLMs use LUC and adaptive tuning for efficiency
LLM-PQ [448] Optimize quantization and layer partitioning for complex setups.
AWQ [207] Preserve key weights based on activation distribution.
MoE-I2 [404] Prune less important experts in MoE.
MobileAIBench [253] Evaluation
EdgeMoE [416] Load experts on activation, tripling memory savings.
GEAR [165] Enhance KV cache quantization by integrating error-reduction techniques.
DMC [256] Adaptively compress KV cache, optimizing storage efficiency.
Transformer-Lite [193] Optimize KV cache to reduce redundancy and memory use.
LLMaaS [419] LLMaaS manages apps via chunk-wise KV cache optimization on mobiles.
Runtime
Efficiency
Optimization
mllm-NPU [395] On-device NPU (neural processing units) to reduce prefill latency.
COST-EFF [305] Distill a multi-exit model from the original PLM.
LLMCad [394] Use SLM for fast token generation and cloud verification.
EdgeMoE [416] Predict expert needs, boosting inference speed and reducing latency.
LinguaLinked [447] Optimize data flow and load, enhancing multi-threading efficiency.
4.2 SLM Deployment on Mobile and Edge Devices
On-device applications benefit uniquely from the memory-saving efficiency and rapid runtime performance of SLMs,
which offer advantages over LLMs. However, devices with extremely limited resources may still struggle with the
parameter sizes of SLMs. To ensure both memory and runtime remain within acceptable range while still maintaining
performance, it is crucial to integrate technologies that facilitate the deployment of SLMs on resource-constrained
devices. The primary challenge for memory-efficient technologies arises from the inherent size of the SLMs and their
associated caches. To address this, we survey existing works focused on compressing SLMs and their caches. Additionally,
the large size of models significantly impacts runtime efficiency due to the extensive computing workload and potential
weight transfers between the memory buffer and RAM/GPU memory. Other challenges include switching the Mixture
of Experts between CPU and GPU memory and managing resource scheduling when deploying SLMs across multiple
local devices. Therefore, in this subsection, we review representative works that address these challenges under two
aspects: memory efficiency optimization and runtime efficiency optimization , as systematically compiled in Table 6.
4.2.1 Memory Efficiency Optimization. Memory efficiency involves minimizing the memory usage of both the model
and the KV cache during deployment. This is typically achieved through model compression techniques such as
quantization [207, 285, 422, 448], the cache of MoE experts [416], and KV cache compression [165].
Compression on model parameters. Quantization, a common method for deploying SLMs, lowers numerical
precision to significantly save memory while preserving accuracy. We detail quantization strategies in Sections 2.3.3
and 3.5, focusing here on representative works for edge devices. EDGE-LLM [422] adapts LLMs for edge devices using
a Layer-wise Unified Compression (LUC) method that combines layer-specific pruning and quantization to reduce
computational demands and an Adaptive Layer Tuning and Voting scheme to optimize memory use while ensuring
performance. Meanwhile, LLM-PQ [448] addresses quantization and model layer partitioning for heterogeneous
clusters, incorporating a Cost Model and an Indicator Generator to optimize bit-width assignment and layer partitioning
through integer linear programming, enhancing quantization for complex computational setups. Activation-aware
Weight Quantization (AWQ) [207] is a hardware-friendly, low-bit, weight-only quantization method for on-device
LLMs, preserving essential weights based on activation distribution to minimize quantization loss. MoE-I2 [404] prune
less important experts and applies low-rank decomposition to the remaining experts to further optimize efficiency.
Manuscript submitted to ACM
A Survey of Small Language Models 31
Cache of MoE Experts. Beyond standard quantization, which reduces storage for all model parameters, another
strategy involves caching a mixture of experts (MoE). Driven by the fact that memory storage is more cost-effective and
scalable than computing capacity, the MoE architecture [154] boosts performance while minimizing computational
costs by activating only portions of the LLM as needed. However, this approach incurs significant memory overhead,
making it impractical for edge device memory constraints. For example, Switch Transformers [100], with 32 experts per
layer, require 54GBs of memory for inference, exceeding the capacity of most edge devices. Yi et al. [416] notes that in
the Switch Transformers model, although most of the model weights (86.5%) are attributed to experts, these weights
account for only a small fraction (26.4%) of the computations. To address this,EdgeMoE [416] introduces a method
where experts are loaded into an expert memory buffer only when activated, achieving approximately 3√ómemory
savings compared to the baseline where all weights are held in memory.
KV Cache Compression . When serving LMs for inference, using a KV cache is common practice to avoid intensive
recalculations and speed up generation [276]. However, cache memory consumption escalates with model size and
sequence length, posing a challenge for edge devices. To manage this, offloading techniques transfer KV caches to
CPU memory [12, 308], although this can introduce significant overhead due to the switching costs between GPUs
and CPUs. Token dropping compresses cache size by keeping only key tokens, often identified by low attention scores
[111, 220, 444]. However, this method struggles with complex tasks, especially at high compression levels, due to
increased estimation errors in compressed KV values. GEAR [165] addresses these issues by enhancing KV cache
quantization with error-reduction techniques, including: (i) quantizing caches of similar magnitudes to ultra-low
precision, (ii) using a low-rank matrix for efficient quantization residual approximation, and (iii) employing a sparse
matrix for correcting outliers. This approach separates coherent from incoherent approximation errors, enabling
near-lossless KV cache compression and achieving up to 2.29√ópeak memory reduction.
Besides, Dynamic Memory Compression (DMC) [256] adaptively compresses the KV cache by either adding
new key and value representations directly or blending them with the top cache item using a weighted average.
Transformer-Lite [193] tackles the redundancy of storing the KV cache twice in model inputs and outputs, which
increases memory use. It optimizes storage by allocating a large tensor based on the maximum sequence length needed
for inference. Sub-tensors are then created from this main tensor at different address offsets to serve as input and output
KV caches, allowing direct writing to the correct locations during inference and removing extra copying steps.LLMaaS
[419] introduces LLM as a Service for mobile devices, managing all apps through LLMS. This system uses chunk-wise
KV cache compression and swapping, enabling efficient context switching within memory constraints. By segmenting
the KV cache into independently compressed and swapped chunks, LLMS balances memory use and I/O bandwidth
better than token-level or context-level management.
4.2.2 Runtime Efficiency Optimization. The goal of decreasing computing workload aligns with enhancing memory
efficiency through methods such as quantization, as mentioned in the previous section. Decreasing model weight
precision or reducing the number of weights naturally lowers latency. Other runtime efficiency techniques of minimizing
inference latency involve, reducing prefill latency, early exits, large and small model collaboration, decreasing switching
time in MoE, and reducing latency in distributed SLMs.
Reducing prefill latency. mllm-NPU [395] is the first LLM inference system that leverages on-device NPU (neural
processing units) to reduce prefill latency and energy consumption. It incorporates a chunk-sharing graph, shadow
outlier execution, and out-of-order subgraph execution to enhance NPU offloading efficiency. Experiments have shown
mllm-NPU‚Äôs superior performance benefits, including up to 43.6√ó speedup and 59.5√ó energy savings.
Manuscript submitted to ACM
32 Fali Wang, et al.
Dynamic Early Exits A decoupled runtime saving technique is dynamic early exits. Originating from BranchyNet
[335], which introduces exit branches after specific convolution layers in the CNN model, this concept has been adapted
for PLMs as Transformer layer-wise early exiting [391]. Early exiting enables dynamic acceleration during inference
and reduces temporal overhead by allowing exit without passing through all model layers. To address the inconsistency
issue arising from exiting at different layers, COST-EFF [305] distills a multi-exit model from the original PLM.
Large and Small Model Collaboration Model collaboration, deploying SLMs on devices with cloud-based LLM
support, enhances runtime efficiency. LLMs will increase latency when directly deployed via mobile engines like
llama.cpp due to a large number of computing operations. LLMCad [394] addresses this by using a real-time, memory-
resident SLM for simple tokens such as determiners and punctuation. The SLM generates tokens, while a cloud-based
LLM verifies and corrects them, speeding up the process. LLMCad enhances token generation up to 9.3√óby pairing the
memory-resident SLM, Llama 2 7B, with the cloud-based LLM, Llama 2 13B, cutting latency from 16.2 to 3.9 seconds on
Xiaomi Pro for TruthfulQA tasks [208].
Reducing MoE Switching Time. To reduce latency in MoE architectures caused by frequently switching experts in
limited device memory, EdgeMoE [416] enhances runtime efficiency by preemptively predicting which expert will be
needed, based on the observed long-tail distribution of unbalanced expert activations. It utilizes a statistical model, built
offline, to estimate expert activation probabilities in transformer layers from previous activations. During inference,
EdgeMoE preemptively loads the most likely needed expert, accelerating inference by 1.11√óto 2.78√óand significantly
reducing latency. For instance, in a switch transformer model with 8 experts, latency drops from approximately 0.7s to
0.3s, outperforming the baseline method that preloads experts based on hit ratios.
Reducing Latency in Distributed SLMs. Distributing an SLM across smaller devices reduces the need for extensive
model compression while preserving accuracy. However, this approach faces challenges that incur latency such as
managing diverse device capabilities, handling data dependencies between model segments, and adapting to dynamic
resource availability. To address these issues,LinguaLinked [447] addresses these issues by optimizing model assign-
ment to match device capabilities and minimize data transmission, implementing runtime load balancing to redistribute
tasks and prevent bottlenecks, and enhancing communication for efficient data exchange between segments. With
multi-threading, the system improves, achieving acceleration rates between 1.73√óand 2.65√ófor both quantized and
full-precision models.
Insights: We draw several key insights from the deployment of SLMs:
‚Ä¢ Model size remains a bottleneck for both memory and runtime efficiency. A common solution is model quantization, which reduces model
precision to save memory and lessen computing workload, thereby boosting inference speed [207, 223, 253, 285, 422, 448]. Similarly, KV
cache compression also helps achieve these efficiency gains [165, 193, 256, 419].
‚Ä¢ Mixture of Experts (MoE) is commonly used in SLMs to enhance performance using the same computing resources, but it results in increased
memory usage. To address this, only activated experts are loaded into the memory buffer while the majority are stored cold on disk. However,
the cost of switching can slow down inference. Designing a preemptive expert pre-load strategy could therefore accelerate the inference
[416].
[416].
‚Ä¢ Model collaboration between local SLMs and cloud-based LLMs enhances both memory and runtime efficiency by using smaller models on
local devices, which are then verified by cloud LLMs to ensure performance is maintained. Using SLMs locally reduces memory usage and
shortens the inference time from the local model. However, internet latency and delays in cloud LLM inference can still introduce latency.
Verifying SLM outputs every ùëÅ tokens using LLMs can effectively mitigate this latency [394].
‚Ä¢ One deployment approach involves deploying SLMs/LLMs across multiple trusted local devices to maintain original performance while only
loading a fraction of the model weights. However, this method can incur latency due to varying device capabilities and resource scheduling
challenges. To address these issues, optimizing model assignment to align with device capabilities and minimizing data transmission are
effective strategies [447].
Manuscript submitted to ACM
A Survey of Small Language Models 33
5 GENERIC AND DOMAIN-SPECIFIC SMALL LANGUAGE MODELS
This section investigates small language models (with fewer than 7 billion parameters) in both general and specific
domains. It details the methods of obtaining these small language models, the datasets, and the evaluation tasks,
exploring the techniques for acquiring SLMs through compression, fine-tuning, or training from scratch. Additionally,
we summarize the representative small language models, as detailed in Table 7 and 10.
5.1 Generic-domain SLMs
Overview. SLMs, with fewer parameters than LLMs, enhance computational efficiency in pre-training, fine-tuning,
and inference, reducing memory and energy demands‚Äîcrucial for resource-limited environments. Their compact,
localized nature boosts privacy, personalization, and response times, making them ideal for low-power edge devices.
Therefore, SLMs are attracting increasing attention, and various models are being developed. Table 7 summarizes
current representative generic-domain 42 SLMs/SLM families. Although all chosen SLMs have similar architectures,
they vary in specific training datasets and techniques, with some datasets not being openly available. Taking the latest
Llama 3.2 1B models [7] in Figure 19 as an example, its parameter size and use of filtered high-quality training data,
pruning-based initialization, knowledge distillation pre-training tasks, and training techniques such as Supervised
Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO) distinguish it from others.
Model Specs
: Meta AI
: 1.23B
: 9/25/2024
: Llama 3.2 Community License
: Commercial and Research use 
: EN, DE, FR, IT, PT, HI, ES, TH
: Text
: Transformer                    
: 16
: 2048
: GQA
: 32
: SiLU
: Shared Embeddings
: TikToken-based
: 128K
: 128256
: Yes
: TikToken-based
: 128K
: 128256
: Yes
: https://huggingface.co/meta-
llama/Llama-3.2-3B 
Model Specs
Developer
Params
Release Date
License
Intended Use
Language
Input Modality
Architecture
Layer Number 
Hidden Size
Attention
Attention Heads  
Activation
Architectural Tech
Tokenizer
Context length
Vocab Size 
Open Source?
Code
Llama 3.2 1B Model
Dataset Specs
Open Training Dataset?
Training Data
Training Data (tokens)
Data Freshness
Training Specs
Pre-text tasks
Pre-training methods:
Hardware
GPU Hours
Pre-training methods:
Hardware
GPU Hours
Post-training methods:
Evaluation Specs
Benchmark
Motivation
Metric
Performance
Responsibility
Safety
Evaluations
Dataset Specs
: No
: Unk
: 9T
: December 2023
Training Specs
: Knowledge Distillation
: Pruning-based initialization
 KD from Llama 3.1 8B & 70B
: H100-80GB
: 370K
: SFT, RS, DPO
Evaluation Space
: MMLU (English)
: Test Multi-subject Knowledge
: Macro Avg
: 32.2
Ethical Consideration
: Red Teaming, Llama safeguards 
: Red Teaming, CBRNE,
: Red Teaming, CBRNE, 
  Child Safety, Cyber Attacks
Fig. 19. Llama 3.2 1B model card
5.1.1 Architecture Design. From Table 7, we observe several trends in component choices for SLMs:
(1) Recent SLMs frequently employ Grouped Query Attention (GQA) in self-attention mechanisms because it can
reduce computational complexity. GQA achieves this by sharing query representations across multiple heads while
keeping key and value representations separate. This approach aligns with the goals of SLM to enhance efficiency
without compromising functionality.
Manuscript submitted to ACM
34 Fali Wang, et al.
Table 7. High-level Overview and Training Details of Generic-domain Small Language Models. #Params means Parameter amounts.
">" indicates parameters larger than 7B.
Model #Params Date Paradigm Domain Training Datasets Training Techniques
PhoneLM [417] 0.5B; 1.5B 2024.11 Pre-train Generic DCLM-baseline [192], StarCoderData [195];
OpenWebMath [267], Dolma-algebraic and
Dolma-arXiv [315]
RoPE, MHA, Gated FFN, RMSNorm, ReLU, FSDP, Flash Attention
2, ZeRO
2, ZeRO
Llama 3.2 [7] 1B; 3B 2024.9 Pre-train Generic no release (9T tokens) GQA, SiLU, Multilingual Text and code, Shared embedding, Prun-
ing, Distillation, SFT, RLHF, RS, DPO
Qwen 1 [23] 1.8B; 7B; > 2023.12 Pre-train Generic no release MHA; RoPE; SwiGLU; RMSNorm
Qwen 1.5 [23] 0.5B; 1.8B; 4B; 7B; > 2024.2 Pre-train Generic no release MHA; RoPE; SwiGLU; RMSNorm; Multilingual support
Qwen 2 [402] 0.5B;1.5B; 7B; > 2024.6 Pre-train Generic no release GQA; RoPE; SwiGLU; RMSNorm; Multilingual support
Qwen 2.5 [402] 0.5B; 1.5B; 3B; 7B; > 2024.9 Pre-train Generic no release GQA; RoPE; SwiGLU; RMSNorm; Multilingual support; Larger
corpus
Gemma [332] 2B; 7B 2024.2 Pre-train Generic Unknown MHA, RoPE, GELUtanh
Gemma 2 [333] 2B; > 2024.7 Pre-train Generic Unknown GQA; RoPE;GELUtanh; Alternating Local and Global Attention;
Logit Soft-Capping; RMSNorm for Pre and Post-Normalization
SmolLM [9] 135M; 360M; 1.7B 2024.7 Pre-train Generic smollm-corpus [27] GQA, trapezoidal LR scheduler
H2O-Danube3 [274] 500M; 4B 2024.7 Pre-train Generic Unknown Three different training stages with different data mixes
Fox-1 [334] 1.6B 2024.6 Pre-train Generic Unknown (3T tokens) GQA; Deep architecture
Rene [113] 1.3B 2024.5 Pre-train Generic Dolma-1.7 [315] Mamba-2 layers, sliding-window attention (SWA)
MiniCPM [143] 1.2B; 2.4B 2024.4 Pre-train Generic Dolma [315]; C4 [284]; Pile [84]; stack [174];
StarCoder [195]; UltraChat [86]; OssInstruct
[373]; EvolInstruct [392]
[373]; EvolInstruct [392]
Warmup-Stable-Decay (WSD) learning rate scheduler
CT-LLM [92] 2B 2024.4 Pre-train Generic MAP-CC Chinese, MHA, RoPE, SwiGLU, RMSNorm
OLMo [116] 1B; 7B 2024.2 Pre-train Generic Dolma [315] 1 SwiGLU; RoPE, Non-parameteric Layer Norm
TinyLlama [439] 1B 2024.1 Pre-train Generic SlimPajama [314] and StarCoder [195] GQA, SiLU, FSDP, Flash Attention [73], xFormers [184]
Phi-1 [120] 1.3B 2023.6 Pre-train Coding CodeTextBook [120] 2 MHA, GELUtanh, RoPE, FlashAttention
Phi-1.5 [200] 1.3B 2023.9 Pre-train Generic CodeTextBook [120]; Synthetic Datasets (20B) MHA, GELUtanh, RoPE, FlashAttention, Deep ZeRO Stage 2
Phi-2 [155] 2.7B 2023.12 Pre-train Generic CodeTextBook [120]; Synthetic Datasets (20T) MHA, GELUtanh, RoPE, FlashAttention, Deep ZeRO Stage 2
Phi-3 [1] 3.8B; 7B; > 2024.4 Pre-train Generic Scaled-up dataset from phi-2 MHA, SiLU, RoPE, FlashAttention, Deep ZeRO Stage 2
Phi-3.5 [1] 3.8B; 4.2B; 6.6B 2024.4 Pre-train Generic more multilingual and long-text data Multilingual; Vision; MHA, SiLU, RoPE, FlashAttention, ZeRO 2
OpenELM [241] 270M; 450M; 1.1B; 3B 2024.4 Pre-train Generic RefinedWeb [269], deduplicated PILE [ 108],
partial RedPajama [ 67], partial Dolma v1.6
[315]
No biases in FC layers; Pre-norm: RMSNorm; Pos encoding:
RoPE; Attention: GQA; FFN: SwiGLU; Tokenizer: LLaMA-style
MobiLlama [336] 0.5B; 0.8B 2024.2 Pre-train Generic LLM360 Amber 3 GQA; SwiGLU; Parameter-sharing
MobileLLM [223] 125M; 350M 2024.2 Pre-train Generic Unknown (1T tokens) SwiGLU FFN, deep and thin architectures, embedding sharing,
and grouped query attention
StableLM [340] 3B; 7B 2023.4 Pre-train Generic RefinedWeb [269], RedPajama [67], the Stack
[174], OpenWebText [114], OpenWebMath
[267], and partial CulturaX [257]
MHA; SiLU; Fine-tuning; DPO; Self-knowledge; RoPE; Layer-
Norm; no Biases
Norm; no Biases
StableLM 2 [26] 1.6B 2024.2 Pre-train Generic
Cerebras-GPT [84] 111M; 256M; 590M;
1.3B; 2.7B; 6.7B; >
2023.4 Pre-train Generic Pile [108] MHA; GELU; Maximal Update Parameterization
Pythia [31] 14M;70M;160M;410M;
1B;1.4B;2.8B;6.9B;>
2023.4 Pre-train Generic Pile [108] MHA; GELU; Flash Attention [74]; RoPE [318]; ZeRO [286]
BLOOM,
BLOOMZ [181]
560M; 1.1B; 1.7B; 3B;
7.1B; >
BLOOM,
BLOOMZ [181]
560M; 1.1B; 1.7B; 3B;
7.1B; >
2022.11 Pre-train Generic ROOTS [180] and 13 programming languages MHA; GELUtanh; ALiBi Positional Embedding [277], Embed-
ding LayerNorm [80]
Galactica [330] 125M; 1.3B; 6.7B; > 2022.11 Pre-train Scientific Open-access scientific materials (106B tokens)
but not released
MHA; GeLU; Learned Positional Embeddings
OPT [440] 125M; 350M; 1.3B;
2.7B; 5.7B
2022.5 Pre-train Generic Pile [108] and PushShift.io Reddit [24] MHA; ReLU
XGLM [209] 1.7B; 2.9B; > 2021.12 Pre-train Generic CC100-XL -
GPT-Neo [33] 125M; 350M; 1.3B;
2.7B
2021.5 Pre-train Generic Pile [108] -
Megatron-gpt2 [310] 355M; 2.5B; > 2019.9 Pre-train Generic Wikipedia [83], CC-Stories [341], RealNews
[430], OpenWebtext
-
MINITRON [252] 4B; > 2024.7 Distillation;
Pruning
Generic 8T tokens in Nemotron-4 [266] LR WSD Scheduler
Orca 2 [247] 7B 2023.11 Distillation Generic Orca 2 dataset LLaMA-2-7B based; prompt erasing
Orca [251] 13B 2023.6 Distillation FLAN-v2 [225] From ChatGPT and GPT4, Explanation tuning; Progressive
Learning
MINIMA [434] 3B 2023.11 Distillation Generic Pile [108], Wudao [424], GitHub [67] From Llama-2-7B, Zero2, Flash Attention, Optimal teacher size
Dolly-v2 [68] 3B; 7B; > 2023.4 Instruction
tuning
Generic Databricks-dolly-15k [68] from pythia
LaMini-LM [171] 61M-7B 2023.4 Distillation Generic LaMini instruction dataset a collection of SLMs distilled from ChatGPT-generated 2.58M
instructions.
instructions.
Specialized
FlanT5 [105]
250M; 760M; 3B 2023.1 Instruction
Tuning
Generic
(math)
GSM8K Base model is FlanT5
Manuscript submitted to ACM
A Survey of Small Language Models 35
(2) The choice of activation function should balance model capability and efficiency. ReLU, known for its efficiency,
introduces greater sparsity to the model, which facilitates faster coefficient calculations for inference acceleration. In
contrast, SwiGLU‚Äôs parameters are learned during training, allowing the model to dynamically adapt to diverse tasks
and datasets, thereby enhancing model capabilities and establishing it as a state-of-the-art option. SiLU, situated
between these two, is favored for its balance of computational efficiency and model performance.
(3) RMS normalization is commonly used than layer normalization due to its reduced computational demands.
A basic introduction to these options is provided in Section 2. Apart from component choices, there are notable
innovations in architecture for SLMs:
‚Ä¢Mobilellm [223] highlights that deeper models are more effective than wider ones for improving performance.
‚Ä¢Embedding sharing [440] is crucial as embedding layers often constitute over 20% of a model‚Äôs parameters‚Äîfor
example, with 512 dimensions and a 32k vocabulary, each layer holds 16M parameters in a 125M-parameter model.
Smaller models often reuse these weights for both input and output layers, enhancing efficiency and compactness.
‚Ä¢Layer sharing [223] increases hidden layers in small Transformer models without additional storage costs.
‚Ä¢Shared FFNs [336] make up about 65% of all trainable parameters, with attention mechanisms and heads accounting
for the rest. Sharing FFN parameters across all transformer layers of an SLM is proposed to increase efficiency.
‚Ä¢Architecture search ahead of pre-training. PhoneLM [ 417] proposes a principle for constructing on-device small
language models: searching for a resource-efficient architecture on a given hardware to optimize the speed-capacity
trade-off before pretraining. This approach inspires the tailored selection of architectural components for on-device
SLMs, based on specific compositional requirements such as computing efficiency, model capability, and safety.
A detailed description of these architectural designs can be found in Section 3.1.
5.1.2 Training Datasets. From Table 7, we can observe a set of widely used training datasets in SLM development. We
provide the details below:
provide the details below:
‚Ä¢Pile [108]: It comprises 22 smaller, high-quality diverse corpora from various domains, such as Pile-CC, PubMed
Central, ArXiv, GitHub, and FreeLaw, designed to offer a comprehensive foundation for language model training.
The dataset contains 207 billion tokens and totals 825 GB.
‚Ä¢C4 (Colossal Clean Crawled Corpus) [284]: This dataset includes 350 billion tokens, representing a cleaned version
of the Common Crawl web corpus, intended to capture a wide snapshot of the internet 4.
‚Ä¢The Stack [174]: It contains 6 trillion tokens of source code from over 300 programming languages, useful for
developing code-centric AI applications. Python-edu in smollm-corpus [27] consists of Python files that are scored
4 or more by the educational code model and are extracted from the stack-v2-train dataset.
‚Ä¢StarCoder [195]: It features 35 billion tokens, predominantly Python code, aimed at programming language under-
standing and generation.
‚Ä¢RedPajama [67]: This dataset encompasses 1.2 trillion tokens derived from over 100 billion text documents, processed
using the CCNet pipeline to ensure a rich collection of web texts.
‚Ä¢RefinedWeb [269]: This dataset includes 5 trillion tokens of high-quality, extensively filtered web data, offering a
valuable resource for training web-aware models.
valuable resource for training web-aware models.
‚Ä¢PushShift.io Reddit [24]: A around 5 billion tokens resource for social media data collection, analysis, and archiving,
specifically of Reddit data, aiding research into social media dynamics.
4Available at https://commoncrawl.org
Manuscript submitted to ACM
36 Fali Wang, et al.
‚Ä¢CulturaX [257]: It comprises 6.3 trillion tokens across 167 languages, supporting the development of models with
extensive linguistic and cultural understanding.
‚Ä¢FineWeb[268], a large-scale (15-trillion tokens, 44 TB disk space) dataset for LLM pretraining. FineWeb is derived
from 96 CommonCrawl snapshots. FineWeb-Edu is a subset of FineWeb constructed using scalable automated
high-quality annotations for educational value.
high-quality annotations for educational value.
From the analysis of these datasets, we can derive several critical insights regarding the development of SLMs: (i) Data
quality is crucial for training effective SLMs, involving sophisticated filtering like removing duplicates or irrelevant
content, often with another LLM‚Äôs help. For example, the TinyStories corpus [96] is tailored for simplicity, ideal for
training models to handle straightforward narratives. RedPajama-V2 [ 67] uses the CCNet pipeline to process 30B
documents, providing quality signals and IDs for creating a 20B deduplicated dataset. (ii) Code Data: Source code
constitutes a significant component of valuable data for training models, particularly because of its structured nature and
logical content. Training on code data enhances a model‚Äôs reasoning capabilities and supports its ability to generalize
across multiple natural languages, which is crucial for applications requiring robust problem-solving and interpretation
skills in diverse coding environments [17, 104, 120, 236]
5.1.3 Training Algorithms. To enhance the alignment of SLMs with desirable properties such as safety and reasoning,
training algorithms, particularly during the fine-tuning phase, are crucial in evolving pre-trained SLMs.
‚Ä¢Direct Preference Optimization (DPO) [283] presents a simpler alternative to RLHF for optimizing language
models based on human preferences, preventing explicit reward modeling and reinforcement learning. Instead, DPO
modifies log probabilities of responses with a dynamic weighting mechanism to prevent model degradation common
in probability ratio-focused methods. The DPO loss function is:
Lùê∑ùëÉùëÇ(ùúãùúÉ; ùúãref)= ‚àíE(ùë•,ùë¶ùë§,ùë¶ùëô )‚àºùê∑

log ùúé

ùõΩlog ùúãùúÉ(ùë¶ùë§|ùë•)
ùúãref(ùë¶ùë§|ùë•)‚àíùõΩlog ùúãùúÉ(ùë¶ùëô|ùë•)
ùúãref(ùë¶ùëô|ùë•)

where ùúãùúÉ is the policy being optimized, ùúãref is the reference policy, ùê∑ includes tuples (ùë•,ùë¶ùë§,ùë¶ùëô), ùúé is the sigmoid
function, and ùõΩ scales the log-ratios between ùúãùúÉ and ùúãref, guiding the model towards human-preferred outputs.
‚Ä¢Reinforcement Learning from Contrast Distillation (RLCD) [405] aims to calibrate generative SLMs/LLMs
towards embodying harmless and beneficial characteristics. The process starts with an unaligned LM and initial
prompts, which are modified into two variants ùëù+and ùëù‚àí, intended to promote and suppress, respectively, attributes
like helpfulness and harmlessness. Upon inputting these prompts, the LM generates outputs ùëú+and ùëú‚àí, with ùëú+
automatically designated as the preferred response. This automation speeds up training by avoiding additional
evaluative scoring. The training continues under the RLHF framework.
‚Ä¢Conditioned Reinforcement Learning Fine-Tuning (C-RLFT) , by OpenChat [356], enhances model performance
by incorporating low-quality data during SFT. C-RLFT leverages varied data qualities with simple rewards (e.g., expert
data at 1 credit, sub-optimal at 0.1), using distinct prompt tokens to condition data sources, eliminating costly human
feedback. Similarly, Data Mix [274] trains on English text in three stages, reducing noisy web data progressively in
each stage in favor of higher-quality data.
‚Ä¢Explanation Tuning, proposed by Orca [251], addresses the limitations of standard instruction-based fine-tuning,
which often restricts SLMs to style imitation rather than reasoning. It uses system prompts with instructions to
direct GPT-4 to produce detailed explanations or perform step-by-step reasoning. The resulting instructions and the
responses are used as a dataset for fine-tuning SLMs to have better ability of reasoning.
Manuscript submitted to ACM
A Survey of Small Language Models 37
‚Ä¢Progressive Learning, proposed by Orca [251], aims to bridge the capability gap between Orca and the more capable
GPT-4. It starts with training on five million data points from ChatGPT, followed by one million from GPT-4. Research
suggests that an intermediate-level teacher can improve distillation effects, enabling a stepwise learning approach
where students start with simpler examples and gradually move to more complex ones, receiving improved reasoning
and explanations from a more advanced teacher.
‚Ä¢Prompt Erasing introduced by Orac 2 [247], is a distillation strategy designed to enhance the independent reasoning
capabilities of student SLMs. In this approach, a more capable teacher LLM is given intricate prompts intended to
elicit specific strategic behaviors and more precise outcomes. During the training phase, the SLM is exposed only to
the task instruction and the resulting behavior, without access to the original intricate prompts that initiate such
responses. This technique, known as Prompt Erasing, positions the student model as a cautious reasoner because it
not only learns to perform specific reasoning steps but also develops strategies for approaching tasks at a higher level.
‚Ä¢Maximal Update Parameterization ( ùúáP) optimizes control initialization, layer-wise learning rates, and activation
magnitudes to ensure stable training regardless of model layer widths. This method enhances training stability and
allows the same optimizer settings, especially learning rates, to be used across different model scales. For instance,
Cerebras-GPT [314] employs ùúáP to train its models efficiently.
5.1.4 Model Performance. To compare the performance of SLMs, we have extracted experimental results from two
recent and concurrent studies published in June 2024, OLMo [116] and MobiLlama [336], and the recently proposed
edge-device Llama 3.2 1B & 3B in September 2024 5. The extracted results are merged and shown in Table 8. From the
table, we can find that the following evaluation benchmarks are commonly used:
(1) MMLU [134]: Evaluate broad knowledge across diverse fields such as humanities, science, technology, engineering,
and management. It includes multiple-choice questions covering 57 tasks ranging from elementary mathematics to
US history, computer science, law, and beyond, with a total of 14K test samples.
(2) HellaSwag [429]: Assesses the model‚Äôs ability to select the correct ending to scenarios from multiple options,
testing common sense reasoning, including 10K test samples.
(3) ARC [65]: The AI2‚Äôs Reasoning Challenge (ARC) dataset features multiple-choice science exam questions for
grades 3 to 9, divided into Easy and Challenge partitions, with the latter containing more complex questions
necessitating reasoning. Most questions offer four answer choices. ARC includes a supporting knowledge base of
14.3M unstructured text passages, with 1.17K test samples in ARC_Challenge and 2.25K in ARC_Easy.
(4) PIQA [32]: A commonsense reasoning dataset designed to evaluate the physical knowledge of NLP models. It
presents questions (goals) that require physical commonsense for correct resolution, alongside two detailed response
options (sol1 and sol2). The dataset comprises 3,000 test samples.
(5) Winogrande [294]: a dataset structured as a fill-in-the-blank task with binary options, designed to assess common-
sense reasoning. The dataset includes 1,767 test samples by default splits.
Accuracy is used as the evaluation metric in the table. Open Language Model (OLMo) [116] is publicly available with
its training data and code 6. MobiLlama [336] is a general-purpose SLM designed from scratch, available in 0.5B and
0.8B versions. It adopts a unique approach by using a shared FFN across all transformer blocks, enhancing efficiency.
MobiLlama also show high efficiency on diverse hardware (Table 9).
5https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/
6https://allenai.org/olmo
Manuscript submitted to ACM
38 Fali Wang, et al.
Table 8. Performance of Various SLMs on Common Benchmarks: data from MobiLlama [336], OLMo [116], and Llama 3.2.
Model Size Range Model MMLU HellaSwag ARC PIQA Winogrande
<1B
gpt-neo-125m 26.0 30.3 23.0 62.5 51.8
tiny-starcoder-170M 26.8 28.2 21.0 52.6 51.2
cerberas-gpt-256m 26.8 29.0 22.0 61.4 52.5
opt-350m 26.0 36.7 23.6 64.7 52.6
megatron-gpt2-345m 24.3 39.2 24.2 66.9 53.0
LiteLlama 26.2 38.5 24.9 67.7 49.9
gpt-sw3-356m 25.9 37.1 23.6 64.9 53.0
pythia-410m 27.3 40.9 26.2 67.2 53.1
pythia-410m 27.3 40.9 26.2 67.2 53.1
xglm-564m 25.2 34.6 24.6 64.9 53.0
Lamini-GPT-LM 0.59B 25.5 31.6 24.2 63.9 47.8
MobiLlama 0.5B 26.5 52.5 29.5 72.0 57.5
MobiLlama 0.8B 26.9 54.1 30.2 73.2 57.5
1B-3B
StableLM 1.6B - 68.2 43.8 74.0 -
Pythia 1B - 44.7 33.1 69.1 -
TinyLlama 1.1B - 58.7 34.8 71.1 -
OLMo-1B - 62.5 34.5 73.7 -
OLMo 1.2B 25.9 62.5 34.5 - 58.9
Boomer 1B 25.4 31.6 22.3 - 51.0
Pythia-Dedup 1B 24.3 49.6 29.1 - 54.0
Falcon-RW 1B 25.4 63.1 35.1 - 61.9
Cerebras-GPT 1.3B 26.7 38.5 26.1 - 53.6
Cerebras-GPT 1.3B 26.7 38.5 26.1 - 53.6
Lamini 1.3B 28.5 38.1 26.6 - 50.6
OPT 1.3B 24.6 54.5 29.6 - 59.7
GPT-NEO 1.3B 24.8 48.5 31.3 - 57.1
Pythia-Deduped 1.4B 25.5 55.0 32.6 - 56.9
MobiLlama 1.2B 24.8 63.0 34.6 - 62.0
Gemma 2 2B 57.8 61.1 76.7 - -
Llama 3.2 1B 49.3 41.2 59.4 - -
Llama 3.2 3B 63.4 69.8 78.6 - -
>3B
Phi-3.5-mini 3.8B 69.0 81.4 87.4 - -
Pythia 6.9B - 63.8 44.1 75.1 -
Falcon-7B - 75.9 47.5 78.5 -
LLaMA 7B - 76.2 44.5 77.2 -
Llama 2 7B - 76.8 48.5 76.7 -
MPT-7B - 77.6 46.5 77.3 -
MPT-7B - 77.6 46.5 77.3 -
RPJ-INCITE-7B - 70.3 42.8 76.0 -
OLMo-7B - 76.4 48.5 78.4 -
Table 9. Comparison of MobiLlama 0.5B with large-base 1.2B, Llama2 7B, and Phi2-2.7B in terms of efficiency and resource consumption
on low-end hardware devices [336].
Platform Model #Params Precision Avg
Tokens/Sec
Avg Memory
Consumption
Avg Battery
Consumption /1k
Tokens
CPU
Utilization
RTX2080Ti
Llama2 7B bf16 14.85 27793 MB 135.51 mAH 31.62%
Phi2 2.7B bf16 32.19 12071 MB 59.13 mAH 24.73%
Phi2 2.7B bf16 32.19 12071 MB 59.13 mAH 24.73%
large-base 1.2B bf16 50.61 6254 MB 18.91 mAH 18.25%
MobiLlama 0.5B bf16 63.38 3046 MB 8.19 mAH 14.79%
CPU-i7
Llama2 7B 4bit 5.96 4188 MB 73.5 mAH 49.16%
Phi2 2.7B 4bit 22.14 1972 MB 27.36 mAH 34.92%
large-base 1.2B 4bit 29.23 1163 MB 10.81 mAH 30.84%
MobiLlama 0.5B 4bit 36.32 799 MB 4.86 mAH 24.64%
Snapdragon-685
Llama2 7B 4bit 1.193 4287 MB 10.07 mAH 77.41%
Phi2 2.7B 4bit 2.882 1893 MB 14.61 mAH 56.82%
large-base 1.2B 4bit 6.687 780 MB 6.00 mAH 17.15%
large-base 1.2B 4bit 6.687 780 MB 6.00 mAH 17.15%
MobiLlama 0.5B 4bit 7.021 770 MB 5.32 mAH 13.02%
Manuscript submitted to ACM
A Survey of Small Language Models 39
From Table 8 and 9, we can conclude that: (1) MobiLlama 0.5B and 0.8B demonstrate that a shared FFN design can
facilitate excellent performance in SLMs with fewer than 1B parameters, even rivaling some models in the 1B-3B range.
(2) The performance of MobiLlama 1.2B and OLMo 1.2B illustrates that advanced SLM architectures incorporating
high-quality data, SwiGLU, non-parametric layer normalization, RoPE, BPE tokenization, and a shared FFN design
can achieve competitive results among models with 1B-3B parameters. (3) MobiLlama demonstrates that SLMs can
significantly reduce resource consumption on low-end hardware devices, achieving comparable performance while
using a smaller proportion of battery power, memory, and GPU utilization. (4) Popular techniques such as pruning,
quantization, distillation, SFT, and DPO, utilized in Llama 3.2, have substantially enhanced SLM performance.
Insights: We draw several key insights from the development of generic-domain SLMs:
‚Ä¢Typical SLM architectures generally incorporate features such as GQA, gated FFN with SiLU activations, RMS normalization,
deep and thin architectures, embedding sharing, layer sharing, and shared FFNs.
‚Ä¢Although these components are widely used, current research has not yet thoroughly explored their specific contributions
within SLMs.
within SLMs.
‚Ä¢The importance of data quality in SLM research is increasingly emphasized, often considered more critical than the
quantity of data and model architectural configurations.
‚Ä¢Post-pretraining, meticulous fine-tuning is often required to enhance the safety of SLMs, involving strategies to distill
capabilities from LLMs better. Common strategies include explanatory tuning, progressive learning, and prompt erasing.
5.2 Domain-Specific SLMs
5.2 Domain-Specific SLMs
Overview. The capability of LLMs to generate human-like text has significantly captured public interest, highlighting
their potential in the field of general artificial intelligence. However, inefficiencies persist when integrating LLMs into
specialized applications due to resource constraints. Unlike the need for extensive general knowledge and capabilities,
domain-specific SLMs should focus on well-defined tasks and expertise pertinent to specific fields. For instance,
specialized models can significantly impact biomedical research and healthcare by fine-tuning for interpretable mental
health analysis, or assisting humans in legal dialogues and financial tasks through instruction tuning, showcasing their
potential transformative influence. Given the limited number of SLMs specialized in specific domains, we demonstrate
some existing SLMs individually across healthcare, science, finance, and law domains.
5.2.1 SLMs for Healthcare. Hippocrates [3] is an open-source medical language model framework with free access to
its data, codebase, checkpoints, and protocols 7. It utilizes a medical pre-training corpus from Medical Guidelines, PMC-
Patients [452], and PubMedQA-contexts [163], totaling about 300M tokens. The Hippo series, a 7B model, undergoes
continuous pre-training, instruction tuning, and RLHF. Fine-tuned on Mistral and Llama-2, it rivals 70B models in
some evaluation. For example, Hippo-Mistral 7B scores 59.9% on MedQA, outperforming Meditron 70B [55] at 58.5%.
BioMedLM [34], a 2.7B GPT-style model trained on PubMed content [108], excels in biomedical QA after fine-tuning,
achieving 57.3% on MedMCQA (dev) and 69.0% on MMLU medical genetics exams. Available on Hugging Face Hub8.
AdaLM [414] enhances domain-specific SLMs by continuing training on a medical-focused SLM atop a general pre-
trained model. It emperically validates adaptation then distillation is the most effective distillation way. AdaLM modified
7https://cyberiada.github.io/Hippocrates/
8https://huggingface.co/stanford-crfm/BioMedLM
Manuscript submitted to ACM
40 Fali Wang, et al.
Table 10. High-level Overview and Training Details of Specific-domain Small Language Models
Model #Params Date Base Models Domain Training Datasets Train Techniques
Hippocrates [3] 7B 2024.4 Instruction Tuning
(LLaMA2 [ 339], Mistral
[160])
Healthcare Medical Guidelines,
PMC-Patients [452],
and PubMedQA-
contexts [163]
Continual pre-training, in-
struction tuning, RLHF
BioMedLM [34] 2.7B 2024.3 From scratch and Fine-
tuning
Healthcare PubMed [108] FSDP
tuning
Healthcare PubMed [108] FSDP
BioMistral [178] 7B 2024.2 Mistral [160] Biomedicine PubMed [108] Continual pretraining
MentaLLaMA [406] 7B; 13B 2023.9 Instruction Tuning
(LLaMA2 [339])
Healthcare IMHI dataset RLHF; PEFT
AdaLM [414] 34M 2021.6 Distillation (BERT [83] or
MiniLM [362])
Healthcare PubMed [108] Continual pretraining,
Adapt-and-Distill
Rho-1 [210] 1B; 7B 2024.4 TinyLlama-1.1B [ 439],
Mistral-7B [160]
Science (Math-
ematics)
OpenWebMath
[267]
Continual pretraining
ematics)
OpenWebMath
[267]
Continual pretraining
ChemLLM [436] 7B 2024.4 Instruction Tuning (In-
ternLM2)
Science (Chem-
istry)
ChemData Continual training and
fine-tuning
SciGLM [435] 6B 2024.3 Instruction Tuning
(ChatGLM-6B)
Science SciInstruct Self-reflective instruction
annotation
Llemma [22] 7B 2023.10 Code Llama 7B Science (Math-
ematics)
Proof-Pile-2 [22] Continual pre-training
OceanGPT [30] 2B; 7B;
14B
2023.10 LLaMA2 [339] Science
(Ocean)
Open-access litera-
ture, DoINSTRUCT
(Ocean)
Open-access litera-
ture, DoINSTRUCT
Continual pre-training, In-
struction tuning
AstroLLaMA [258] 7B 2023.9 Tuning (LLaMA-2-7B) Science (As-
tronomy)
arXiv abstracts from
Kaggle
Continual training
DARWIN [388] 7B 2023.8 LLaMa 7B Science
(physics, chem-
istry, and
material)
SciQ [ 375], Scien-
tific paper[ 388],
FAIR [388]
Fine-tuning
MindLLM [412] 1.3B; 3B 2023.10 From-scratch and Super-
vised Fine-tuning
Law, Finance Pile [ 108], Wudao
[424], CBooks
Train on Bilingual Mixture
Data, SFT
Train on Bilingual Mixture
Data, SFT
a BERT_base model (12 layers, 768 hidden size) [83] with a 16GB PubMed 9 abstracts corpus. MentalLLaMA [406]
introduces the first IMHI dataset for mental health analysis and the first open-source LM for explainable analysis on
social media. The IMHI is compiled from ten sources, totaling 105K samples. Expert-designed mental health analysis
prompts are employed via ChatGPT for explanations. Based on Llama-2-7B, MentalLLaMA is instruction-tuned on this
data and matches top methods in accuracy on the IMHI test set. Project code is available at 10.
5.2.2 SLMs for Science. SciGLM [435] is a collegiate-level scientific language model overcoming data scarcity with
a self-reflective instruction annotation framework. Utilizing GPT-4 [2], it generates detailed reasoning for unlabeled
scientific problems through three steps with designed prompts in Table ??: (i) CoT prompt for step-by-step answers
(Prompt 1), (ii) reflective prompt for correcting errors (Prompt 2), and (iii) integrating the correct answer for clarity
(Prompt 3). The SciInstruct dataset spans physics, chemistry, math, and proofs, tuning ChatGLM-6B‚Äôs [93] reasoning
abilities. SciGLM boosts the base model‚Äôs (ChatGLM3-6B-Base) scientific QA accuracy by 3.06% on benchmarks such as
CEval-Hard [149], CEval-Sci [149], MMLU-Sci [134], SciEval [319], and SciBench [363]. Llemma [22], an SLM derived
from CodeLlama [292], specializes in mathematical reasoning. By continual pre-training, its 7B model is evolved on
55B tokens from the newly created Proof-Pile-2 dataset, which includes scientific papers, math web content, and
9https://pubmed.ncbi.nlm.nih.gov/
10https://github.com/SteveKGYang/MentaLLaMA
Manuscript submitted to ACM
A Survey of Small Language Models 41
Table 11. Prompts for self-reflective instruction annotation framework
Chain-of-Thought [Prompt 1] The following input consists of a science problem, please generate an elaborate step-
by-step solution to the problem.
Reflective Generation [Prompt 2] The following input comprises a science problem and a corresponding solution.
However, this solution is incorrect, please reflect on its errors and then generate a correct step-by-
step solution to the problem.
step solution to the problem.
Prompt Answer [Prompt 3] The following input consists of a science problem, a corresponding solution, and the
real answer. The given solution is incorrect, please reflect on its errors and then generate a correct
step-by-step solution to the problem based on the real answer.
mathematical code up until April 2023, to enhance few-shot capabilities. It excels in mathematical benchmarks like
MATH [134], GSM8k [66], OCWCourses [186], MMLU-STEM [134], and SAT, surpassing all comparable open-weight
models. ChemLLM [436] is a chemistry-focused language model that utilizes its proposed ChemData, a dataset
designed for instruction tuning that transforms chemical data into dialogue format for training. ChemLLM is based
on InternLM2-Base-7B [40], initially enhancing its language skills with a multi-corpus of 1.7 million Q&A pairs from
Hugging Face, then fine-tunes using ChemData and the multi-corpus to maintain its general capabilities. ChemLLM
excels in interdisciplinary chemical tasks within the proposed ChemBench, achieving results comparable to GPT-4 [2]
and outperforming GPT-3.5 with a score of 92.6 in Mol2caption, slightly below that of GPT-4. AstroLLaMA [258]
introduces an astronomy-focused language model. Based on Llama-2-7B [339] and enhanced via continual pre-training,
it has been developed using over 300K astronomy abstracts from arXiv 11. AstroLLaMA achieves 30% lower perplexity
than Llama-2-7B, indicating substantial improvements in domain adaptability. AstroLLaMA is available12 for tasks
such as automated paper summarization and conversational agent development in astronomy.
5.2.3 SLMs for Finance and Law. MindLLM [412] introduces a bilingual (Chinese and English) SLM, pretrained on
the Pile dataset [108] for English and WuDao [424], CBook, and various Chinese web content for Chinese. Bilingual
training enhances capacity and prevents catastrophic forgetting. It explores specific domains such as law and finance
through supervised fine-tuning. In law, it utilizes publicly available legal data, scenario-based Q&A from LaW-GPT
[139], and NLP-based legal tasks from DISC-LawLLM [427]. In finance, EastMoney 13 is selected as the data source.
Insights: We draw several key insights from the development of domain-specific SLMs:
‚Ä¢Adapting SLMs to domain-specific data is a common practice for acquiring domain-specific SLMs, prompting many to
create their datasets [258, 406, 435, 436]. These datasets are often annotated using LLMs like GPT-4 and used to continual
pre-train or fine-tune general models such as LLaMa-2-7B [ 3, 34]. To ensure the data quality, specialized annotation
frameworks are developed, such as SciGLM [435].
frameworks are developed, such as SciGLM [435].
‚Ä¢In domains with abundant corpora, training a general model from scratch and fine-tuning it using SFT [412] is practical.
Bilingual settings during training can prevent catastrophic forgetting.
‚Ä¢Distilling general capabilities from LLMs while integrating domain-specific knowledge from corpora is another method
for developing domain-specific SLMs [414].
11https://www.kaggle.com/Cornell-University/arxiv
12https://huggingface.co/universeTBD/astrollama
12https://huggingface.co/universeTBD/astrollama
13https://www.eastmoney.com/default.html
Manuscript submitted to ACM
42 Fali Wang, et al.
Table 12. SLMs help LLMs in different aspects
Aspect Representative work Key point
SLM for reliable
LLM generations
APRICOT [343] Trains a small auxiliary model to predict LLM‚Äôs confidence using
only textual inputs and outputs.
POLAR [450] Using a BERT model to calibrate LLM responses.
Hallucination Detector in NMT
[399]
Using lightweight classifiers to detect hallucinations in Neural Ma-
chine Translation.
SAPLMA [21] Using a BERT Small Language Model as a classifier to assess the
truthfulness of statements accurately.
Question Decomposer [383] Distilled SLM decomposes complex questions to aid reasoning.
SuperICL [393] SLM Plug-ins provide confidence and prediction for contextual ex-
emplars to aid in-context learning.
SuperContext [408] Specific SLM enhances ICL by providing confidence and predictions
to overcome out-of-domain challenges.
Self-RAG [18] A proxy model labels special tokens during RAG data generation
for fine-tuning.
for fine-tuning.
SKR [365] Training a small model to detect its self-knowledge for better use of
external knowledge.
SlimPLM [325] Detecting missing knowledge in LLMs with a slim proxy model,
enhancing the LLM‚Äôs knowledge integration.
In-Context RALM [287] Training a RoBERTa-based reranker for top-k BM25 documents
using LM signals to enhance LM gains.
CRAG [401] Training a lightweight evaluator to assess document quality and
trigger actions based on confidence levels.
trigger actions based on confidence levels.
GSR [148] Training a Generative Sub-graph Retriever (GSR) for relation chain
in RAG when retrieving from knowledge graphs.
SLM for
extracting LLM
prompts
Prompt Extraction [443] Small model trained to predict confidence of extracted system
prompt from adversarial prompts.
Prompt Stealing Attacks [297] Using small models fine-tuned as parameter extractors to facilitate
hierarchical prompt reconstruction.
hierarchical prompt reconstruction.
Output2prompt [433] Using a sparse encoder-decoder-based T5 small model to reverse-
engineer LLM inputs from outputs.
Model Purifying [197] Using SLMs to ensemble with LLMs, mitigating negative effects
from uncurated data.
SLM for
Fine-tuning
LLMs
LP[242] Learning Percentage as a difficulty metric.
Emulated Fine-tuning [246] Emulating pre-training and fine-tuning at different scales by sum-
ming base log probabilities with behavior deltas.
ming base log probabilities with behavior deltas.
CROSSLM [79] SLMs enhance LLMs by generating task-specific high-quality data.
Weak-to-Strong Search [453] Framing LLM alignment as a test-time greedy search to maximize
the log-probability difference between tuned and untuned SLMs.
SLM for LLM
applications
SLCoLM [327] Using SLM predictions to guide the LLM generation process in
Chinese Entity Relation Extraction.
HEF [413] Using SLMs as plugins to improve LLM‚Äôs nuanced understanding.
Contrastive decoding [198] Enhancing text quality by maximizing the difference between expert
and amateur log probabilities.
SLM for LLM
safety
Llama Guard [153] An LLM-based input-output safeguard model geared towards
Human-AI conversation use cases.
SLM as Guardian [177] A smaller LLM for both harmful query detection and safeguard
response generation.
SLM for LLM
evaluation
SLIDE [449] Utilizing SLMs trained via contrast learning to distinguish and score
responses in dialogue scenarios effectively.
responses in dialogue scenarios effectively.
Kuhn et al. [175] An SLM is used as the natural language inference classifier.
SelfCheckGPT [240] An SLM is used to calculate BERTScore.
Factscore [244] An SLM functions as the natural language inference classifier.
Manuscript submitted to ACM
A Survey of Small Language Models 43
6 SLMS FOR LLMS
In this section, we provide a comprehensive review of how SLMs enhance LLMs. While LLMs are robust, they face
challenges such as latency during inference, labor-intensive fine-tuning, noise filtration issues in retrieval, suboptimal
zero-shot performance, copyright infringement risks, and evaluation difficulties. SLMs can help LLMs to alleviate these
issues. Research in this field can be categorized into five primary areas: (i) using SLMs for reliable LLM generation; (ii)
extracting prompts for LLMs using SLMs; (iii) fine-tuning LLMs with SLMs; (iv) applying SLMs in LLM applications; (v)
utilizing SLMs as guardian; and (vi) evaluating LLMs using SLMs. A summary of representative work in each category
along with their key point is given in Table 12. Next, we introduce each category in detail.
6.1 SLM for Reliable LLM Generation
6.1 SLM for Reliable LLM Generation
Although LLMs generally produce fluent and convincing text, they can occasionally generate erroneous responses
[159, 349]. Additionally, LLMs are susceptible to privacy breaches from untrusted data collection, which can erode user
trust or cause harm. To address these issues, recent studies have focused on using SLMs to calibrate LLM confidence,
detect hallucinations, and improve retrieval-augmented LLMs and their reasoning capabilities.
SLM
LLM
Calibrated
Confidence
SLM
LLM
Calibrated
Confidence
Question LLM 
Answers
(a) SLM-based Calibrator
SLM
LLM LLM Internal 
States
Hallucination 
score
Question (b) SLM-based Hallucination Detector
Fig. 20. Architectures of Enhancing Calibration and Hallucination Detection of LLMs.
Enhancing Calibration and Hallucination Detection of LLMs As illustrated in Figure 20 (a), to calibrate
LLMs, an SLM processes both questions and LLM-generated answers to predict calibrated confidence. This training
involves minimizing the discrepancy between estimated calibration error and predicted confidence score. For instance,
APRICOT [343] uses an auxiliary DeBERTaV3 model [131] to assess LLM confidence in open-question scenarios, aiming
to improve uncertainty expression and response adjustment. Similarly, POLAR [450] has developed a self-supervised
approach that generates risk scores for each response to calibrate LLM confidence, utilizing a small BERT model [83] to
synchronize LLM outputs with other weak supervision sources. As shown in Figure 20 (b), for hallucination detection,
an SLM analyzes LLM internal states to output the likelihood of hallucination. This process uses supervised data
obtained by testing the knowledge boundaries of the LLM. In neural machine translation, Xu et al . [399] develop a
lightweight detector that analyzes token contributions to hallucinations, outperforming both model-free baselines and
quality estimation classifiers. Furthermore, SAPLMA [21] found that LLM internal states can signal the truthfulness of
statements, with a small BERT classifier trained to differentiate correct from incorrect predictions achieving accuracies
of 71% to 83%.
SLM
LLM
SLM Heuristic 
Answers
Retrieval Necessity
Judgement Model
known
unknown
Search 
Engine
Question LLM 
Answers
Retrieval 
Docs
Fig. 21. Architecture of SLM as a Heuristic RAG Prober.
Enhancing Retrieval-Augmented Generation Gener-
Enhancing Retrieval-Augmented Generation Gener-
ally, as shown in Figure 21,SLMs can also serve as proxy models
to evaluate the familiarity of LLMs with user queries, determin-
ing whether LLMs need to retrieve additional information or
can respond directly. For example, SlimPLM [325] is a small
proxy model that assesses the necessity for LLM retrieval by
Manuscript submitted to ACM
44 Fali Wang, et al.
generating heuristic answers. High-quality responses indicate that LLMs can handle queries independently, whereas
lower-quality outputs require further retrieval. Additionally, Self-Knowledge Guided Retrieval (SKR) [365] enables
SLMs to autonomously decide when LLMs should operate independently, based on their self-assessment of knowledge
limitations. Further, SELF-RAG [18] improves the factual accuracy and quality of LLM outputs through on-demand
retrieval and self-reflection. This method employs a small critic language model to issue reflective markers and make
binary decisions regarding the need for further information retrieval. Moreover, some studies utilize SLMs to evaluate the
relevance of retrieved documents. LongLLMLingua [162] employs SLMs to calculate the relevance of documents to a
query ùë•ùëûùë¢ùëí using perplexity, as formalized by the equation:
ùëüùëò = ‚àí1
ùëÅùëê
‚àëÔ∏Å
ùëñ
log ùëùSLM (ùë•ùëûùë¢ùëí
ùëñ |ùë•ùëëùëúùëê
ùëò ), ùëò ‚àà{1,2,...,ùêæ } (2)
where ùë•ùëûùë¢ùëí
ùëñ |ùë•ùëëùëúùëê
ùëò ), ùëò ‚àà{1,2,...,ùêæ } (2)
where ùë•ùëûùë¢ùëí
ùëñ is the ùëñ-th token in the query sequence, ùë•ùëëùëúùëê
ùëò is the retrieved document, and ùëÅùëê is the total number of
tokens in the query. ùëùSLM represents the probability generated by an SLM. CRAG [401] employs SLMs as evaluators
of document relevance in the same way. RA-ISF [219] trains a small language model that checks the base LLM in
self-knowledge, relevance judgment, and question decomposition. In addition, some research employs SLMs as re-rankers
to refine the order of documents provided by initial retrieval efforts such as BM25 [ 291]. In-Context RALM [287] positions
SLMs as rankers, optimizing the document sequence with a fine-tuning process on RoBERTa [218] as defined by the
loss function:
min
ùëüùëéùëõùëòùëíùëü
ùëò‚àëÔ∏Å
ùëñ=1
‚àílog ùëùrank (ùëëùëñ|ùë•‚â§ùë†ùëó )¬∑ùëùùúÉ(ùë¶|ùëëùëñ; ùë•‚â§ùë†ùëó ) (3)
where ùë•‚â§ùë†ùëñ is a prefix sampled from the training data, ùë¶ = ùë•ùë†ùëñ +1,...,ùë• ùë†ùëñ +ùë† represents the text to be generated in the
next stride, ùëùùúÉ(ùë¶|ùëëùëñ; ùë•‚â§ùë†ùëñ)denotes the probability of the LLM generating ùë¶ given ùëëùëñ and ùë•‚â§ùë†ùëñ, and ùëùrank (ùëëùëñ|ùë•‚â§ùë†ùëó )is
the ranking score of ùëëùëñ. Lastly, some studies leverage SLMs to retrieve sub-graphs when utilizing knowledge graphs as
external sources. Huang et al. [148] introduces the Generative Sub-graph Retriever (GSR) , which employs SLMs to
predict relation chains for answering questions, offering a cost-effective alternative to training LLMs. Specifically, it
uses customized T5 (220M, 770M, and 3B) [284] as retrievers to enhance LLM readers, including Llama2-chat-7B [339]
and Llama3-instruct-8B [94], on the WebQSP [418] and CWQ [324] datasets.
SLM LLM
Question
Examples
Question | Predicted Label |
Confidence | Ground Truth
Question | Predicted Label |
Confidence
LLM 
Answers
Fig. 22. SLM transfers knowledge into ICL.
Enhancing Reasoning Capabilities of LLMs As
illustrated in Figure 22, SLMs enhance LLMs reasoning
by transferring task knowledge to in-context examples,
effectively reducing hallucinations. While In-context
Learning (ICL) generally handles few-shot learning
with 16 to 32 examples, it struggles when faced with extensive supervised data. SLMs, specialized in task-specific
training, complement the broader domain knowledge of extensively pre-trained LLMs. For example, SuperICL [393]
incorporates SLMs as plugins for efficiently executing supervised tasks. It predicts labels for contextual examples and
integrates these predictions with the input text and actual labels to enhance knowledge transfer, thereby boosting
the understanding and responsiveness of LLMs. SuperContext [408] tackles challenges that LLMs encounter with
new tasks and out-of-distribution data in natural language understanding by synergizing SLM outputs with LLM
prompts during inference. This integration merges model predictions with their confidence levels, effectively leveraging
SLM task-specific knowledge and LLM domain expertise. Furthermore, SLMs efficiently decompose complex reasoning
Manuscript submitted to ACM
A Survey of Small Language Models 45
Language model outputs +
Attack queries ‚ë†‚ë°‚ë¢‚Ä¶
LLMService
Provider ùëÄùëÄùêøùêø 
SLM
Extracted prompts ‚ë†‚ë°‚ë¢‚Ä¶
SLM-based
Estimation ùëÄùëÄùëÜùëÜ
Extracted prompt ‚ë†
(a) Prompt Likelihood Estimation
Language model outputs
SLMParameter
Extractor ùëÄùëÄ
ùëÜùëÜ
LLMPrompt
Reconstructor ùëÄùëÄ
ùêøùêø 
Extracted prompt
(b) Prompt Parameter Extractor
Language model outputs
Encoder ùëÄùëÄùëÜùëÜùëÜ
Decoder ùëÄùëÄùëÜùëÜùëÜ
Extracted prompt
(c) Direct Model Inversion
SLM
SLM
(c) Direct Model Inversion
SLM
SLM
Fig. 24. SLM for LLM Prompt Extraction Paradigm. ùëÄùëÜ denotes small language models and ùëÄùêø denotes large language models.
(a) SLM-based prompt estimation tries various attack prompts; ùëÄùëÜ selects the most likely extracted one. (b) SLM-based Parameter
Extractor identifies the type of input prompt. (c) SLM-based Model Inversion uses ùëÄùëÜ to invert the LLM output back into the input.
by breaking tasks into simpler components , as demonstrated in [383]. This strategy increases efficiency and reduces
deployment costs when SLMs and LLMs are used collaboratively, transforming complex tasks into manageable segments.
LLM
SLM
Ensemble
Œ±
1- Œ±
Fig. 23. Architecture of SLM-based Data Protection
Alleviate Copyright and Privacy Issues
of LLMs LLMs pose significant security risks
due to their tendency to memorize training
data, leading to potential privacy breaches
data, leading to potential privacy breaches
and copyright infringement. As depicted in
Figure 23, SLMs can assist LLMs in address-
ing copyright and privacy concerns arising
from online data collection. By training on
selectively curated data subsets, SLMs effectively reduce copyright infringement and privacy risks, although they are
less effective than full-scale LLMs. To harness the combined benefits of both models, Li et al. [197] integrates untrusted
LLMs with benign SLMs using the CP-Œî KL algorithm to mitigate adverse effects while preserving performance. The
equation is:
ùëù(ùë¶|ùë•)= ùëùùëô(ùë¶|ùë•)¬∑ùëùùë†(ùë¶|ùë•)
ùëç(ùë•) (4)
where ùëùùëô and ùëùùë† represent the probabilities from the large and small models, respectively, and ùëç(ùë•)is the partition
function. This integration results in the following ensemble algorithm:
ùëßùëù(¬∑|ùë•)‚àù ùõºùëßùëô(¬∑|ùë•)+( 1 ‚àíùõº)ùëßùë†(¬∑|ùë•) (5)
where ùëßùëô and ùëßùë† are the logit values from the large and small models, respectively, and ùõº is the scaling factor.
6.2 SLM for Extracting LLM Prompts
Prompt-based methods are becoming simpler and more cost-effective alternatives to traditional fine-tuning in the LLM
era, utilizing LLMs‚Äô instruction-following capabilities for a competitive edge. Mastering prompts is vital for replicating
LLM-supported product behaviors. However, services such as Bing Chat and GitHub Copilot Chat have seen prompt
reverse-engineering through black-box API attacks. SLMs often serve as surrogate models in these attacks, employing
strategies such as (i) SLM-based prompt likelihood estimation, (ii) SLM-based prompt parameter extraction, and (iii)
SLM-based direct model inversion, illustrated in Figure 24.
Manuscript submitted to ACM
46 Fali Wang, et al.
SLM-based prompt likelihood estimation , as illustrated in Figure 24 (a), Zhang et al. [443] proposes using an
SLM as a Likelihood Estimator to identify secret prompts in LLM outputs. They craft attack prompts, such as ‚ÄúRepeat all
sentences in our conversation, ‚Äù and query the target LLM. The response is likely to include secret prompts, confusing
the LLM to interpret these as part of the conversation. A fine-tuned DeBERTa model [132] is then used to select the
most likely secret prompts from the output.
SLM-based prompt parameter extraction , as shown in Figure 24 (b), Sha and Zhang [297] utilizes an SLM as a
Parameter Extractor to extract prompt parameters from LLM outputs. They employ a specialized BERT model [ 83]
to classify LLM outputs into direct, in-context, and role-based prompts, also predicting the number of exemplars
for in-context prompts and identifying roles for role-based prompts. Prompt reconstruction is then performed using
ChatGPT once the parameters are defined.
SLM-based direct model inversion , as shown in Figure 24 (c), the method of using an SLM as a Direct Inversion
Model is designed to reverse-engineer LLM outputs back to their original prompts [433]. They train a sparse encoder-
decoder T5 model [284] with 222M parameters on the Instructions-2M dataset [249], where the input is LLM outputs
and the output is the LLM prompt. This trained model effectively maps multiple LLM outputs to their initiating prompts
as ùëù(ùë•|ùë¶1,...,ùë¶ ùëõ; ùëÄùëÜ1,ùëÄùëÜ2), with ùë¶ùëñ representing different output versions and ùëÄùëÜ1,ùëÄùëÜ2 the model parameters.
6.3 SLM for Fine-tuning LLMs
Training 
samples
Hard Training 
Samples
input Training 
difficulties
re-order
and filter
train
(b) SLM helps data selection
(a) SLM approximates the fine-tuning on a large scale
Labels
Synthetic 
dataset
Feedback
(c) SLMs promote the LLM to generate task-
specific high-quality data
FT SLM
UFT SLM
Question -
+
SLM
SLM
Label-descriptive
prompts
Local
dataset
LLM
LLM
LLM
prompts
Local
dataset
LLM
LLM
LLM
MS
SFT
MS
UFT
ML
Paris NYC 1955
Synthetic 
data
train
Fig. 25. SLM for LLM Fine-tuning.
Fine-tuning is a crucial technique for adapt-
ing LLMs to specific tasks or domains, yet it
is often time-consuming. For instance, fine-
tuning the LLaMA-2-13B [ 339] checkpoint
on 32 NVIDIA A100 GPUs with 80GB mem-
ory using bfloat16 format requires approx-
imately 70 hours [ 247]. This process also
demands high-quality data. Therefore, we
examine how SLMs can enhance LLM fine-
examine how SLMs can enhance LLM fine-
tuning through three approaches: (i) proxy
fine-tuning, (ii) selecting high-quality data,
and (iii) guiding LLM-generated task data, as
illustrated in Figure 25.
SLMs as proxy models : SLMs can approximate the gradient of fine-tuning large-scale LLMs on target datasets,
avoiding the costly fine-tuning process in terms of time and computational resources. As shown in Figure 25 (a),
Emulated Fine-Tuning (EFT) [246] simulates both unsupervised pre-training and supervised fine-tuning stages across
different scales by manipulating log probabilities. EFT, for example, combines base log probabilities from a 70B model
with behavioral deltas from a 7B model‚Äîthese deltas represent differences between fine-tuned and unfine-tuned SLMs,
effectively emulating outcomes for the Llama-2 series. This method allows fine-tuning on smaller models such as
Falcon-7B [10] while capturing most benefits of fine-tuning larger models such as Falcon-180B, benefiting applications
such as dialogue, question-answering, and code generation. Similarly, Proxy-tuning [212] adjusts LLM predictions by
adding the differences between the outputs of a fine-tuned small model and its untuned version to the LLM‚Äôs output
vocabulary during decoding, maintaining the advantages of large-scale pre-training while integrating small-scale
Manuscript submitted to ACM
A Survey of Small Language Models 47
fine-tuning benefits. Moreover, SLMs can act as proxies for approximate LLM fine-tuning during decoding. Weak-
to-Strong Search [453] strategy frames the alignment of LLMs as a test-time greedy search, aiming to maximize the
log-probability difference between small tuned and untuned models while sampling from the frozen large model. This
approach serves as a dual-purpose method: (1) a compute-efficient model up-scaling strategy that circumvents direct
tuning of the large model, and (2) an instance of weak-to-strong generalization that bolsters a strong model with weak
test-time guidance.
SLMs play a role in selecting high-quality fine-tuning data for LLMs. Figure 25 (b) illustrates how SLMs within
the same family as the LLM can identify training samples that are likely to be challenging, enhancing the training
efficiency and generalization capability of the LLM. As demonstrated by Swayamdipta et al. [323] and further advanced
by Mekala et al. [242], the learning percentage ùêøùëÉ(ùëñ)is a metric used to curate high-quality datasets with hard samples:
ùêøùëÉ(ùëñ)= ùëÉùëñ‚àí1 ‚àíùëÉùëñ
ùëÉ0 ‚àíùëÉùëõ where ùëÉùëñ represents the perplexity at the end of epoch- ùëñ, and ùëÉ0 is the initial perplexity. A higher
ùêøùëÉ(ùëñ)early in training indicates significant learning in the initial epochs, highlighting the potential of these samples to
enhance LLMs. SmallToLarge (S2L) [411] utilizes training loss trajectories from smaller models to guide data selection
for larger models fine-tuning. Experimental results demonstrate that S2L significantly enhances data efficiency in SFT
for mathematical problem-solving, reducing the required training data to just 11% of the original MathInstruct dataset
[428] to achieve performance comparable to that obtained using the full dataset.
SLMs enhance the quality of LLM-generated data for specific tasks. As depicted in Figure 25 (c), CROSSLM
[79] promotes the local training of SLMs on client-specific private data to mitigate privacy risks associated with
server-based LLMs. An SLM trained in this manner can guide the server-side LLM to produce high-quality synthetic
datasets. Feedback from SLMs regarding the quality of this synthetic data serves as a supervisory signal, enhancing
both the quality of LLM outputs and the utility of the data for further training.
6.4 SLM for LLM Applications
0.27 Hawaii
0.18 the
0.16 Honolulu
0.10 1961
0.27 Hawaii
0.18 the
0.16 Honolulu
0.10 1961
0.02 Washington
...
0.08 Honolulu
0.04 Washington
0.04 the
0.001 1961
...
Prompt:
Greedy: Hawaii. He was born in Hawaii. He was born in Hawaii...
Nucleus: Washington, D.C., to Barack Obama and Michelle Robinson...
CD: 1961 to a Kenyan father, Barack Hussein Obama and a mother of American 
descent, Stanley Ann Dunham...
Barack Obama was born in Honolulu, Hawaii. 
He was born in
Continuations:
next token prediction
1961
Hawaii
Honolulu
Washington
...
4.13
2.34
1961
Hawaii
Honolulu
Washington
...
4.13
2.34
0.65
-0.73
Constrastive Decoding
Amateur
LM
(GPT-2 small)
Expert
LM
(GPT-2 XL)
log ùëùùëùEXP ‚àílog ùëùùëùAMA
Fig. 26. Contrastive Decoding [198].
LLMs are utilized across various applications due to their open-ended
generation capabilities, yet they often lack specialized knowledge and
other generation issues. SLMs can supplement this by providing task-
specific knowledge or reflecting weaknesses. Therefore, we explore
how SLMs enhance the performance of LLMs in specific applications,
focusing on open-ended generation, knowledge integration, relation
extraction, and empathetic response.
In open-ended text generation ‚Äîsuch as writing assistance and
story creation‚ÄîLLMs often suffer from issues such as incoherence and
thematic drift over extended sequences. Due to more frequent failure
patterns observed in SLMs, such as short, repeated, and irrelevant
strings, these patterns serve as negative examples for LLM decoding.
Contrastive Decoding (CD) [198] improves coherence and lexical diversity by leveraging the differential capabilities
between a large model, OPT-13B [ 440], and a smaller model, OPT-125M. As illustrated in Figure 26, CD improves
content quality by sampling generation based on the difference in log probabilities, log ùëùùê∏ùëãùëÉ ‚àílog ùëùùê¥ùëÄùê¥, between an
expert LM and an amateur LM, rather than relying solely on the expert LM‚Äôs log probability. This approach effectively
reduces generative failures, including repetition.
Manuscript submitted to ACM
48 Fali Wang, et al.
SLM SLM SLM SLM Test Input
Domain-specific 
Knowledge
LLM
feedbackgenerate
Question-oriented
domain knowledge
1. Domain-specific 
pre-training
2. Knowledge 
Instruction Tuning
3. Bayesian Prompted 
Optimization
Train SLM
Inference
Prediction
from LLM
Fig. 27. BLADE Framework [189].
In knowledge injection , general LLMs may lack
domain-specific expertise for specialized tasks like law
or medicine [91, 353]. Domain-specific SLMs can sup-
ply crucial knowledge in a format suitable for LLMs.
To this end, BLADE [189] integrates black-box LLMs
with small domain-specific models. BLADE combines
the comprehensive language capabilities of LLMs with
the specialized knowledge of small LMs. As shown in
Figure 27, BLADE‚Äôs process includes: 1) pre-training SLMs on domain-specific data, 2) fine-tuning with knowledge
instruction to meet task-specific needs, and 3) using joint Bayesian optimization to enhance synergy between the LLM
and the small LM, boosting overall performance.
In relation extraction , a field limited by scarce labeled data and prevalent long-tail categories, the ‚ÄúTrain-Guide-
Predict‚Äù framework [327] employs SLMs to learn task-specific knowledge for dominant categories. SLMs struggle
with rare categories, whereas LLMs manage these effectively due to their extensive pre-trained text. Therefore, this
framework leverages the strengths of both models: it utilizes SLMs to acquire task knowledge and guide the LLM‚Äôs
generative process with initial SLM predictions, enhancing the LLM‚Äôs handling of underrepresented categories.
In generating empathetic responses , LLMs excel in expressiveness but struggle with nuanced emotions and
cognition. HEF [413] addresses this by incorporating Small Empathy Models (SEMs) to enhance LLMs‚Äô emotional and
cognitive depth. This framework employs a two-tiered emotion prediction method: SEMs identify primary emotions,
directing LLMs to concentrate on these emotions and their triggers, resulting in more accurate and empathetic responses.
6.5 SLM for LLM Safety
As demonstrated by various works [250, 302, 426, 460], LLMs are vulnerable to adversarial attacks and jailbreaking.
For example, Wang et al. [358] shows that ChatGPT‚Äôs performance on adversarial datasets is still far from perfect,
indicating that potential risks of adversarial vulnerability remain. Another example includes jailbreaking ChatGPT
by asking it to ‚Äôpretend to be a sarcastic mean girl. ‚Äô Using such techniques, it has been shown that even the most
advanced LLMs are far from being safe against generating potentially harmful content. Hence, the widely adopted
LLM-based services to generate are at high risk of being misused for nefarious purposes. Consequently, resources
such as the Llama 2 Responsible Use Guide 14 strongly advocate for implementing robust guardrails in products that
utilize Generative AI. These guardrails are specifically designed to mitigate risks associated with both inputs to and
outputs from the model, ensuring safeguards against the generation of high-risk or policy-violating content, as well as
protecting against adversarial inputs and attempts to compromise the model. In addition to developing trustworthy
LLMs, adopting SLMs for LLM safety [153, 177] has also attracted increasing attention. For example, Llama Guard [153],
fine-tuned on Llama2-7B, has publicly released an input-output safeguard tool specifically for classifying safety risks in
prompts and responses within conversational AI applications. However, this tool is limited to assessing the harmfulness
of questions and answers and does not facilitate the generation of fluent, safe responses. In response to this limitation,
Kwon et al. [177] fine-tune a specialized small language model with harmful query detection and safeguard answer
generation tasks to accurately detect harmful user queries and generate appropriate safeguard explanations, thereby
enhancing the safety measures in conversational AI.
14https://ai.meta.com/static-resource/responsible-use-guide/
Manuscript submitted to ACM
A Survey of Small Language Models 49
6.6 SLM for LLM Evaluation
SLMs can also enhance the evaluation of LLMs. In dialog evaluation, generating dialog reference responses is computa-
tionally complex, making accurate assessment difficult due to the multiple plausible but semantically different responses
possible for a single dialog context. Relying on LLM prompting for evaluation can lead to problems such as dependency
on prompt wording and inconsistent results. One solution involves training specialized SLMs to evaluate LLMs, as these
SLMs can be fine-tuned more quickly and generate outputs faster during inference, owing to their reduced number
of parameters. For example, SLIDE [449] employs contrastive learning to fine-tune an SLM to effectively distinguish
between positive and negative responses. Based on its observation that SLMs are more accurate in identifying positive
responses and LLMs excel at classifying negative ones, the trained SLM is subsequently integrated with an LLM to
assign a score to each response. The scoring method used is formalized as follows:
ùë†ùëêùëúùëüùëí =
Ô£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥ Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥
ùë†ùëêùëúùëüùëíùëÜùêøùëÄ , if ùë†ùëêùëúùëüùëíùëÜùêøùëÄ ‚â•0.5
ùë†ùëêùëúùëüùëíùêøùêøùëÄ , elif ùë†ùëêùëúùëüùëíùêøùêøùëÄ < 0.5
ùë†ùëêùëúùëüùëí ùëÜùêøùëÄ +ùë†ùëêùëúùëüùëí ùêøùêøùëÄ
2 , otherwise
(6)
This equation allows for adaptive response evaluation, leveraging the strengths of both models to ensure a more
reliable and consistent assessment across varying dialogue contexts. In the natural language generation task, Kuhn
et al. [175] designs a novel entropy to evaluate the uncertainty of LLMs. It aims to tackle the challenge of semantic
equivalence [175]. For instance, A‚Äôs son is B and B is A‚Äôs son are semantically equivalent. It should not be considered
uncertain if an LLM is unsure about which of the two previously mentioned sentences to generate due to semantic
equivalence. A DeBERTa-Large [132] fine-tuned on the MNLI [378] dataset serves as the classifier guided by semantic
equivalence in the clustering stage. SelfCheckGPT [240] proposes a black-box hallucination detection method for
LLMs. The core idea is to leverage uncertainty derived from sampled outputs. To be specific, Manakul et al. [240] claim
that an LLM trained on a concept generates responses that are similar and factually consistent. One of the five variants
of SelfCheckGPT uses BertScore to achieve it. A DeBERTa-Large [218] is utilized to calculate the BERTScore. Factscore
[244] is proposed to evaluate the factuality of LM-generated long-form content. It divides the generated long content
into multiple short texts, enabling a more precise assessment of factual accuracy. In addition to manual evaluation, Min
et al. [244] also proposes an automated evaluation framework to estimate Factscore which can reduce costs. LLaMa
7B [338] fine-tuned on Super-NaturalInstructions [ 367] is one of the LMs employed as an evaluation assistant and
shows promising performance. They also employ Generalizable T5-based dense retrievers [259] to facilitate passage
retrieval.
Insights: SLMs can improve LLMs in various aspects, including enhancing the reliability of LLM generation, extracting prompts, fine-tuning,
application, and evaluation. This discussion seeks to answer when SLMs should be utilized to augment LLMs. We identify several suitable
scenarios:
‚Ä¢ Adapting LLMs to specific tasks can require substantial computational resources and time. In such cases, a smaller model could be fine-tuned
instead to serve functions such as hallucination detection.
‚Ä¢ SLMs can outperform LLMs in certain aspects, hence combining SLMs with LLMs can create a more powerful model, e.g., SLMs typically
have fewer security issues than LLMs, and integrating both can generate a model that is both powerful and secure.
‚Ä¢ SLMs, despite their limitations, can alert LLMs to these issues, such as the tendency to produce repetitive vocabulary. Designing contrastive
losses can help LLMs overcome these issues by learning from the nuanced feedback of SLMs.
‚Ä¢ The fast inference speed and certain characteristics of SLMs can emulate and thus enhance the behavior of LLMs, acting as effective proxies.
For example, the training data selection for LLMs can be guided by the difficulty metrics assessed by SLMs, and the parameter adjustments
during the fine-tuning of SLMs can also approximate the fine-tuning processes of LLMs.
Manuscript submitted to ACM
50 Fali Wang, et al.
Table 13. Synergy between SLMs and LLMs
Synergy Representative Work Key Point
Cloud-Edge
Synergy
(Inference)
CoGenesis [437] Divide user instructions into general part by LLMs and private parts by SLMs.
Xu et al. [397] Introduce split learning in 6G to distribute LLM agents.
LLM-to-SLM [28] Encode prompts with server-side LLM and decodes with edge-side SLM.
Synergy of Thoughts [298] SLMs suggest reasoning paths; LLMs correct contradictions.
Hao et al. [127] SLM generates local tokens; LLM checks and corrects complex tokens.
LLMCad [394] Combine lightweight and high-precision LLMs for on-device inference.
Khattab et al. [167], Ma et al.
[234]
Focuse on LLM‚Äôs reasoning and SLM‚Äôs efficient decoding.
Cloud-Edge
Synergy
(Training)
CROSSLM [79] Preserve client data privacy by training SLM locally and LLM remotely; mutual
improvement using SLM-labeled data from LLM outputs.
Task-Centric
Synergy
Task-Centric
Synergy
ùõº-UMi [307] Break down a single LLM into specialized agents.
SynCID [204] Merge LLM‚Äôs semantic with SLM‚Äôs speed; refine labels via contrastive learning.
Filter-then-rerank [234] SLMs process simple samples and flag complex ones for LLM reranking.
Data Shunt+ (DS+) [46] Process easy samples with SLMs and delegates hard samples to LLMs.
7 SYNERGY BETWEEN SMALL AND LARGE LANGUAGE MODELS
7 SYNERGY BETWEEN SMALL AND LARGE LANGUAGE MODELS
The synergy between small and large language models leverages the unique strengths of each to enhance overall
system performance and efficiency. SLMs, being lightweight and resource-efficient, are ideal for deployment on edge
devices, enabling rapid responses and low latency for straightforward tasks. LLMs, on the other hand, possess greater
computational power and a deeper understanding of complex language patterns, allowing them to handle more intricate
and nuanced tasks. By integrating SLMs and LLMs, systems can dynamically allocate tasks based on complexity,
ensuring that simple queries are processed quickly on the edge while more demanding requests are escalated to the
cloud. This collaborative approach optimizes resource usage, reduces operational costs, and maintains high-quality
outputs across a diverse range of applications. The synergy between small and large language models can be categorized
into two parts: cloud-edge synergy and task-centric synergy . Cloud-edge synergy refers to a setup where SLMs operate
on edge devices, while LLMs reside on the server. When the SLM is not powerful enough, the LLM compensates by
handling more complex tasks and providing additional support. Task-centric synergy refers to the scenario where SLMs
and LLMs leverage their respective strengths to improve task-oriented efficiency. Table 13 summarizes representative
work in each category and their key points. Next, we introduce each category in detail.
Specialized SLM 
on User Device
Generic LLM on 
Cloud
High-level knowledge
General instructions
(a) Collaborating LLMs and SLMs enhances privacy and 
performance during inference.
SLM 1
SLM 
SLM 3SLM 2
Synthetic data
Labels
Labels
 Label-descriptive 
prompts
LLM
(b) Client-server collaborative training framework 
for language models.
Fig. 28. Could-edge synergy between LLMs and SLMs.
Manuscript submitted to ACM
A Survey of Small Language Models 51
7.1 Cloud-Edge Synergy
The current utilization of LLMs typically involves uploading private data to the cloud for response. Fine-tuning LLMs
usually also requires uploading data to clouds for computing. However, this raises privacy concerns as the collection
and usage of private data are constrained by personal privacy awareness and legal regulations [348]. Consequently, the
cloud-edge synergy between SLMs and LLMs is proposed to alleviate this issue, i.e., SLMs handle privacy-sensitive data
locally, LLMs handle de-identified or non-privacy-sensitive data, and these two models collaborate. This section discusses
such cloud-edge synergy, dividing them into two categories: cloud-edge synergy during inference and cloud-edge
synergy during training, as shown in Figure 28.
Cloud-edge Synergy During Inference. CoGenesis [437] breaks down the user instruction into a general section
and a personal section. The LLM generates replies solely based on general instruction, and the SLM considers both user
instruction and additional personal context for its output generation. A fusion strategy blends the output of LLM and
SLM synergistically.Xu et al. [397] introduces a split learning system for LLM agents in 6G networks, optimizing mobile
device and cloud server collaboration. Mobile devices operate lightweight SLMs with 0‚Äì10B parameters for real-time
tasks, while cloud servers handle larger LLMs with over 10B parameters for complex reasoning and planning. This setup
allows efficient local task management on mobile devices and offloads heavy operations to cloud servers. The system‚Äôs
architecture features three modules‚Äîperception, grounding, and alignment‚Äîfacilitating effective communication to
meet the sophisticated needs of 6G networks.
Besides these frameworks, more specific models are proposed to facilitate the cloud-edge synergy. A common
strategy is to use SLM‚Äôs fast decoding ability. LLM-to-SLM [28] proposes a framework in which the pre-trained frozen
encoder-decoder LLM resides on the server and computes a high-quality representation of the prompt for the planning
of an appropriate response. The SLM residing on the edge device, conditioned on this representation, decodes the
response efficiently. Some variants put more emphasis on the reasoning ability of LLMs [167, 234, 298]. In Synergy
of Thoughts [298], the SLMs generate multiple low-cost reasoning paths. If these paths conflict, the larger LLMs
are invoked to provide reflective reasoning and correct any intuitive errors. Hao et al . [127] proposes a framework
in which an SLM residing on the edge devices generates tokens, calling LLMs to verify and correct threshold-gated
"harder" tokens, to achieve a controllable trade-off between inference quality and cost. LLMCad [394] presents an
on-device inference engine addressing memory and latency issues in deploying LLMs on mobile devices. It combines a
lightweight LM for token generation with a high-precision LLM for verification, leveraging a token tree structure and
speculative generation for efficiency. Tested on devices such as Jetson TX2, it achieves up to 9.3√ó speedup for LLMs
with over 10 billion parameters while maintaining accuracy.
Cloud-edge Synergy During Training. CROSSLM [79] introduces a client-server collaborative training framework
that preserves data privacy by having clients locally train SLMs instead of fine-tuning LLMs. The framework enables
mutual enhancement through a feedback loop where SLMs evaluate LLM-generated synthetic data and provide feedback
to improve the LLM‚Äôs generative capabilities, ensuring high-quality and task-specific data. Concurrently, the synthetic
data trains the SLMs, boosting their performance. This cyclical exchange fosters cloud-edge synergy and mutual model
improvement.
7.2 Task-Centric Synergy
The advent of LLMs has significantly propelled various natural language processing tasks and inspired research into
their synergistic interactions with SLMs to enhance the performance of models tailored for specific tasks. This section
Manuscript submitted to ACM
52 Fali Wang, et al.
introduces scenarios where small language models exhibit specialized capabilities after fine-tuning and discusses how
combining their unique strengths with the versatile abilities of LLMs can yield superior performance on specific tasks.
For example, LLMs excel at handling difficult examples or can rewrite content to eliminate task-irrelevant redundancy,
thereby enhancing overall task performance, as illustrated in Figure 29, 30 and 31.
Planner
Caller
Summarizer
Instruction
Tools
Planner
Caller
Summarizer
Instruction
Tools
Fig. 29. Synergizing SLMs and LLMs in tool learning.
ùõº-UMi [307] introduces a multi-agent framework to en-
hance tool learning by overcoming the limitations of single-
LLM approaches for complex tasks. It utilizes three specialized
LMs‚Äîplanner, caller, and summarizer‚Äîas depicted in Figure
29‚Äîeach handling specific subtasks such as planning, tool invo-
cation, and summarization. This modular design allows the use of small and large open-source LLMs (e.g., LLaMa-7B/12B)
and supports easy tool updates. Evaluated on benchmarks like ToolBench [279] and ToolAlpaca [326], ùõº-UMi outper-
forms traditional single-LLM methods and even exceeds GPT-4 in tool learning performance.
LLM
SLM
Unlabeled  Labeled
Concise utterance ùë•
Generated intent descriptor ùë¶ 
(include zero-shot labels)
Utterance Space 
Descriptor Space 
Alignment
Selected close-to-center
Alignment
Selected close-to-center 
utterances as clusters of existing 
intent and zero-shot intent
LLM
Test utterance ùë•
New label ùë¶
Fig. 30. Synergizing SLMs and LLMs in Conversational In-
tent Detection.
SynCID [204] focuses on Conversational Intent Discovery
(CID), a task where both known and new intents must be iden-
tified from user utterances in an open-world setting. SynCID
combines LLMs‚Äô deep semantic insights with SLMs‚Äô agility and
specialized capabilities. As illustrated in Figure 30, the frame-
work uses LLM prompting to refine discourse and intent labels,
enhancing semantic accuracy and assigning new labels to un-
labeled data. SLMs are trained via contrastive learning to align
semantic spaces of discourse and intent descriptors, reducing clustering distortion and improving new intent detection.
Tested on BANKING [43], CLINC [179], and StackOverflow [396], SynCID outperforms CID baselines significantly.
Test Input SLM Hard? LLM Prediction 
from LLM
Prediction 
from SLM
Y
N
by confidence Top-k
Prediction 
from SLM
Y
N
by confidence Top-k 
reranking
Fig. 31. Synergizing SLMs and LLMs in Information Extraction.
Filter-then-rerank [234] ad-
dresses LLMs‚Äô poor performance
on simpler IE tasks by integrat-
ing LLMs and SLMs. SLMs act
as filters, predicting and identify-
ing difficult samples, while LLMs
rerank the top N candidate labels for these cases. As illustrated in Figure 31, SLM predictions are final for non-difficult
samples, minimizing reliance on LLMs and reducing latency and costs; for those difficult samples, the top N predicted
candidate labels from the SLM are passed to the LLM for reranking (predicting). Tested on small-sample IE tasks, this
approach improves performance by an average of 2.4% compared to previous methods. Data Shunt+ (DS+) [46] intro-
duces a framework to reduce costs by minimizing large model queries during inference and boosting LLM performance
with SLMs for tasks like sentiment analysis and image processing. DS+ uses SLMs for ‚Äúeasy‚Äù samples within the main
training distribution and LLMs for "hard" outliers or boundary cases, maintaining accuracy while reducing LLM use.
It incorporates S4L and L4S modules with Prompt Pruning (PP) and 2-stage Confidence Distillation (2CD) for better
input processing and knowledge transfer. Tests show DS+ outperforms fine-tuning in accuracy and cost efficiency,
significantly cutting down on LLM queries.
significantly cutting down on LLM queries.
Manuscript submitted to ACM
A Survey of Small Language Models 53
Fairness
PrivacyRobustness Safety
Adversarial 
Robustness
Out-of-Distribution 
Robustness
Toxicity
Reliability
Hallucination
SycophancyMisinformation
Fig. 32. Scenarios we discuss in this section. The taxonomy is inspired by previous works [ 320, 351]. Please note that the trustworthy
scenarios listed here are not exhaustive.
8 TRUSTWORTHINESS IN SMALL LANGUAGE MODELS
8 TRUSTWORTHINESS IN SMALL LANGUAGE MODELS
Language models have become ubiquitous in our daily lives, and we increasingly rely on them. However, they pose risks
regarding their limitations in trustworthy dimensions like privacy and fairness. These concerns are especially critical
in high-stakes domains such as healthcare [130] and finance [202]. Consequently, numerous studies have emerged
to evaluate the trustworthiness of LMs [88, 95, 138, 176, 176, 248, 254, 273, 351, 370, 425]. In this section, we consider
the works that benchmark various LMs‚Äô trustworthiness and omit the specific attack methods [ 41, 53, 150, 461] or
work [407] that only focuses on early pre-trained LMs like BERT [83] as they are already covered in previous survey
papers [78, 115, 123, 288]. Inspired by previous works [320, 351], we discuss the following five key trustworthy scenarios:
robustness, privacy, reliability, safety , and fairness, as shown in Figure 32. We consider two dimensions for robustness:
Adversarial (Adv) Robustness [352] and Out-of-Distribution (OOD) Robustness [38, 213]. For safety, we explore two key
concerns: Misinformation [344] and Toxicity [374]. For reliability, we focus on Hallucination [146] and Sycophancy [301].
Please note that these are just the aspects we are focusing on, and therefore this is not a comprehensive classification or
taxonomy. For example, robustness also contains robustness to adversarial demonstration.
Though there are a lot of works benchmarking LMs‚Äô trustworthiness, their main focus is on LLMs. Therefore, we
survey some representative works evaluating the trustworthiness of LMs, focusing specifically on those that include
SLMs of around 7B parameters or smaller. We also summarize these works in Table 14. Next, we briefly introduce them.
Holistic Evaluation of Language Models (HELM) [205] benchmarks a large number of LMs from various aspects,
including a lot of metrics related to trustworthiness such as robustness and fairness. Do-Not-Answer [364] introduces a
dataset to evaluate how LMs act when they face content that should not be answered. Wang et al. [364] also label the
output of several LMs output on their dataset and then use the labeled data to train some classifiers. PromptRobust [456]
constructs two kinds of adversarial prompts to evaluate LMs‚Äô robustness: One kind is designed under non-adversarial
settings with semantic integrity while another category is created under adversarial settings. Their results show that
LMs perform poorly under such prompts. HaluEval [191] builds a dataset comprising both the samples generated by their
proposed framework and human-labeled hallucinations. It facilitates analysis of when LMs produce hallucinated output
and how well they detect hallucinated content. Then they use some strategies such as knowledge retrieval to help LMs
better recognize hallucinations. Mo et al. [248] evaluates the trustworthiness of open-source LMs, presenting a variety
of scenarios such as fairness and privacy. Results show that smaller LMs sometimes outperform larger ones in terms of
trustworthiness. PrivLM-Bench [190] is designed to evaluate the privacy issues in LMs. It enables a fair comparison of
privacy-preserving LMs by considering more than just differential privacy parameters. FFT [71] introduces around two
thousand crafted examples to evaluate LMs‚Äô performances on three trustworthy dimensions: factuality, fairness, and
toxicity. Their results suggest that larger LMs do not always show better harmlessness. ROBBIE [98] first benchmarks
various series of LMs using a lot of datasets, including two newly introduced datasets developed by ROBBIE. It also
Manuscript submitted to ACM
54 Fali Wang, et al.
Table 14. Comparison of Different Works that Evaluate the Trustworthiness Issues in LMs. Please note that for the "No. of LMs"
attribute, compressed or pruned LMs are not included in the count.
Paper
Adv Robustness
OOD Robustness
Toxicity
Misinformation
Hallucination
Sycophancy
Privacy
Fairness
Have Compressed SLMs
HELM [205] ‚úì √ó ‚úì ‚úì √ó √ó √ó ‚úì √ó
Do-Not-Answer [364] √ó √ó ‚úì ‚úì √ó √ó ‚úì ‚úì √ó
PromptRobust [456] ‚úì √ó √ó √ó √ó √ó √ó √ó √ó
HaluEval [191] √ó √ó √ó √ó ‚úì √ó √ó √ó √ó
Mo et al. [248] ‚úì √ó ‚úì √ó ‚úì ‚úì ‚úì ‚úì √ó
Mo et al. [248] ‚úì √ó ‚úì √ó ‚úì ‚úì ‚úì ‚úì √ó
PrivLM-Bench [190] √ó √ó √ó √ó √ó √ó ‚úì √ó √ó
FFT [71] √ó √ó ‚úì ‚úì ‚úì √ó √ó ‚úì √ó
ROBBIE [98] √ó √ó ‚úì √ó √ó √ó √ó ‚úì √ó
TrustLLM [320] ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì √ó
RAmBLA [35] ‚úì √ó √ó √ó ‚úì √ó √ó √ó √ó
JailbreakBench [45] √ó √ó ‚úì ‚úì √ó √ó ‚úì √ó √ó
Xie et al. [390] √ó √ó ‚úì √ó ‚úì √ó √ó √ó √ó
OR-Bench [70] √ó √ó ‚úì ‚úì √ó √ó ‚úì √ó √ó
SORRY-Bench [387] √ó √ó ‚úì ‚úì √ó √ó ‚úì √ó √ó
BeHonest [59] √ó √ó √ó ‚úì ‚úì ‚úì √ó √ó √ó
Hong et al. [138] ‚úì ‚úì ‚úì √ó √ó √ó ‚úì ‚úì ‚úì
RUPBench [370] ‚úì √ó √ó √ó √ó √ó √ó √ó √ó
Nakka et al. [254] √ó √ó ‚úì √ó √ó √ó ‚úì ‚úì √ó
Nakka et al. [254] √ó √ó ‚úì √ó √ó √ó ‚úì ‚úì √ó
evaluates mitigation techniques designed to reduce bias and toxicity. TrustLLM [320] is a comprehensive benchmark
that contains a large number of datasets and various well-designed metrics to systematically evaluate various LMs across
multiple trustworthy dimensions, including truthfulness, safety, fairness, robustness, privacy, and machine ethics. They
also carefully design specific subcategories for each dimension. RAmBLA [35] evaluates the trustworthiness of four LMs
as biomedical assistants from three dimensions: Robustness, High Recall, and Hallucination. RAmBLA suggests LMs
with more parameters are less likely to cause hallucinations and may choose to reject providing an answer in uncertain
situations. JailbreakBench [45] constructs a jailbreaking dataset named JBB-Behaviors and jailbreak artifacts to evaluate
current LMs‚Äô performance regarding jailbreaking. It also proposes a unified evaluation pipeline that can incorporate
new jailbreak defense techniques. Xie et al. [390] tests online safety analysis methods, filling the gap where no methods
focus on the generation phase. OR-Bench [ 70] constructs three datasets: OR-Bench-80K, OR-Bench-Hard-1K, and
OR-Bench-Toxic, to systematically evaluate over-refusal problems in LMs, emphasizing the challenge of balancing
safety alignment with the models‚Äô usefulness. SORRY-Bench [387] systematically tests 43 different LMs to see how they
perform when facing requests that should be refused. They also collect more than annotations created by humans and
find that fine-tuned 7B LMs can achieve performance comparable to GPT-4 scale LMs as evaluators. BeHonest [59]
evaluates the honesty of LMs from three aspects: Self-Knowledge, Non-Deceptiveness, and Consistency. They use
many different metrics for each aspect. For example, sycophancy rate and lying rate are adopted in Non-Deceptiveness.
The results in both the Self-Knowledge and Consistency parts reveal that larger model sizes generally bring improved
performance for the Llama-2 [ 339] and Llama-3 [ 94] series. Hong et al . [138] examines the effects of compression
Manuscript submitted to ACM
A Survey of Small Language Models 55
methods, including quantization and pruning, on the trustworthiness of language models. They find that pruning and
extreme quantization significantly affect the trustworthiness of LMs. RUPBench [370] comprises 15 reasoning datasets
designed to assess the performance of LMs both in normal conditions and under various adversarial perturbations. Their
results indicate that larger LMs generally demonstrate better resilience to perturbations. Nakka et al. [254] investigates
the trust and ethical implications of SLMs deployed on personal devices. It reveals the vulnerabilities of on-device SLMs
compared with their on-server counterparts.
Please note that the dimensions discussed in this section reflect only those relevant to our current focus; additional
dimensions may be discussed in those works, but not listed in table 14. For example, TrustLLM [ 320] also explores
Machine Ethics.
9 FUTURE DIRECTIONS
Machine Ethics.
9 FUTURE DIRECTIONS
In this section, we offer insights into several promising future research directions that could inspire and motivate the
community to address existing gaps in the development of small language models.
9.1 Developing Efficient SLM Model Architecture
Although Transformers [346] are foundational in most language models, they face significant computational and
memory challenges that worsen with model size, impacting training and autoregressive decoding. Recently, Mamba
[117] has emerged as a promising alternative, adapting state space models to dynamically select inputs based on
demands, thereby enhancing efficiency. Thereafter, xLSTM [25] demonstrates that an improved LSTM could function as
an LLM, revealing the potential of traditional SSMs. The integration of global static information captured by SSMs with
the dynamic information processing of Transformers could complement each other, leading to new architectures that
balance effectiveness and efficiency.
balance effectiveness and efficiency.
9.2 Addressing SLM Training Inefficiencies
One study [ 85] explores the disparate learning dynamics between small and large language models. Utilizing the
Pythia model suite, the research demonstrates that layers‚Äô activations in larger models converge more rapidly and
monotonically to their final states. This phenomenon is associated with a higher proportional effective rank (PER) in the
parameters and gradients of larger models. The analysis enhances our understanding of training inefficiencies in small
models and provides insights for future efforts, such as developing methods to increase the PER of layers‚Äô parameters.
9.3 Expanding Domain-Specific SLMs
Domain-specific SLMs, which are tailored for specific fields, can provide a stronger foundation for relevant downstream
tasks than general-purpose models. Currently, these models primarily focus on scientific and healthcare domains.
However, there is significant potential for expansion into other key areas such as law, finance, education, telecommuni-
cations, and transportation. The scarcity of SLMs that cater to these domains presents an urgent call for research into
developing more specialized models.
9.4 Establishing Benchmarking and Leaderboard Platforms for SLMs
Several compelling reasons justify the establishment of benchmarking and leaderboard platforms for SLMs. Firstly, most
state-of-the-art SLMs are trained on proprietary datasets, which may include test sets from existing evaluation tasks,
presenting challenges for fair capability comparisons. Secondly, many SLMs are designed for specific device applications,
Manuscript submitted to ACM
56 Fali Wang, et al.
significantly differing from general open-domain tasks. Thus, there is a lack of comprehensive benchmarks that
accurately reflect SLM performance in specific device applications. For example, SLMs deployed on smartphones often
handle tasks sensitive to user data, such as auto-replies based on historical chat texts or GUI context understanding‚Äîtasks
not typically included in current benchmarks, potentially leading to an underestimation of their importance. Finally,
current evaluation tasks focus primarily on metrics like accuracy. Evaluating on-device SLMs involves balancing
multiple factors, including overall capabilities, response times, storage and memory usage, power consumption, CPU
utilization, additional fine-tuning requirements, and context window constraints, making comprehensive and detailed
assessments essential.
9.5 Enhancing SLM Performance and Efficiency
9.5 Enhancing SLM Performance and Efficiency
In terms of enhancing SLM performance and efficiency, the efficiency of using teacher LLMs via instruction tuning can
be further developed, such as Efficient Instruction Tuning of SLMs from LLMs-generated data, Optimizing Teacher LLM
Selection for SLM Learning, and Applying Emerging Techniques from LLMs to SLMs.
‚Ä¢Efficient Instruction Tuning of SLMs from LLMs-generated data. Enhancing the specialization of SLMs through
instruction tuning from LLMs-generated data is crucial, yet finding the most cost-effective instructional strategies
remains an underexplored status. Some key areas for exploration are:
(1) Instruction Design Adaptability : The performance of LLMs and SLMs varies significantly with changes in instruc-
tions. Therefore, designing tailored instructions that effectively activate relevant sub-competencies and reasoning
pathways in SLMs for specific tasks is crucial. This approach would optimize their ability to utilize instructional data,
representing a significant future research direction.
(2) SLM Capability Adaptability : Given that SLMs exhibit diverse capabilities across domains, simply supplying
extensive data samples for instruction tuning is often inefficient, as SLMs may spend excessive time processing
unnecessary data. To optimize efficiency when adapting to specific domains, we suggest first assessing the intrinsic
capabilities of an SLM within those domains. Subsequently, one could select appropriate data and activate essential
fine-grained capabilities to effectively adapt to domain shifts. This targeted approach ensures efficient and domain-
specific instruction tuning.
(3) Optimizing Data Efficiency : SLMs may possess missing or latent domain knowledge, and activating this latent
knowledge may not require substantial data. Thus, identifying inherent knowledge within SLMs and determining the
minimal data necessary for effective fine-tuning is a future direction. This research aims to optimize performance
while minimizing training resources.
‚Ä¢Optimizing Teacher LLM Selection for SLM Learning. Teacher LLMs with different abilities and knowledge
facilitate diverse applications for SLM training, including data rewriting and generation. Selecting the appropriate
teacher model based on specific use cases is crucial. This process requires evaluating the teacher‚Äôs capabilities and
knowledge to ensure optimal application. For example, GPT-4 excels in generating domain-specific data, outperforming
ChatGPT, which may produce inferior outcomes. Strategic selection of teacher LLMs is essential for future work to
ensure their strengths are effectively utilized to enhance SLM performance.
‚Ä¢Applying Emerging Techniques from LLMs to SLMs. To improve LLM performance, techniques such as Retrieval-
Augmented Generation (RAG) and Mixture of Experts (MoE) are employed. The adoption of RAG in SLMs shows
significant promise [215], suggesting benefits from further tailoring retrieved information for SLMs. Future research
should account for SLMs‚Äô constraints, such as limited context windows, and customize RAG accordingly. MoE uses
Manuscript submitted to ACM
A Survey of Small Language Models 57
multiple experts to enhance learning without increasing active neurons, but its storage demands pose challenges
for SLM deployment, making this a promising area for exploration. Additionally, the application of LLM techniques
such as in-context learning and prompting engineering to maximize SLM performance, while accounting for SLMs‚Äô
constraints, warrants further investigation.
9.6 Applications of SLMs
9.6 Applications of SLMs
In real-world applications, SLMs often need to provide personalized services and need to be updated periodically
to reflect new needs and new knowledge. Hence, there are several promising directions in terms of the real-world
application of SLMs, which are listed as follows:
‚Ä¢LoRA for Personalized Services. Companies often provide personalized services, but user-specific complexities
can render simple rules ineffective. Training a separate SLM for each user is impractical. LoRA suggests a method of
separable training weights alongside fixed original weights, enabling scalable customization. For instance, RecLoRA
[455] integrates personalized knowledge into SLMs/LLMs tailored for recommendation tasks by maintaining a set of
parallel, independent LoRA weights. This approach effectively customizes language model parameters to align with
individual user preferences. This approach is a promising direction that inspires further investigation.
‚Ä¢Lifelong On-device Learning for Knowledge Injection. SLMs on devices can access local data without risking
data privacy through two main methods. The first method uses retrieval-augmented generation to integrate local data
into prompts, requiring SLMs with advanced processing and reasoning capabilities. The second method fine-tunes
SLMs with local data, integrating customized knowledge into the model‚Äôs weights. However, this approach demands
significant device resources, including memory and energy. A promising solution is lifelong learning, where SLMs
continuously learn and adapt while in use.
‚Ä¢Strategic Use of SLMs and LLMs in Multi-Agent Systems. LLMs can function as agents; however, their extensive
capabilities are often underutilized in many scenarios, leading to resource wastage. Consequently, strategically
routing to appropriately capable SLMs and LLMs within multi-agent systems can optimize cost and functionality.
9.7 Multimodal SLMs
Research on small language models also includes multimodal data. For example, SmolVLM [99] is a compact model
that handles image and text inputs to produce text outputs, suitable for on-device use and various multimodal tasks.
SOLO [54] integrates vision and language processing in a single 7B Transformer model. The limited scope of existing
research on multimodal SLMs provides a compelling impetus for researchers to investigate the integration of various
modalities, including audio and graphs.
9.8 SLMs Assisting LLMs
In Section 6, we introduced existing works on the use of SLMs to assist LLMs. For instance, EFT [ 246] emulates
fine-tuning on LLMs by leveraging behavior deltas between SLMs‚Äô pre-trained and fine-tuned weights to alleviate
the time-cost issues associated with fine-tuning LLMs; SlimPLM [ 325] detects missing knowledge in LLMs using a
slim proxy SLM to accelerate knowledge injection; Contrastive Decoding [198] enhances text quality by maximizing
the difference between the log probabilities of an expert LLM and an amateur SLM to mitigate issues of low-quality
generation. The research on adopting SLMs to assist LLMs is still in its early stages, with many promising directions
yet to be explored. We list some as follows:
yet to be explored. We list some as follows:
Manuscript submitted to ACM
58 Fali Wang, et al.
‚Ä¢Enhancing LLM Performance Across Broader Tasks Through SLM Integration. SLMs can outperform LLMs in
certain scenarios. For example, SLMs often present fewer security vulnerabilities compared to their larger counterparts
and demonstrate superior performance on easier samples in specific tasks [197, 234]. Therefore, integrating SLMs
with LLMs can promote the development of models that are not only more robust but also inherently safer. Currently,
research in this domain is relatively sparse, suggesting that this collaborative framework could potentially be applied
to a wider array of tasks.
‚Ä¢Efficient Enhancement of LLMs through Proxy SLMs. Existing research [18, 212, 246, 325] indicates that SLMs,
owing to their accelerated fine-tuning and inference speeds, can effectively mimic the behaviors of LLMs, thereby
serving as efficient proxies for optimization. However, the application of SLMs as operational proxies for LLMs is
currently underexplored. This mimicry could potentially be expanded to include various aspects of LLM functionality,
such as the optimization of prompts, the filtration and integration of supplementary knowledge, and the management
of additional knowledge repositories.
‚Ä¢SLMs Assist in Managing Data Quality. LLMs tend to produce hallucinations and toxic content due to low-quality
real-world training data. One solution is to remove these low-quality data [ 355]. However, directly eliminating
low-quality content can diminish certain functionalities of LLMs, such as versatility [356]. Therefore, it is crucial to
define more refined data quality assessment criteria across dimensions such as factuality, safety, and diversity [377]
for real-world data. Researching efficient data selection methods using small models represents a valuable research
direction. Additionally, while synthetic data serves as a vital complement to scarce human-generated data [224], the
potential for small models to effectively manage synthetic data remains largely unexplored.
‚Ä¢SLMs Assist in LLM Assessment. LLMs are producing vast amounts of increasingly complex texts, such as
specialized code and scientific papers, presenting challenges not only for human evaluators but also for traditional
assessment metrics. Consequently, developing effective evaluators to assess various aspects of generated content,
including factuality, safety, and uncertainty, becomes crucial. Given their proficiency in handling specific tasks,
exploring the potential of SLMs to evaluate LLM outputs is a promising research direction.
‚Ä¢SLMs Optimize Query and Reduce Noise for LLM RAG. For Retrieval-Augmented Generation (RAG) using LLMs,
differing query requirements between LLMs and search engines pose a challenge. The query for LLMs is often abstract
and difficult for search engines to handle, so they require more detailed query keywords. Moreover, LLMs may not
need all the information related to a query because they only require partial additional knowledge. Thus, intermediate
agents are crucial to adapting LLM queries for search engines by clarifying the required detailed keywords that can
search for necessary extra knowledge. Additionally, search engine outputs contain noises, requiring refinement to
boost LLM efficiency. SLMs, skilled in a single task, are ideal for optimizing query rewriting and noise reduction in
RAG systems, making their application in LLM RAG a promising research area.
‚Ä¢SLMs safeguard LLM. Resources such as the Llama 2 Responsible Use Guide strongly advocate for the implementation
of robust guardrails in products that utilize Generative AI. SLMs can be strategically designed to serve as such
guardrails, mitigating risks associated with both inputs and outputs from the model. This approach ensures safeguards
against the generation of high-risk or policy-violating content and provides protection against adversarial inputs
and attempts to compromise the model. Future research can investigate the various safety roles that SLMs play in
protecting LLMs.
Manuscript submitted to ACM
A Survey of Small Language Models 59
9.9 Synergy between Small and Large Language Models
In Section 7, we discussed how small and large language models can complement each other. For example, CoGenesis
[437] integrates SLMs for private data and LLMs for broader context, while Synergy of Thoughts [298] uses SLMs for
initial reasoning and LLMs for conflict resolution. CROSSLM [79] shows how privacy can be preserved by training
SLMs locally to support LLMs without data exposure. Research in this area is still evolving, and we outline several
promising future directions below:
‚Ä¢Refined Cloud-Edge Division of Labor. Current research mainly focuses on splitting tasks between edge-based
SLMs and cloud-based LLMs along privacy-sensitive and non-sensitive data boundaries. A potential future direction
involves more granular task partitioning: determining which subtasks should be handled locally by SLMs (e.g.,
initial data filtering, quick semantic parsing) and which should be delegated to the cloud-based LLM (e.g., advanced
reasoning, complex generation). This approach can further optimize latency, privacy, and resource utilization.
‚Ä¢Adaptive On-Device Specialization for Dynamic Environments. Although SLMs have shown the ability to
handle private or personalized data locally, continuous changes in user preferences, application requirements, and
data distributions pose challenges. Future work can explore adaptive strategies where edge-based SLMs dynamically
specialize or update their parameters, guided by the cloud-based LLM. For instance, the LLM can periodically distill
new knowledge into the SLM or provide feedback signals to help the SLM adapt to evolving scenarios.
9.10 Trustworthy SLMs
As SLMs are playing crucial roles in various aspects, understanding and improving the trustworthiness of SLMs are
essential. Hence, two promising directions are:
‚Ä¢A Comprehensive Evaluation of SLMs‚Äô Trustworthiness. While numerous studies address trustworthiness
issues in LLMs, research on SLMs remains sparse. Most existing literature focuses on models with at least 7 billion
parameters, leaving a gap in the comprehensive analysis of SLMs‚Äô trustworthiness. Current evaluations typically
cover only a fraction of the necessary aspects. Therefore, a systematic assessment, such as TrustLLM [320], is essential
to thoroughly evaluate the trustworthiness of SLMs and understand their reliability across various applications.
‚Ä¢Developing Trustworthy SLMs. Developing trustworthy SLMs is crucial, with three key research directions: (i)
Training SLMs to be trustworthy from scratch; (ii) Ensuring SLMs retain or gain trustworthiness when compressed
from LLMs‚Äîmaintaining trustworthiness if the LLM is trustworthy and instilling trustworthiness if it is not; (iii)
Fine-tuning non-trustworthy SLMs to enhance their robustness.
10 CONCLUSION
This paper provides a comprehensive survey of Small Language Models (SLMs) with up to 7 billion parameters. Initially,
we address the need to clearly define SLMs due to existing ambiguities in their characterization. We then present the
foundational concepts essential for constructing SLMs. The survey progresses to explore enhancement techniques,
including knowledge distillation and quantization, as well as strategies for adapting Large Language Models (LLMs) to
SLM contexts. We survey representative SLMs, both general-domain and domain-specific, discussing their preferred
datasets and architectural decisions. We also assess their applications across various tasks and deployment strategies
on devices. Further, we investigate their role in augmenting the capabilities of LLMs, serving as proxies for fine-tuning
and facilitating two types of synergies: cloud-local and task-centric. Additionally, we discuss the critical aspect of their
trustworthiness. The paper concludes with key insights aimed at guiding future research on small language models.
Manuscript submitted to ACM
60 Fali Wang, et al.
REFERENCES
[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari,
Harkirat Behl, et al. 2024. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219 (2024).
[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam
Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).
[3] Emre Can Acikgoz, Osman Batur ƒ∞nce, Rayene Bench, Arda Anƒ±l Boz, ƒ∞lker Kesen, Aykut Erdem, and Erkut Erdem. 2024. Hippocrates: An
Open-Source Framework for Advancing Large Language Models in Healthcare. arXiv preprint arXiv:2404.16621 (2024).
[4] Harshavardhan Adepu, Zhanpeng Zeng, Li Zhang, and Vikas Singh. 2024. FrameQuant: Flexible Low-Bit Quantization for Transformers. arXiv
preprint arXiv:2403.06082 (2024).
[5] Abien Fred Agarap. 2018. Deep Learning using Rectified Linear Units (ReLU). CoRR abs/1803.08375 (2018). arXiv:1803.08375 http://arxiv.org/abs/
1803.08375
[6] Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and Olivier Bachem. 2024. On-policy
distillation of language models: Learning from self-generated mistakes. In The Twelfth International Conference on Learning Representations .
[7] Meta AI. 2024. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models . https://ai.meta.com/blog/llama-3-2-connect-2024-
vision-edge-mobile-devices/ Accessed: 2024-9-25.
[8] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr√≥n, and Sumit Sanghai. 2023. GQA: Training Generalized
Multi-Query Transformer Models from Multi-Head Checkpoints. arXiv:2305.13245 [cs.CL] https://arxiv.org/abs/2305.13245
[9] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Leandro von Werra, and Thomas Wolf. 2024. SmolLM - blazingly fast and remarkably powerful.
[10] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M√©rouane Debbah, √âtienne Goffinet, Daniel
Hesslow, Julien Launay, Quentin Malartic, et al. 2023. The falcon series of open language models. arXiv preprint arXiv:2311.16867 (2023).
[11] Guilherme FCF Almeida, Jos√© Luiz Nunes, Neele Engelmann, Alex Wiegmann, and Marcelo de Ara√∫jo. 2024. Exploring the psychology of LLMs‚Äô
moral and legal reasoning. Artificial Intelligence 333 (2024), 104145.
[12] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang,
Jeff Rasley, et al. 2022. Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale. In SC22: International
Conference for High Performance Computing, Networking, Storage and Analysis . IEEE, 1‚Äì15.
[13] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang. 2024. Fluctuation-based adaptive structured pruning for large language models. In
Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 10865‚Äì10873.
[14] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey,
Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403 (2023).
[15] AI Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card 1 (2024).
[16] David Anugraha, Genta Indra Winata, Chenyue Li, Patrick Amadeus Irawan, and En-Shiun Annie Lee. 2024. ProxyLM: Predicting Language Model
Performance on Multilingual Tasks via Proxy Models. arXiv preprint arXiv:2406.09334 (2024).
[17] Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet √úst√ºn, and Sara Hooker. 2024. To
Code, or Not To Code? Exploring Impact of Code in Pre-training. arXiv preprint arXiv:2408.10914 (2024).
[18] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through
Self-Reflection. In The Twelfth International Conference on Learning Representations . https://openreview.net/forum?id=hSyW5go0v8
[19] Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. 2024. SliceGPT: Compress Large
Language Models by Deleting Rows and Columns. In The Twelfth International Conference on Learning Representations . https://openreview.net/
forum?id=vXxardq6db
forum?id=vXxardq6db
[20] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le,
et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 (2021).
[21] Amos Azaria and Tom Mitchell. 2023. The Internal State of an LLM Knows When It‚Äôs Lying. In Findings of the Association for Computational
Linguistics: EMNLP 2023 . 967‚Äì976.
Linguistics: EMNLP 2023 . 967‚Äì976.
[22] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean
Welleck. 2023. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631 (2023).
[23] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li,
Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan,
Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang,
Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou,
Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen Technical Report. arXiv:2309.16609 [cs.CL] https://arxiv.org/abs/2309.16609
[24] Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. The pushshift reddit dataset. In Proceedings of
the international AAAI conference on web and social media , Vol. 14. 830‚Äì839.
[25] Maximilian Beck, Korbinian P√∂ppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G√ºnter Klambauer, Johannes
Brandstetter, and Sepp Hochreiter. 2024. xLSTM: Extended Long Short-Term Memory. InThe Thirty-eighth Annual Conference on Neural Information
Processing Systems . https://openreview.net/forum?id=ARAxPPIAhq
Manuscript submitted to ACM
A Survey of Small Language Models 61
[26] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan
Cooper, Ashish Datta, et al. 2024. Stable lm 2 1.6 b technical report. arXiv preprint arXiv:2402.17834 (2024).
[27] Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. 2024. SmolLM-Corpus. https://huggingface.co/
datasets/HuggingFaceTB/smollm-corpus
datasets/HuggingFaceTB/smollm-corpus
[28] Benjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, and Babak Ehteshami Bejnordi. 2024. Think Big, Generate Quick:
LLM-to-SLM for Fast Autoregressive Decoding. arXiv preprint arXiv:2402.16844 (2024).
[29] Milan Bhan, Jean-Noel Vittaut, Nicolas Chesneau, and Marie-Jeanne Lesot. 2024. Self-AMPLIFY: Improving Small Language Models with Self Post
Hoc Explanations. arXiv preprint arXiv:2402.12038 (2024).
[30] Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, and Huajun Chen. 2023. Oceangpt: A large language model for ocean
science tasks. arXiv preprint arXiv:2310.02031 (2023).
[31] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O‚ÄôBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A Suite for Analyzing Large Language
Models Across Training and Scaling. arXiv:2304.01373 [cs.CL] https://arxiv.org/abs/2304.01373
[32] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. Piqa: Reasoning about physical commonsense in natural language. In Proceedings
of the AAAI conference on artificial intelligence , Vol. 34. 7432‚Äì7439.
[33] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-
Tensorflow. https://doi.org/10.5281/zenodo.5297715 If you use this software, please cite it using these metadata..
[34] Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang,
Michael Carbin, et al. 2024. Biomedlm: A 2.7 b parameter language model trained on biomedical text. arXiv preprint arXiv:2403.18421 (2024).
[35] William James Bolton, Rafael Poyiadzi, Edward R Morrell, Gabriela van Bergen Gonzalez Bueno, and Lea Goetz. 2024. RAmBLA: A Framework for
Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain. arXiv preprint arXiv:2403.14578 (2024).
[36] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. Inpars: Data augmentation for information retrieval using large
language models. arXiv preprint arXiv:2202.05144 (2022).
[37] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish
Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,
Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural
Information Processing Systems , H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877‚Äì1901.
https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf
[38] Saikiran Bulusu, Bhavya Kailkhura, Bo Li, Pramod K Varshney, and Dawn Song. 2020. Anomalous example detection in deep learning: A survey.
IEEE Access 8 (2020), 132330‚Äì132347.
[39] Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. 2024. A survey on mixture of experts. Authorea Preprints (2024).
[40] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. 2024. Internlm2
technical report. arXiv preprint arXiv:2403.17297 (2024).
[41] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar
Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21) . 2633‚Äì2650.
[42] Samuel Carreira, Tom√°s Marques, Jos√© Ribeiro, and Carlos Grilo. 2023. Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT
LLM on Mobile. arXiv:2310.01434 [cs.CL] https://arxiv.org/abs/2310.01434
[43] I√±igo Casanueva, Tadas Temƒçinas, Daniela Gerz, Matthew Henderson, and Ivan Vuliƒá. 2020. Efficient Intent Detection with Dual Sentence Encoders.
In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI . Association for Computational Linguistics, Online, 38‚Äì45.
[44] Wei-Cheng Chang, X Yu Felix, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. [n. d.]. Pre-training Tasks for Embedding-based Large-scale
Retrieval. In International Conference on Learning Representations .
[45] Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas
Flammarion, George J Pappas, Florian Tramer, et al. 2024. Jailbreakbench: An open robustness benchmark for jailbreaking large language models.
arXiv preprint arXiv:2404.01318 (2024).
arXiv preprint arXiv:2404.01318 (2024).
[46] Dong Chen, Shuo Zhang, Yueting Zhuang, Siliang Tang, Qidong Liu, Hua Wang, and Mingliang Xu. 2024. Improving Large Models with Small
models: Lower Costs and Better Performance. arXiv preprint arXiv:2406.15471 (2024).
[47] Dong Chen, Yueting Zhuang, Shuo Zhang, Jinfeng Liu, Su Dong, and Siliang Tang. 2024. Data Shunt: Collaboration of Small and Large Models for
Lower Costs and Better Performance. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 11249‚Äì11257.
[48] Hongzhan Chen, Siyue Wu, Xiaojun Quan, Rui Wang, Ming Yan, and Ji Zhang. 2023. MCC-KD: Multi-CoT Consistent Knowledge Distillation. In
Findings of the Association for Computational Linguistics: EMNLP 2023 . 6805‚Äì6820.
[49] Lihu Chen and Ga√´l Varoquaux. 2024. What is the role of small models in the llm era: A survey. arXiv preprint arXiv:2409.06857 (2024).
[50] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas
Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).
[51] Tianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, and Luming Liang. 2023. Lorashear: Efficient large language model structured pruning and
knowledge recovery. arXiv preprint arXiv:2310.18356 (2023).
Manuscript submitted to ACM
62 Fali Wang, et al.
[52] Wei Chen, Zhiyuan Li, and Mingyuan Ma. 2024. Octopus: On-device language model for function calling of software APIs. arXiv:2404.01549 [cs.CL]
https://arxiv.org/abs/2404.01549
[53] Yangyi Chen, Fanchao Qi, Hongcheng Gao, Zhiyuan Liu, and Maosong Sun. 2022. Textual Backdoor Attacks Can Be More Harmful via Two Simple
Tricks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP . 11215‚Äì11221.
[54] Yangyi Chen, Xingyao Wang, Hao Peng, and Heng Ji. 2024. A Single Transformer for Scalable Vision-Language Modeling. Transactions on Machine
Learning Research (2024). https://openreview.net/forum?id=nuzFG0Rbhy
[55] Zeming Chen, Alejandro Hern√°ndez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan,
Andreas K√∂pf, Amirkeivan Mohtashami, et al. 2023. MEDITRON-70B: Scaling Medical Pretraining for Large Language Models. arXiv preprint
arXiv:2311.16079 (2023).
arXiv:2311.16079 (2023).
[56] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R
Routledge, et al. 2021. FinQA: A Dataset of Numerical Reasoning over Financial Data. In Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing . 3697‚Äì3711.
[57] Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. 2022. ConvFinQA: Exploring the Chain of
Numerical Reasoning in Conversational Finance Question Answering. In Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing . 6279‚Äì6292.
[58] Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Hongzhi Zhang, Fuzheng Zhang, Di Zhang, Kun Gai, and Ji-Rong Wen. 2024. Small Agent Can Also
Rock! Empowering Small Language Models as Hallucination Detector. arXiv preprint arXiv:2406.11277 (2024).
[59] Steffi Chern, Zhulin Hu, Yuqing Yang, Ethan Chern, Yuan Guo, Jiahe Jin, Binjie Wang, and Pengfei Liu. 2024. BeHonest: Benchmarking Honesty of
Large Language Models. arXiv preprint arXiv:2406.13261 (2024).
[60] Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. 2024. InstructEval: Towards Holistic Evaluation of Instruction-Tuned Large
Language Models. In Proceedings of the First edition of the Workshop on the Scaling Behavior of Large Language Models (SCALE-LLM 2024) . 35‚Äì64.
[61] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.org/blog/2023-
03-30-vicuna/
[62] Yae Jee Cho, Luyang Liu, Zheng Xu, Aldi Fahrezi, and Gauri Joshi. 2024. Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation
Models. arXiv:2401.06432 [cs.LG] https://arxiv.org/abs/2401.06432
[63] Xiaokai Chu, Jiashu Zhao, Lixin Zou, and Dawei Yin. 2022. H-ERNIE: A multi-granularity pre-trained language model for web search. InProceedings
of the 45th International ACM SIGIR conference on research and development in information retrieval . 1478‚Äì1489.
[64] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha
Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research 25, 70 (2024), 1‚Äì53.
[65] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved
question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 (2018).
[66] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton,
Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 (2021).
[67] Together Computer. 2023. RedPajama: an Open Dataset for Training Large Language Models . https://github.com/togethercomputer/RedPajama-Data
[68] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023.Free
Dolly: Introducing the World‚Äôs First Truly Open Instruction-Tuned LLM . https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-
viable-instruction-tuned-llm
[69] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 deep learning track. CoRR abs/2102.07662
(2021). arXiv:2102.07662 https://arxiv.org/abs/2102.07662
[70] Justin Cui, Wei-Lin Chiang, Ion Stoica, and Cho-Jui Hsieh. 2024. OR-Bench: An Over-Refusal Benchmark for Large Language Models. arXiv
preprint arXiv:2405.20947 (2024).
[71] Shiyao Cui, Zhenyu Zhang, Yilong Chen, Wenyuan Zhang, Tianyun Liu, Siqi Wang, and Tingwen Liu. 2023. Fft: Towards harmlessness evaluation
and analysis for llms with factuality, fairness, toxicity. arXiv preprint arXiv:2311.18580 (2023).
[72] Luigi Daniele and Suphavadeeprasit. 2023. Amplify-Instruct: Synthetically Generated Diverse Multi-turn Conversations for efficient LLM Training.
arXiv preprint arXiv:(coming soon) (2023). https://huggingface.co/datasets/LDJnr/Capybara
[73] Tri Dao. [n. d.]. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. In The Twelfth International Conference on
Learning Representations .
Learning Representations .
[74] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness.
Advances in Neural Information Processing Systems 35 (2022), 16344‚Äì16359.
[75] Tri Dao and Albert Gu. 2024. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. arXiv
preprint arXiv:2405.21060 (2024).
preprint arXiv:2405.21060 (2024).
[76] Rocktim Jyoti Das, Liqun Ma, and Zhiqiang Shen. 2023. Beyond size: How gradients shape pruning decisions in large language models. arXiv
preprint arXiv:2311.04902 (2023).
[77] Anirban Dasgupta, Ravi Kumar, and Tam√°s Sarl√≥s. 2011. Fast locality-sensitive hashing. In Proceedings of the 17th ACM SIGKDD international
conference on Knowledge discovery and data mining . 1073‚Äì1081.
Manuscript submitted to ACM
A Survey of Small Language Models 63
[78] Pieter Delobelle, Ewoenam Kwaku Tokpo, Toon Calders, and Bettina Berendt. 2022. Measuring fairness with biased rulers: A comparative study on
bias metrics for pre-trained language models. InProceedings of the 2022 Conference of the North American Chapter of the Association for Computational
Linguistics. 1693‚Äì1706.
[79] Yongheng Deng, Ziqing Qiao, Ju Ren, Yang Liu, and Yaoxue Zhang. 2023. Mutual enhancement of large and small language models with cross-silo
knowledge transfer. arXiv preprint arXiv:2312.05842 (2023).
[80] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale.
In Advances in Neural Information Processing Systems , Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.). https:
//openreview.net/forum?id=dXiGWqBoxaD
//openreview.net/forum?id=dXiGWqBoxaD
[81] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized llms. Advances in Neural
Information Processing Systems 36 (2024).
[82] Tim Dettmers and Luke Zettlemoyer. 2023. The case for 4-bit precision: k-bit inference scaling laws. In International Conference on Machine
Learning. PMLR, 7750‚Äì7774.
Learning. PMLR, 7750‚Äì7774.
[83] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language
Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers) . 4171‚Äì4186.
[84] Nolan Dey, Gurpreet Gosal, Zhiming Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, and Joel Hestness. 2023. Cerebras-GPT:
Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster. CoRR abs/2304.03208 (2023).
[85] Richard Diehl Martinez, Pietro Lesci, and Paula Buttery. 2024. Tending Towards Stability: Convergence Challenges in Small Language Models. In
Findings of the Association for Computational Linguistics: EMNLP 2024 , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for
Computational Linguistics, Miami, Florida, USA, 3275‚Äì3286. https://doi.org/10.18653/v1/2024.findings-emnlp.187
[86] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat
language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233 (2023).
[87] Tinghe Ding. 2024. MobileAgent: enhancing mobile control via human-machine interaction and SOP integration. arXiv:2401.04124 [cs.HC]
https://arxiv.org/abs/2401.04124
[88] Ricardo Dominguez-Olmedo, Moritz Hardt, and Celestine Mendler-D√ºnner. 2023. Questioning the survey responses of large language models.
arXiv preprint arXiv:2306.07951 (2023).
[89] Qian Dong, Yiding Liu, Qingyao Ai, Haitao Li, Shuaiqiang Wang, Yiqun Liu, Dawei Yin, and Shaoping Ma. 2023. I3 retriever: incorporating implicit
interaction in pre-trained language models for passage retrieval. In Proceedings of the 32nd ACM International Conference on Information and
Knowledge Management . 441‚Äì451.
[90] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung
Chen, Yoshi Suhara, et al. 2024. Hymba: A Hybrid-head Architecture for Small Language Models. arXiv preprint arXiv:2411.13676 (2024).
[91] Jason Xiaotian Dou, Haiyi Mao, Runxue Bao, Paul Pu Liang, Xiaoqing Tan, Shiyi Zhang, Minxue Jia, Pengfei Zhou, and Zhi-Hong Mao. 2023.
The Measurement of Knowledge in Knowledge Graphs. In Proceedings of the AAAI 2023 Workshop on Representation Learning for Responsible
Human-Centric AI (R2HCAI) . Association for the Advancement of Artificial Intelligence (AAAI) Washington . . . .
[92] Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo,
Guorui Zhou, Wenhu Chen, and Ge Zhang. 2024. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. arXiv:2404.04167 [cs.CL]
https://arxiv.org/abs/2404.04167
[93] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model Pretraining with
Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) .
320‚Äì335.
[94] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang,
Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024).
[95] Kazuki Egashira, Mark Vero, Robin Staab, Jingxuan He, and Martin Vechev. 2024. Exploiting LLM Quantization. arXiv preprint arXiv:2405.18137
(2024).
[96] Ronen Eldan and Yuanzhi Li. 2023. Tinystories: How small can language models be and still speak coherent english?arXiv preprint arXiv:2305.07759
(2023).
[97] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. 2018. Sigmoid-weighted linear units for neural network function approximation in reinforcement
learning. Neural networks 107 (2018), 3‚Äì11.
learning. Neural networks 107 (2018), 3‚Äì11.
[98] David Esiobu, Xiaoqing Tan, Saghar Hosseini, Megan Ung, Yuchen Zhang, Jude Fernandes, Jane Dwivedi-Yu, Eleonora Presani, Adina Williams,
and Eric Smith. 2023. ROBBIE: Robust bias evaluation of large generative language models. In Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing . 3764‚Äì3814. https://aclanthology.org/2023.emnlp-main.230
[99] Hugging Face. 2024. SmolVLM - small yet mighty Vision Language Model . https://huggingface.co/blog/smolvlm Accessed: 2024-11-26.
[100] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
Journal of Machine Learning Research 23, 120 (2022), 1‚Äì39.
[101] Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. 2023. Knowledge Card: Filling LLMs‚Äô Knowledge
Gaps with Plug-in Specialized Language Models. arXiv preprint arXiv:2305.09955 (2023).
Manuscript submitted to ACM
64 Fali Wang, et al.
[102] Elias Frantar and Dan Alistarh. 2023. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on
Machine Learning . PMLR, 10323‚Äì10337.
[103] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained
Transformers. In The Eleventh International Conference on Learning Representations .
[104] Hao Fu, Yao; Peng and Tushar Khot. 2022. How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources. Yao
Fu‚Äôs Notion (Dec 2022). https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-
Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1
[105] Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023. Specializing smaller language models towards multi-step reasoning. In
International Conference on Machine Learning . PMLR, 10421‚Äì10430.
[106] Philip Gage. 1994. A new algorithm for data compression. The C Users Journal 12, 2 (1994), 23‚Äì38.
[107] Chongming Gao, Shiqi Wang, Shijun Li, Jiawei Chen, Xiangnan He, Wenqiang Lei, Biao Li, Yuan Zhang, and Peng Jiang. 2023. CIRS: Bursting filter
bubbles by counterfactual interactive recommender system. ACM Transactions on Information Systems 42, 1 (2023), 1‚Äì27.
[108] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al.
2020. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 (2020).
[109] Luyu Gao and Jamie Callan. 2022. Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval. In Proceedings of the
60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . 2843‚Äì2853.
[110] Shangqian Gao, Chi-Heng Lin, Ting Hua, Zheng Tang, Yilin Shen, Hongxia Jin, and Yen-Chang Hsu. 2024. DISP-LLM: Dimension-Independent
Structural Pruning for Large Language Models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems . https:
//openreview.net/forum?id=YxaY6tHgg0
[111] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. 2024. Model Tells You What to Discard: Adaptive KV Cache
Compression for LLMs. In The Twelfth International Conference on Learning Representations . https://openreview.net/forum?id=uNrFpDPMyo
[112] Alex Gichamba, Tewodros Kederalah Idris, Brian Ebiyau, Eric Nyberg, and Teruko Mitamura. 2024. ColBERT Retrieval and Ensemble Response
Scoring for Language Model Question Answering. arXiv:2408.10808 [cs.CL] https://arxiv.org/abs/2408.10808
[113] Karan Goel. 2024. The On-Device Intelligence Update . https://www.cartesia.ai/blog/on-device
[114] Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. 2019. Openwebtext corpus.
[115] Shreya Goyal, Sumanth Doddapaneni, Mitesh M Khapra, and Balaraman Ravindran. 2023. A survey of adversarial defenses and robustness in nlp.
Comput. Surveys 55, 14s (2023), 1‚Äì39.
[116] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson,
Yizhong Wang, et al. 2024. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838 (2024).
[117] Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023).
[118] Naibin Gu, Peng Fu, Xiyu Liu, Bowen Shen, Zheng Lin, and Weiping Wang. 2024. Light-PEFT: Lightening Parameter-Efficient Fine-Tuning via
Early Pruning. In Findings of the Association for Computational Linguistics: ACL 2024 , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.).
Association for Computational Linguistics, Bangkok, Thailand, 7528‚Äì7541. https://doi.org/10.18653/v1/2024.findings-acl.447
[119] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2024. MiniLLM: Knowledge distillation of large language models. In The Twelfth International
Conference on Learning Representations .
Conference on Learning Representations .
[120] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C√©sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann,
Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S√©bastien Bubeck, Ronen Eldan, Adam Tauman Kalai,
Yin Tat Lee, and Yuanzhi Li. 2023. Textbooks Are All You Need. arXiv:2306.11644 [cs.CL] https://arxiv.org/abs/2306.11644
[121] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. 2024. DeepSeek-Coder:
When the Large Language Model Meets Programming‚ÄìThe Rise of Code Intelligence. arXiv preprint arXiv:2401.14196 (2024).
[122] Jinyang Guo, Jianyu Wu, Zining Wang, Jiaheng Liu, Ge Yang, Yifu Ding, Ruihao Gong, Haotong Qin, and Xianglong Liu. 2024. Compressing large
language models by joint sparsification and quantization. In Forty-first International Conference on Machine Learning .
[123] Shangwei Guo, Chunlong Xie, Jiwei Li, Lingjuan Lyu, and Tianwei Zhang. 2022. Threats to pre-trained language models: Survey and taxonomy.
arXiv preprint arXiv:2202.06862 (2022).
[124] Song Guo, Jiahang Xu, Li Lyna Zhang, and Mao Yang. 2023. Compresso: Structured pruning with collaborative prompting learns compact large
language models. arXiv preprint arXiv:2310.05015 (2023).
[125] Zhen Guo, Peiqi Wang, Yanwei Wang, and Shangdi Yu. 2023. Improving Small Language Models on PubMedQA via Generative Data Augmentation.
arXiv:2305.07804 [cs.CL] https://arxiv.org/abs/2305.07804
[126] Song Han, Jeff Pool, John Tran, and William Dally. 2015. Learning both weights and connections for efficient neural network. Advances in neural
information processing systems 28 (2015).
information processing systems 28 (2015).
[127] Zixu Hao, Huiqiang Jiang, Shiqi Jiang, Ju Ren, and Ting Cao. 2024. Hybrid SLM and LLM for Edge-Cloud Collaborative Inference. In Proceedings of
the Workshop on Edge and Mobile Foundation Models . 36‚Äì41.
[128] Tim Hartill, Diana Benavides-Prado, Michael Witbrock, and Patricia J. Riddle. 2023. Answering Unseen Questions With Smaller Language Models
Using Rationale Generation and Dense Retrieval. arXiv:2308.04711 [cs.CL] https://arxiv.org/abs/2308.04711
[129] Tim Hartill, Neset Tan, Michael Witbrock, and Patricia J. Riddle. 2023. Teaching Smaller Language Models To Generalise To Unseen Compositional
Questions. arXiv:2308.00946 [cs.CL] https://arxiv.org/abs/2308.00946
Manuscript submitted to ACM
A Survey of Small Language Models 65
[130] Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, and Erik Cambria. 2023. A survey of large language models for healthcare:
from data, technology, and applications to accountability and ethics. arXiv preprint arXiv:2310.05694 (2023).
[131] Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-
Disentangled Embedding Sharing. In The Eleventh International Conference on Learning Representations . https://openreview.net/forum?id=sE7-
XhLxHA
[132] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint
arXiv:2006.03654 (2020).
[133] Narges Heidari, Parham Moradi, and Abbas Koochari. 2022. An attention-based deep learning method for solving the cold-start and sparsity issues
of recommender systems. Knowledge-Based Systems 256 (2022), 109835.
[134] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring Massive Multitask
Language Understanding. In International Conference on Learning Representations . https://openreview.net/forum?id=d7KBjmI3GmQ
[135] Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 (2016).
[136] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015).
[137] Sepp Hochreiter and J√ºrgen Schmidhuber. 1996. LSTM can solve hard long time lag problems. Advances in neural information processing systems 9
(1996).
[138] Junyuan Hong, Jinhao Duan, Chenhui Zhang, Zhangheng Li, Chulin Xie, Kelsey Lieberman, James Diffenderfer, Brian R. Bartoldson, Ajay Kumar
Jaiswal, Kaidi Xu, Bhavya Kailkhura, Dan Hendrycks, Dawn Song, Zhangyang Wang, and Bo Li. 2024. Decoding Compressed Trust: Scrutinizing
the Trustworthiness of Efficient LLMs Under Compression. In Proceedings of the Forty-first International Conference on Machine Learning, ICML .
https://openreview.net/forum?id=e3Dpq3WdMv
[139] Yutong Meng Yuhao Wang Hongcheng Liu, Yusheng Liao. 2023. XieZhi: Chinese Law Large Language Model. https://github.com/LiuHC0428/
LAW_GPT.
LAW_GPT.
[140] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
Gelly. 2019. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning . PMLR, 2790‚Äì2799.
[141] Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
2023. Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. In Findings of the
Association for Computational Linguistics: ACL 2023 . 8003‚Äì8017.
[142] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
of large language models. arXiv preprint arXiv:2106.09685 (2021).
[143] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. 2024. Minicpm:
Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395 (2024).
[144] Xing Hu, Yuan Chen, Dawei Yang, Sifan Zhou, Zhihang Yuan, Jiangyong Yu, and Chen Xu. 2024. I-LLM: Efficient Integer-Only Inference for
Fully-Quantized Low-Bit Large Language Models. arXiv preprint arXiv:2405.17849 (2024).
[145] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. [n. d.]. Large Language Models Can Self-Improve.
In The 2023 Conference on Empirical Methods in Natural Language Processing .
[146] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing
Qin, et al. 2023. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint
arXiv:2311.05232 (2023).
[147] Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele Magno, and Xiaojuan Qi. 2024. Billm: Pushing the limit
of post-training quantization for llms. arXiv preprint arXiv:2402.04291 (2024).
[148] Wenyu Huang, Guancheng Zhou, Hongru Wang, Pavlos Vougiouklis, Mirella Lapata, and Jeff Z. Pan. 2024. Less is More: Making Smaller
Language Models Competent Subgraph Retrievers for Multi-hop KGQA. In Findings of the Association for Computational Linguistics: EMNLP
2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 15787‚Äì15803.
https://doi.org/10.18653/v1/2024.findings-emnlp.927
[149] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. 2024.
C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems 36 (2024).
[150] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. 2023. Catastrophic Jailbreak of Open-source LLMs via Exploiting
Generation. arXiv preprint arXiv:2310.06987 (2023).
[151] Yuheng Huang, Jiayang Song, Zhijie Wang, Shengming Zhao, Huaming Chen, Felix Juefei-Xu, and Lei Ma. 2023. Look before you leap: An
exploratory study of uncertainty measurement for large language models. arXiv preprint arXiv:2307.10236 (2023).
[152] Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. [n. d.]. Poly-encoders: Architectures and Pre-training Strategies for Fast
and Accurate Multi-sentence Scoring. In International Conference on Learning Representations .
[153] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine,
et al. 2023. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674 (2023).
[154] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive mixtures of local experts. Neural computation 3, 1
(1991), 79‚Äì87.
Manuscript submitted to ACM
66 Fali Wang, et al.
[155] Mojan Javaheripi, S√©bastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio C√©sar Teodoro Mendes, Weizhu Chen, Allie Del Giorno,
Ronen Eldan, Sivakanth Gopi, et al. 2023. Phi-2: The surprising power of small language models. Microsoft Research Blog (2023).
[156] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Hwang, and Jong C Park. 2023. Test-Time Self-Adaptive Small Language Models for Question
Answering. In Findings of the Association for Computational Linguistics: EMNLP 2023 . 15459‚Äì15469.
[157] Ananya Harsh Jha, Tom Sherborne, Evan Pete Walsh, Dirk Groeneveld, Emma Strubell, and Iz Beltagy. 2024. Just CHOP: Embarrassingly Simple
LLM Compression. arXiv:2305.14864 [cs.CL] https://arxiv.org/abs/2305.14864
[158] Yixin Ji, Yang Xiang, Juntao Li, Wei Chen, Zhongyi Liu, Kehai Chen, and Min Zhang. 2024. Feature-based Low-Rank Compression of Large
Language Models via Bayesian Optimization. arXiv preprint arXiv:2405.10616 (2024).
[159] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023. Towards mitigating LLM hallucination via self reflection. In
Findings of the Association for Computational Linguistics: EMNLP 2023 . 1827‚Äì1843.
[160] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023).
[161] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las
Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088 (2024).
[162] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2024. LongLLMLingua: Accelerating and
Enhancing LLMs in Long Context Scenarios via Prompt Compression. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of
Foundation Models . https://openreview.net/forum?id=9YvfRrpmyw
[163] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: A Dataset for Biomedical Research Question
Answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP) . 2567‚Äì2577.
[164] Rudolph Emil Kalman. 1960. A new approach to linear filtering and prediction problems. (1960).
[165] Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, and Tuo Zhao. 2024. Gear: An efficient kv cache
compression recipefor near-lossless generative inference of llm. arXiv preprint arXiv:2403.05527 (2024).
[166] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario
Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).
[167] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T
Joshi, Hanna Moazam, et al. 2023. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714
(2023).
[168] Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, and Dongsoo Lee. 2024. Memory-efficient fine-tuning
of compressed large language models via sub-4-bit integer quantization. Advances in Neural Information Processing Systems 36 (2024).
[169] Minsoo Kim, Sihwa Lee, Janghwan Lee, Sukjin Hong, Du-Seong Chang, Wonyong Sung, and Jungwook Choi. 2024. Token-scaled logit distillation
for ternary weight generative language models. Advances in Neural Information Processing Systems 36 (2024).
[170] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. 2023. Squeezellm:
Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629 (2023).
[171] Yoon Kim and Alexander M Rush. 2016. Sequence-Level Knowledge Distillation. In Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing . 1317‚Äì1327.
[172] Young Jin Kim, Raffy Fahim, and Hany Hassan Awadalla. 2023. Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit
Quantization and Robustness. arXiv preprint arXiv:2310.02410 (2023).
[173] Jongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-Young Yun. 2024. DistiLLM: Towards Streamlined Distillation for Large Language Models. arXiv
preprint arXiv:2402.03898 (2024).
[174] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu√±oz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes,
Thomas Wolf, et al. 2022. The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533 (2022).
[175] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language
Generation. In Proceedings of the Eleventh International Conference on Learning Representations . https://openreview.net/forum?id=VD-AYtP0dve
[176] Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal, and Prashanth Harshangi. 2024. Fine-Tuning, Quantization, and LLMs: Navigating Unintended
Outcomes. arXiv preprint arXiv:2404.04392 (2024).
Outcomes. arXiv preprint arXiv:2404.04392 (2024).
[177] Ohjoon Kwon, Donghyeon Jeon, Nayoung Choi, Gyu-Hwung Cho, Hwiyeol Jo, Changbong Kim, Hyunwoo Lee, Inho Kang, Sun Kim, and Taiwoo
Park. 2024. SLM as Guardian: Pioneering AI Safety with Small Language Model. In Proceedings of the 2024 Conference on Empirical Methods
in Natural Language Processing: Industry Track , Franck Dernoncourt, Daniel Preo≈£iuc-Pietro, and Anastasia Shimorina (Eds.). Association for
Computational Linguistics, Miami, Florida, US, 1333‚Äì1350. https://doi.org/10.18653/v1/2024.emnlp-industry.99
[178] Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. 2024. Biomistral: A collection of
open-source pretrained large language models for medical domains. arXiv preprint arXiv:2402.10373 (2024).
[179] Stefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K. Kummerfeld, Kevin Leach, Michael A.
Laurenzano, Lingjia Tang, and Jason Mars. 2019. An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction. In Proceedings of the
2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, Hong Kong, China,
Manuscript submitted to ACM
A Survey of Small Language Models 67
1311‚Äì1316. https://doi.org/10.18653/v1/D19-1131
[180] Hugo Lauren√ßon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao
Mou, Eduardo Gonz√°lez Ponferrada, Huu Nguyen, et al. 2022. The bigscience roots corpus: A 1.6 tb composite multilingual dataset. Advances in
Neural Information Processing Systems 35 (2022), 31809‚Äì31826.
[181] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Iliƒá, Daniel Hesslow, Roman Castagn√©, Alexandra Sasha Luccioni, Fran√ßois
Yvon, Matthias Gall√©, et al. 2023. Bloom: A 176b-parameter open-access multilingual language model. (2023).
[182] Hojae Lee, Junho Kim, and SangKeun Lee. 2024. Mentor-KD: Making Small Language Models Better Multi-step Reasoners. In Proceedings of the
2024 Conference on Empirical Methods in Natural Language Processing , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for
Computational Linguistics, Miami, Florida, USA, 17643‚Äì17658. https://doi.org/10.18653/v1/2024.emnlp-main.977
[183] Jooyoung Lee, Fan Yang, Thanh Tran, Qian Hu, Emre Barut, and Kai-Wei Chang. 2024. Can Small Language Models Help Large Language Models
Reason Better?: LM-Guided Chain-of-Thought. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language
Resources and Evaluation (LREC-COLING 2024) . 2835‚Äì2843.
[184] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan
Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. 2022. xFormers: A modular and hackable Transformer
modelling library. https://github.com/facebookresearch/xformers.
[185] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. ArXiv e-prints (2016), arXiv‚Äì1607.
[186] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol
Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing
Systems 35 (2022), 3843‚Äì3857.
Systems 35 (2022), 3843‚Äì3857.
[187] Chenglin Li, Qianglong Chen, Liangyue Li, Caiyu Wang, Yicheng Li, Zulong Chen, and Yin Zhang. 2023. Mixed distillation helps smaller language
model better reasoning. arXiv preprint arXiv:2312.10730 (2023).
[188] Guangyan Li, Yongqiang Tang, and Wensheng Zhang. 2024. LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for
Large Language Models. arXiv preprint arXiv:2404.09695 (2024).
[189] Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Zhijing Wu, Yiqun Liu, Chong Chen, and Qi Tian. 2024. BLADE: Enhancing Black-box Large Language
Models with Small Domain-Specific Models. arXiv:2403.18365 [cs.CL] https://arxiv.org/abs/2403.18365
[190] Haoran Li, Dadi Guo, Donghao Li, Wei Fan, Qi Hu, Xin Liu, Chunkit Chan, Duanyi Yao, Yuan Yao, and Yangqiu Song. 2024. PrivLM-Bench: A
Multi-level Privacy Evaluation Benchmark for Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational
Linguistics. 54‚Äì73.
[191] Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large
Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing . Association for Computational
Linguistics, Singapore, 6449‚Äì6464. https://doi.org/10.18653/v1/2023.emnlp-main.397
[192] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et al. 2024.
Datacomp-lm: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794 (2024).
[193] Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. Transformer-Lite: High-efficiency Deployment of Large Language
Models on Mobile Phone GPUs. arXiv:2403.20041 [cs.CL] https://arxiv.org/abs/2403.20041
[194] Pingzhi Li, Xiaolong Jin, Yu Cheng, and Tianlong Chen. 2024. Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark. arXiv
preprint arXiv:2406.08155 (2024).
[195] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny
Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo√£o Monteiro,
Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham
Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour
Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero,
Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson,
Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu√±oz
Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. StarCoder: may the source be with you!
arXiv:2305.06161 [cs.CL] https://arxiv.org/abs/2305.06161
[196] Shengrui Li, Xueting Han, and Jing Bai. 2024. Nuteprune: Efficient progressive pruning with numerous teachers for large language models. arXiv
preprint arXiv:2402.09773 (2024).
preprint arXiv:2402.09773 (2024).
[197] Tianlin Li, Qian Liu, Tianyu Pang, Chao Du, Qing Guo, Yang Liu, and Min Lin. 2024. Purifying large language models by ensembling a small
language model. arXiv preprint arXiv:2402.14845 (2024).
[198] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori B Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2023. Contrastive
Decoding: Open-ended Text Generation as Optimization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) . 12286‚Äì12312.
[199] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 (2021).
[200] Yuanzhi Li, S√©bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks Are All You Need II: phi-1.5
technical report. arXiv:2309.05463 [cs.CL] https://arxiv.org/abs/2309.05463
Manuscript submitted to ACM
68 Fali Wang, et al.
[201] Yun Li, Lin Niu, Xipeng Zhang, Kai Liu, Jianchen Zhu, and Zhanhui Kang. 2023. E-sparse: Boosting the large language model inference through
entropy-based n: M sparsity. arXiv preprint arXiv:2310.15929 (2023).
[202] Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2023. Large language models in finance: A survey. In Proceedings of the fourth ACM
international conference on AI in finance . 374‚Äì382.
[203] Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium". 2023. SlimOrca: An Open Dataset of
GPT-4 Augmented FLAN Reasoning Traces, with Verification. https://https://huggingface.co/Open-Orca/SlimOrca
[204] Jinggui Liang, Lizi Liao, Hao Fei, and Jing Jiang. 2024. Synergizing Large Language Models and Pre-Trained Smaller Models for Conversational
Intent Discovery. In Findings of the Association for Computational Linguistics ACL 2024 . 14133‚Äì14147.
[205] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu,
Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher
Re, Diana Acosta-Navas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue WANG,
Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter
Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi
Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2023. Holistic Evaluation of Language Models.
Transactions on Machine Learning Research (2023). https://openreview.net/forum?id=iO4LZibEqW
[206] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024. Rella:
Retrieval-enhanced large language models for lifelong sequential behavior comprehension in recommendation. In Proceedings of the ACM on Web
Conference 2024 . 3497‚Äì3508.
[207] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han.
2024. AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration. Proceedings of Machine Learning and
Systems 6 (2024), 87‚Äì100.
[208] Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring How Models Mimic Human Falsehoods. In Proceedings of the 60th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . 3214‚Äì3252.
[209] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du,
Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O‚ÄôHoro, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona
Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot Learning with Multilingual Generative Language Models. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Processing , Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational
Linguistics, Abu Dhabi, United Arab Emirates, 9019‚Äì9052. https://doi.org/10.18653/v1/2022.emnlp-main.616
[210] Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, et al. 2024. Rho-1: Not all
tokens are what you need. arXiv preprint arXiv:2404.07965 (2024).
[211] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al . 2024.
Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434 (2024).
[212] Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A Smith. 2024. Tuning language models by proxy. arXiv preprint
arXiv:2401.08565 (2024).
arXiv:2401.08565 (2024).
[213] Jiashuo Liu, Zheyan Shen, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. 2021. Towards out-of-distribution generalization: A survey.
arXiv preprint arXiv:2108.13624 (2021).
[214] Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2024. Once: Boosting content-based recommendation with both open-and closed-source
large language models. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining . 452‚Äì461.
[215] Suqing Liu, Zezhu Yu, Feiran Huang, Yousef Bulbulia, Andreas Bergen, and Michael Liut. 2024. Can Small Language Models With Retrieval-
Augmented Generation Replace Large Language Models When Learning Computer Science? InProceedings of the 2024 on Innovation and Technology
in Computer Science Education V. 1 . 388‚Äì393.
[216] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2023. What makes good data for alignment? a comprehensive study of automatic
data selection in instruction tuning. arXiv preprint arXiv:2312.15685 (2023).
[217] Yinpeng Liu, Jiawei Liu, Xiang Shi, Qikai Cheng, and Wei Lu. 2024. Let‚Äôs Learn Step by Step: Enhancing In-Context Learning Ability with
Curriculum Learning. arXiv preprint arXiv:2402.10738 (2024).
[218] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).
[219] Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin, Jiannan Cao, and Tianyu Du. 2024. RA-ISF: Learning to Answer and
Understand from Retrieval Augmentation via Iterative Self-Feedback. arXiv preprint arXiv:2403.06840 (2024).
[220] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. 2024.
Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information
Processing Systems 36 (2024).
[221] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas
Chandra. 2023. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888 (2023).
[222] Zhengxiao Liu, Bowen Shen, Zheng Lin, Fali Wang, and Weiping Wang. 2023. Maximum Entropy Loss, the Silver Bullet Targeting Backdoor Attacks
in Pre-trained Language Models. In Findings of the Association for Computational Linguistics: ACL 2023 , Anna Rogers, Jordan Boyd-Graber, and
Manuscript submitted to ACM
A Survey of Small Language Models 69
Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 3850‚Äì3868. https://doi.org/10.18653/v1/2023.findings-acl.237
[223] Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman
Krishnamoorthi, et al. 2024. Mobilellm: Optimizing sub-billion parameter language models for on-device use cases. arXiv preprint arXiv:2402.14905
(2024).
(2024).
[224] Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. 2024. On LLMs-Driven Synthetic Data Generation,
Curation, and Evaluation: A Survey. In Findings of the Association for Computational Linguistics ACL 2024 . 11065‚Äì11082.
[225] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The flan
collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning . PMLR, 22631‚Äì22648.
[226] Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad
Kabbara, Kartik Perisetla, et al. 2023. The data provenance initiative: A large scale audit of dataset licensing & attribution in ai. arXiv preprint
arXiv:2310.16787 (2023).
arXiv:2310.16787 (2023).
[227] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu,
Yuxiang Wei, et al. 2024. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173 (2024).
[228] Wenhao Lu, Jian Jiao, and Ruofei Zhang. 2020. Twinbert: Distilling knowledge to twin-structured compressed bert models for large-scale retrieval.
In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 2645‚Äì2652.
[229] Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D Lane, and Mengwei Xu. 2024. Small language models:
Survey, measurements, and insights. arXiv preprint arXiv:2409.15790 (2024).
[230] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. BioGPT: generative pre-trained transformer for
biomedical text generation and mining. Briefings in bioinformatics 23, 6 (2022), bbac409.
[231] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei. 2024.
The era of 1-bit llms: All large language models are in 1.58 bits. arXiv preprint arXiv:2402.17764 (2024).
[232] Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. Llm-pruner: On the structural pruning of large language models. Advances in neural
information processing systems 36 (2023), 21702‚Äì21720.
[233] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query Rewriting in Retrieval-Augmented Large Language Models. In
Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing . 5303‚Äì5315.
[234] Yubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun. 2023. Large language model is not a good few-shot information extractor, but a good
reranker for hard samples! arXiv preprint arXiv:2303.08559 (2023).
[235] Yuhan Ma, Chenyou Fan, and Haiqi Jiang. 2023. Sci-cot: Leveraging large language models for enhanced knowledge distillation in small models for
scientific qa. In 2023 9th International Conference on Computer and Communications (ICCC) . IEEE, 2394‚Äì2398.
[236] YINGWEI MA, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. 2024. At Which Training Stage Does Code Data
Help LLMs Reasoning?. In The Twelfth International Conference on Learning Representations . https://openreview.net/forum?id=KIPJKST4gw
[237] Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind Tafjord, Dustin Schwenk, Evan Pete Walsh, Yanai
Elazar, Kyle Lo, et al. 2023. Paloma: A benchmark for evaluating language model fit. arXiv preprint arXiv:2312.10523 (2023).
[238] Dakota Mahan, Ryan Carlow, Louis Castricato, Nathan Cooper, and Christian Laforte. [n. d.]. Stable Beluga models. [https://huggingface.co/
stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)
[239] Vladimir Malinovskii, Denis Mazur, Ivan Ilin, Denis Kuznedelev, Konstantin Burlachenko, Kai Yi, Dan Alistarh, and Peter Richtarik. 2024. PV-Tuning:
Beyond Straight-Through Estimation for Extreme LLM Compression. arXiv preprint arXiv:2405.14852 (2024).
[240] Potsawee Manakul, Adian Liusie, and Mark Gales. 2023. SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large
Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , Houda Bouamor, Juan Pino, and
Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 9004‚Äì9017. https://doi.org/10.18653/v1/2023.emnlp-main.557
[241] Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Seyed Iman Mirzadeh, Mahyar Najibi,
Dmitry Belenko, Peter Zatloukal, et al . 2024. Openelm: An efficient language model family with open training and inference framework. In
Workshop on Efficient Systems for Foundation Models II@ ICML2024 .
[242] Dheeraj Mekala, Alex Nguyen, and Jingbo Shang. 2024. Smaller language models are capable of selecting instruction-tuning training data for
larger language models. arXiv preprint arXiv:2402.10430 (2024).
[243] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. 2024. Shortgpt: Layers in large
language models are more redundant than you expect. arXiv preprint arXiv:2403.03853 (2024).
[244] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. In Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore,
12076‚Äì12100. https://doi.org/10.18653/v1/2023.emnlp-main.741
[245] Go Min-su. 2024. Deep Learning Bible - 8. Large Language Models. WikiDocs. https://wikidocs.net/237419
[246] Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher D Manning. 2024. An Emulator for Fine-tuning Large Language Models
using Small Language Models. In The Twelfth International Conference on Learning Representations . https://openreview.net/forum?id=Eo7kv0sllr
[247] Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik
Jones, Kriti Aggarwal, et al. 2023. Orca 2: Teaching small language models how to reason. arXiv preprint arXiv:2311.11045 (2023).
Manuscript submitted to ACM
70 Fali Wang, et al.
[248] Lingbo Mo, Boshi Wang, Muhao Chen, and Huan Sun. 2024. How Trustworthy are Open-Source LLMs? An Assessment under Malicious
Demonstrations Shows their Vulnerabilities. InProceedings of the 2024 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies (Volume 1: Long Papers) . 2775‚Äì2792.
[249] John Xavier Morris, Wenting Zhao, Justin T Chiu, Vitaly Shmatikov, and Alexander M Rush. 2024. Language Model Inversion. In The Twelfth
International Conference on Learning Representations . https://openreview.net/forum?id=t9dWHpGkPj
[250] Maximilian Mozes, Xuanli He, Bennett Kleinberg, and Lewis D Griffin. 2023. Use of llms for illicit purposes: Threats, prevention measures, and
vulnerabilities. arXiv preprint arXiv:2308.12833 (2023).
[251] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning
from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707 (2023).
[252] Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro,
Jan Kautz, and Pavlo Molchanov. 2024. Compact language models via pruning and knowledge distillation. arXiv preprint arXiv:2407.14679 (2024).
[253] Rithesh Murthy, Liangwei Yang, Juntao Tan, Tulika Manoj Awalgaonkar, Yilun Zhou, Shelby Heinecke, Sachin Desai, Jason Wu, Ran Xu, Sarah
Tan, Jianguo Zhang, Zhiwei Liu, Shirley Kokane, Zuxin Liu, Ming Zhu, Huan Wang, Caiming Xiong, and Silvio Savarese. 2024. MobileAIBench:
Benchmarking LLMs and LMMs for On-Device Use Cases. arXiv:2406.10290 [cs.CL] https://arxiv.org/abs/2406.10290
[254] Kalyan Nakka, Jimmy Dani, and Nitesh Saxena. 2024. Is On-Device AI Broken and Exploitable? Assessing the Trust and Ethics in Small Language
Models. arXiv preprint arXiv:2406.05364 (2024).
[255] Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. 2024. Using an llm to help with code understanding. In
Proceedings of the IEEE/ACM 46th International Conference on Software Engineering . 1‚Äì13.
[256] Piotr Nawrot, Adrian ≈Åa≈Ñcucki, Marcin Chochowski, David Tarjan, and Edoardo Ponti. 2024. Dynamic Memory Compression: Retrofitting LLMs
for Accelerated Inference. In Forty-first International Conference on Machine Learning . https://openreview.net/forum?id=tDRYrAkOB7
[257] Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A Rossi, and Thien Huu Nguyen.
2024. CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages. In Proceedings of the 2024 Joint
International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024) . 4226‚Äì4237.
[258] Tuan Dung Nguyen, Yuan-Sen Ting, Ioana Ciuca, Charles O‚ÄôNeill, Ze-Chang Sun, Maja Jab≈Ço≈Ñska, Sandor Kruk, Ernest Perkowski, Jack Miller,
Jason Jason Jingsh Li, et al. 2023. AstroLLaMA: Towards Specialized Foundation Models in Astronomy. In Proceedings of the Second Workshop on
Information Extraction from Scientific Publications . 49‚Äì55.
[259] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang.
2022. Large Dual Encoders Are Generalizable Retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing .
Association for Computational Linguistics, 9844‚Äì9855. https://doi.org/10.18653/v1/2022.emnlp-main.669
[260] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. arXiv preprint arXiv:1901.04085 (2019).
[261] A Noorian. 2024. A BERT-based sequential POI recommender system in social media. Computer Standards & Interfaces 87 (2024), 103766.
[262] OpenAI. 2024. GPT-4o mini: advancing cost-efficient intelligence . https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/
Accessed: 2024-7-18.
[263] OpenAI. 2024. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/ Accessed: 2024-5-13.
[264] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35
(2022), 27730‚Äì27744.
(2022), 27730‚Äì27744.
[265] Shankar Padmanabhan, Yasumasa Onoe, Michael Zhang, Greg Durrett, and Eunsol Choi. 2024. Propagating knowledge updates to lms through
distillation. Advances in Neural Information Processing Systems 36 (2024).
[266] Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha
Jhunjhunwala, Ayush Dattagupta, et al. 2024. Nemotron-4 15B Technical Report. arXiv preprint arXiv:2402.16819 (2024).
[267] Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. 2024. OpenWebMath: An Open Dataset of High-Quality Mathematical Web
Text. In The Twelfth International Conference on Learning Representations . https://openreview.net/forum?id=jKHmjlpViu
[268] Guilherme Penedo, Hynek Kydl√≠ƒçek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf.
2024. The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale. arXiv:2406.17557 [cs.CL]
[269] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam
Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only.
arXiv preprint arXiv:2306.01116 (2023).
[270] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon
Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bart≈Çomiej
Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanis≈Çaw
Wo≈∫niak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. 2023. RWKV: Reinventing RNNs for the Transformer Era. In Findings of
the Association for Computational Linguistics: EMNLP 2023 , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational
Linguistics, Singapore, 14048‚Äì14077. https://doi.org/10.18653/v1/2023.findings-emnlp.936
[271] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277
(2023).
Manuscript submitted to ACM
A Survey of Small Language Models 71
[272] Zhiyuan Peng, Xuyang Wu, Qifan Wang, and Yi Fang. 2023. Soft prompt tuning for augmenting dense retrieval with large language models. arXiv
preprint arXiv:2307.08303 (2023).
[273] Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav
Kadavath, Andy Jones, Anna Chen, Benjamin Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da Yan, Daniela Amodei,
Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon
Hyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson
Elhage, Nicholas Joseph, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, Shauna Kravec, Sheer
El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark,
Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, and Jared Kaplan. 2023.
Discovering Language Model Behaviors with Model-Written Evaluations. In Findings of the Association for Computational Linguistics: ACL 2023 .
13387‚Äì13434.
13387‚Äì13434.
[274] Pascal Pfeiffer, Philipp Singer, Yauhen Babakhin, Gabor Fodor, Nischay Dhankhar, and Sri Satish Ambati. 2024. H2O-Danube3 Technical Report.
arXiv preprint arXiv:2407.09276 (2024).
[275] Karmvir Singh Phogat, Sai Akhil Puranam, Sridhar Dasaratha, Chetan Harsha, and Shashishekar Ramakrishna. 2024. Fine-tuning Smaller
Language Models for Question Answering over Financial Documents. In Findings of the Association for Computational Linguistics: EMNLP 2024 ,
Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 10528‚Äì10548.
https://doi.org/10.18653/v1/2024.findings-emnlp.617
[276] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean.
2023. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems 5 (2023), 606‚Äì624.
[277] Ofir Press, Noah Smith, and Mike Lewis. 2022. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. In
International Conference on Learning Representations . https://openreview.net/forum?id=R8sQPpGCv0
[278] Ruiyang Qin, Jun Xia, Zhenge Jia, Meng Jiang, Ahmed Abbasi, Peipei Zhou, Jingtong Hu, and Yiyu Shi. 2023. Enabling on-device large language
model personalization with self-supervised data selection and synthesis. arXiv preprint arXiv:2311.12275 (2023).
[279] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. [n. d.]. ToolLLM: Facilitating
Large Language Models to Master 16000+ Real-world APIs. In The Twelfth International Conference on Learning Representations .
[280] Haohao Qu, Liangbo Ning, Rui An, Wenqi Fan, Tyler Derr, Hui Liu, Xin Xu, and Qing Li. 2024. A survey of mamba. arXiv preprint arXiv:2408.01129
(2024).
(2024).
[281] Haohao Qu, Yifeng Zhang, Liangbo Ning, Wenqi Fan, and Qing Li. 2024. Ssd4rec: a structured state space duality model for efficient sequential
recommendation. arXiv preprint arXiv:2409.01192 (2024).
[282] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al . 2019. Language models are unsupervised multitask
learners. OpenAI blog 1, 8 (2019), 9.
learners. OpenAI blog 1, 8 (2019), 9.
[283] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization:
Your language model is secretly a reward model. Advances in Neural Information Processing Systems 36 (2024).
[284] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research 21, 140 (2020), 1‚Äì67.
[285] Mohammad Wali Ur Rahman, Murad Mehrab Abrar, Hunter Gibbons Copening, Salim Hariri, Sicong Shao, Pratik Satam, and Soheil Salehi. 2023.
Quantized Transformer Language Model Implementations on Edge Devices. arXiv:2310.03971 [cs.CL] https://arxiv.org/abs/2310.03971
[286] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models.
In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis . IEEE, 1‚Äì16.
[287] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-Context Retrieval-
Augmented Language Models. Transactions of the Association for Computational Linguistics 11 (2023), 1316‚Äì1331.
[288] Krithika Ramesh, Arnav Chavan, Shrey Pandit, and Sunayana Sitaram. 2023. A Comparative Study on the Impact of Model Compression Techniques
on Fairness in Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) .
Association for Computational Linguistics, 15762‚Äì15782. https://aclanthology.org/2023.acl-long.878
[289] Al Mamunur Rashid, George Karypis, and John Riedl. 2008. Learning preferences of new users in recommender systems: an information theoretic
approach. Acm Sigkdd Explorations Newsletter 10, 2 (2008), 90‚Äì100.
[290] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. 2024. Androidinthewild: A large-scale dataset for android
device control. Advances in Neural Information Processing Systems 36 (2024).
[291] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends ¬Æ in Information
Retrieval 3, 4 (2009), 333‚Äì389.
[292] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J√©r√©my Rapin, et al.
2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023).
[293] Caitlin Sadowski and Greg Levin. 2007. Simhash: Hash-based similarity detection.
[294] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale.
Commun. ACM 64, 9 (2021), 99‚Äì106.
Manuscript submitted to ACM
72 Fali Wang, et al.
[295] Timo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas
Scialom. 2024. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems 36 (2024).
[296] Rico Sennrich, Jannis Vamvas, and Alireza Mohammadshahi. 2023. Mitigating Hallucinations and Off-target Machine Translation with Source-
Contrastive and Language-Contrastive Decoding. arXiv preprint arXiv:2309.07098 (2023).
[297] Zeyang Sha and Yang Zhang. 2024. Prompt stealing attacks against large language models. arXiv preprint arXiv:2402.12959 (2024).
[298] Yu Shang, Yu Li, Fengli Xu, and Yong Li. 2024. Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models. arXiv preprint
arXiv:2402.02563 (2024).
arXiv:2402.02563 (2024).
[299] Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. 2023. PB-LLM: Partially Binarized Large Language Models. arXiv:2310.00034 [cs.LG]
https://arxiv.org/abs/2310.00034
[300] Hang Shao, Bei Liu, and Yanmin Qian. 2024. One-shot sensitivity-aware mixed sparsity pruning for large language models. In ICASSP 2024-2024
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 11296‚Äì11300.
[301] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-
Dodds, Scott R Johnston, et al. 2023. Towards understanding sycophancy in language models. arXiv preprint arXiv:2310.13548 (2023).
[302] Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, and Nael Abu-Ghazaleh. 2023. Survey of vulnerabilities in large
language models revealed by adversarial attacks. arXiv preprint arXiv:2310.10844 (2023).
[303] Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150 (2019).
[304] Noam Shazeer. 2020. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 (2020).
[305] Bowen Shen, Zheng Lin, Yuanxin Liu, Zhengxiao Liu, Lei Wang, and Weiping Wang. 2022. COST-EFF: Collaborative Optimization of Spatial and
Temporal Efficiency with Slenderized Multi-exit Language Models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates,
1719‚Äì1730. https://doi.org/10.18653/v1/2022.emnlp-main.112
[306] Bowen Shen, Zheng Lin, Daren Zha, Wei Liu, Jian Luan, Bin Wang, and Weiping Wang. 2024. Pruning Large Language Models to Intra-module Low-
rank Architecture with Transitional Activations. In Findings of the Association for Computational Linguistics: ACL 2024 , Lun-Wei Ku, Andre Martins,
and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 9781‚Äì9793. https://doi.org/10.18653/v1/2024.findings-
acl.582
[307] Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang. 2024. Small LLMs Are Weak
Tool Learners: A Multi-LLM Agent. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing . Association for
Computational Linguistics, Miami, Florida, USA, 16658‚Äì16680. https://doi.org/10.18653/v1/2024.emnlp-main.929
[308] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher R√©, Ion Stoica, and Ce Zhang. 2023.
Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning . PMLR,
31094‚Äì31116.
31094‚Äì31116.
[309] Wentao Shi, Xiangnan He, Yang Zhang, Chongming Gao, Xinyue Li, Jizhi Zhang, Qifan Wang, and Fuli Feng. 2024. Large language models are
learnable planners for long-term recommendation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in
Information Retrieval . 1893‚Äì1903.
[310] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion
parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 (2019).
[311] Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan K Reddy. 2024. Llm-sr: Scientific equation discovery via
programming with large language models. arXiv preprint arXiv:2404.18400 (2024).
[312] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. 2022. Test-time prompt tuning for
zero-shot generalization in vision-language models. Advances in Neural Information Processing Systems 35 (2022), 14274‚Äì14289.
[313] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis,
Stephen Pfohl, et al. 2023. Large language models encode clinical knowledge. Nature 620, 7972 (2023), 172‚Äì180.
[314] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. 2023. SlimPajama: A 627B token cleaned
and deduplicated version of RedPajama. https://cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama.
https://huggingface.co/datasets/cerebras/SlimPajama-627B
[315] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas,
Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas
Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant
Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and
Kyle Lo. 2024. Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. arXiv preprint (2024).
[316] Xinying Song, Alex Salcianu, Yang Song, Dave Dopson, and Denny Zhou. 2021. Fast WordPiece Tokenization. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Processing . 2089‚Äì2103.
[317] Sofia Eleni Spatharioti, David M Rothschild, Daniel G Goldstein, and Jake M Hofman. 2023. Comparing traditional and llm-based search for
consumer choice: A randomized experiment. arXiv preprint arXiv:2307.03744 (2023).
[318] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position
embedding. Neurocomputing 568 (2024), 127063.
Manuscript submitted to ACM
A Survey of Small Language Models 73
[319] Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. 2024. Scieval: A multi-level large language model
evaluation benchmark for scientific research. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 19053‚Äì19061.
[320] Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et al. 2024.
Trustllm: Trustworthiness in large language models. arXiv preprint arXiv:2401.05561 (2024).
[321] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 2024. A Simple and Effective Pruning Approach for Large Language Models. In Proceedings
of the Twelfth International Conference on Learning Representations, ICLR .
[322] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2020. Mobilebert: a compact task-agnostic bert for
resource-limited devices. arXiv preprint arXiv:2004.02984 (2020).
[323] Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A Smith, and Yejin Choi. 2020. Dataset
Cartography: Mapping and Diagnosing Datasets with Training Dynamics. In Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) . 9275‚Äì9293.
Language Processing (EMNLP) . 9275‚Äì9293.
[324] Alon Talmor and Jonathan Berant. 2018. The Web as a Knowledge-Base for Answering Complex Questions. InProceedings of the 2018 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , Marilyn Walker, Heng
Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, New Orleans, Louisiana, 641‚Äì651. https://doi.org/10.18653/v1/N18-1059
[325] Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, and Ji-Rong Wen. 2024. Small Models, Big Insights: Leveraging Slim Proxy Models
To Decide When and What to Retrieve for LLMs. arXiv preprint arXiv:2402.12052 (2024).
[326] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. 2023. Toolalpaca: Generalized tool learning for language
models with 3000 simulated cases. arXiv preprint arXiv:2306.05301 (2023).
[327] Xuemei Tang, Jun Wang, and Qi Su. 2024. Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction.
arXiv preprint arXiv:2402.14373 (2024).
[328] Yehui Tang, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai, Yi-Qi Hu, Sichao Liu, Shangling Jui, Kai Han, and Yunhe Wang. 2024.
Rethinking optimization and architecture for tiny language models. arXiv preprint arXiv:2402.02791 (2024).
[329] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca.
[330] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert
Stojnic. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085 (2022).
[331] CodeGemma Team. 2024. Codegemma: Open code models based on gemma. arXiv preprint arXiv:2406.11409 (2024).
[332] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi√®re, Mihir Sanjay
Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295 (2024).
[333] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L√©onard Hussenot, Thomas Mesnard,
Bobak Shahriari, Alexandre Ram√©, et al. 2024. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118 (2024).
[334] TensorOpera Team. 2024. TensorOpera Unveils Fox Foundation Model: A Pioneering Small Language Model (SLM) for Cloud and Edge . https:
//blog.tensoropera.ai/tensoropera-unveils-fox-foundation-model-a-pioneering-open-source-slm-leading-the-way-against-tech-giants/ Accessed:
2024-6-13.
[335] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. 2016. Branchynet: Fast inference via early exiting from deep neural networks.
In 2016 23rd international conference on pattern recognition (ICPR) . IEEE, 2464‚Äì2469.
[336] Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M Anwer, Michael Felsberg, Tim Baldwin, Eric P Xing, and Fahad Shahbaz
Khan. 2024. Mobillama: Towards accurate and lightweight fully transparent gpt. arXiv preprint arXiv:2402.16840 (2024).
[337] Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. 2024. Toward Self-Improvement of LLMs via Imagination,
Searching, and Criticizing. arXiv preprint arXivko2024distillm:2404.12253 (2024).
[338] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric
Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).
[339] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).
[340] Jonathan Tow, Marco Bellagente, Dakota Mahan, and Carlos Riquelme. [n. d.]. StableLM 3B 4E1T. [https://huggingface.co/stabilityai/stablelm-3b-
4e1t](https://huggingface.co/stabilityai/stablelm-3b-4e1t)
[341] Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847 (2018).
[342] Adina Trufinescu. 2024. Discover the New Multi-Lingual High-Quality Phi-3.5 SLMs. https://techcommunity.microsoft.com/t5/ai-azure-ai-services-
blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/4225280.
[343] Dennis Ulmer, Martin Gubri, Hwaran Lee, Sangdoo Yun, and Seong Joon Oh. 2024. Calibrating Large Language Models Using Their Generations
Only. arXiv preprint arXiv:2403.05973 (2024).
Only. arXiv preprint arXiv:2403.05973 (2024).
[344] Sander Van Der Linden. 2022. Misinformation: susceptibility, spread, and interventions to immunize the public. Nature medicine 28, 3 (2022),
460‚Äì467.
[345] Chien Van Nguyen, Xuan Shen, Ryan Aponte, Yu Xia, Samyadeep Basu, Zhengmian Hu, Jian Chen, Mihir Parmar, Sasidhar Kunapuli, Joe Barrow,
et al. 2024. A Survey of Small Language Models. arXiv preprint arXiv:2410.20011 (2024).
[346] A Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems (2017).
Manuscript submitted to ACM
74 Fali Wang, et al.
[347] Olga Veksler. 2023. Test time adaptation with regularized loss for weakly supervised salient object detection. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition . 7360‚Äì7369.
[348] Paul Voigt and Axel Von dem Bussche. 2017. The eu general data protection regulation (gdpr).A Practical Guide, 1st Ed., Cham: Springer International
Publishing 10, 3152676 (2017), 10‚Äì5555.
Publishing 10, 3152676 (2017), 10‚Äì5555.
[349] Yuxian Wan, Wenlin Zhang, and Zhen Li. 2023. Multi-Task Feature Self-Distillation for Semi-Supervised Machine Translation. InInternational
Conference on Neural Information Processing . Springer, 238‚Äì254.
[350] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis
Platform for Natural Language Understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural
Networks for NLP . 353‚Äì355.
[351] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T.
Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, and Bo Li. 2023. DecodingTrust: A
Comprehensive Assessment of Trustworthiness in GPT Models. In Proceedings of the Annual Conference on Neural Information Processing Systems .
[352] Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li. 2021. Adversarial glue: A
multi-task benchmark for robustness evaluation of language models. arXiv preprint arXiv:2111.02840 (2021).
[353] Fali Wang, Runxue Bao, Suhang Wang, Wenchao Yu, Yanchi Liu, Wei Cheng, and Haifeng Chen. 2024. InfuserKI: Enhancing Large Language
Models with Knowledge Graphs via Infuser-Guided Knowledge Integration. arXiv preprint arXiv:2402.11441 (2024).
[354] Fali Wang, Zheng Lin, Zhengxiao Liu, Mingyu Zheng, Lei Wang, and Daren Zha. 2021. Macrobert: Maximizing certified region of bert to adversarial
word substitutions. In Database Systems for Advanced Applications: 26th International Conference, DASFAA 2021, Taipei, Taiwan, April 11‚Äì14, 2021,
Proceedings, Part II 26 . Springer, 253‚Äì261.
[355] Guan Wang, Sijie Cheng, Qiying Yu, and Changling Liu. 2023.OpenLLMs: Less is More for Open-source Models . https://doi.org/10.5281/zenodo.8105775
[356] Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2024. OpenChat: Advancing Open-source Language Models with
Mixed-Quality Data. In The Twelfth International Conference on Learning Representations . https://openreview.net/forum?id=AOJyfhWYHf
[357] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. 2023. Bitnet:
Scaling 1-bit transformers for large language models. arXiv preprint arXiv:2310.11453 (2023).
[358] Jindong Wang, HU Xixu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Wei Ye, Haojun Huang, Xiubo Geng, et al. [n. d.]. On
the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. In ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale
Machine Learning Models .
[359] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Improving Text Embeddings with Large Language
Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for
Computational Linguistics, 11897‚Äì11916. https://doi.org/10.18653/v1/2024.acl-long.642
[360] Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. 2023. SCOTT: Self-Consistent Chain-of-Thought Distillation. In
Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . 5546‚Äì5558.
[361] Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye Zhang, Binbin Lin, Deng Cai, and Xiaofei He. 2024. Model compression
and efficient inference for large language models: A survey. arXiv preprint arXiv:2402.09748 (2024).
[362] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. MiniLM: Deep Self-Attention Distillation for Task-Agnostic
Compression of Pre-Trained Transformers. arXiv:2002.10957 [cs.CL] https://arxiv.org/abs/2002.10957
[363] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang.
2024. SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. In Forty-first International Conference on
Machine Learning . https://openreview.net/forum?id=bq1JEgioLr
[364] Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. 2024. Do-Not-Answer: Evaluating Safeguards in LLMs. InFindings of the
Association for Computational Linguistics: EACL 2024 . Association for Computational Linguistics, 896‚Äì911. https://aclanthology.org/2024.findings-
eacl.61
eacl.61
[365] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023. Self-Knowledge Guided Retrieval Augmentation for Large Language Models. In Findings of
the Association for Computational Linguistics: EMNLP 2023 . 10303‚Äì10315.
[366] Yubo Wang, Xueguang Ma, and Wenhu Chen. 2023. Augmenting black-box llms with medical textbooks for clinical question answering. arXiv
preprint arXiv:2309.02233 (2023).
preprint arXiv:2309.02233 (2023).
[367] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran,
Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby
Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha
Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta
Patro, Tanay Dixit, and Xudong Shen. 2022. Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. In
Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 5085‚Äì5109.
https://doi.org/10.18653/v1/2022.emnlp-main.340
https://doi.org/10.18653/v1/2022.emnlp-main.340
[368] Yuyan Wang, Mohit Sharma, Can Xu, Sriraj Badam, Qian Sun, Lee Richardson, Lisa Chung, Ed H. Chi, and Minmin Chen. 2022. Surrogate for
Long-Term User Experience in Recommender Systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining. Association for Computing Machinery, 4100‚Äì4109. https://doi.org/10.1145/3534678.3539073
Manuscript submitted to ACM
A Survey of Small Language Models 75
[369] Yuling Wang, Changxin Tian, Binbin Hu, Yanhua Yu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Liang Pang, and Xiao Wang. 2024. Can Small Language
Models be Good Reasoners for Sequential Recommendation?. In Proceedings of the ACM on Web Conference 2024 . 3876‚Äì3887.
[370] Yuqing Wang and Yun Zhao. 2024. RUPBench: Benchmarking Reasoning Under Perturbations for Robustness Evaluation in Large Language Models.
arXiv preprint arXiv:2406.11020 (2024).
arXiv preprint arXiv:2406.11020 (2024).
[371] Zhepeng Wang, Runxue Bao, Yawen Wu, Jackson Taylor, Cao Xiao, Feng Zheng, Weiwen Jiang, Shangqian Gao, and Yanfu Zhang. 2024. Unlocking
Memorization in Large Language Models with Dynamic Soft Prompting. In Proceedings of the 2024 Conference on Empirical Methods in Natural
Language Processing , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA,
9782‚Äì9796. https://doi.org/10.18653/v1/2024.emnlp-main.546
[372] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022. Finetuned
Language Models are Zero-Shot Learners. In International Conference on Learning Representations . https://openreview.net/forum?id=gEZrGCozdqR
[373] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. Magicoder: Source code is all you need.arXiv preprint arXiv:2312.02120
(2023).
(2023).
[374] Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben
Coppin, and Po-Sen Huang. 2021. Challenges in Detoxifying Language Models. In Findings of the Association for Computational Linguistics: EMNLP
2021. Association for Computational Linguistics, 2447‚Äì2469. https://doi.org/10.18653/v1/2021.findings-emnlp.210
[375] Johannes Welbl, Nelson F Liu, and Matt Gardner. 2017. Crowdsourcing Multiple Choice Science Questions. In Proceedings of the 3rd Workshop on
Noisy User-generated Text . 94‚Äì106.
[376] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. 2024.
AutoDroid: LLM-powered Task Automation in Android. arXiv:2308.15272 [cs.AI] https://arxiv.org/abs/2308.15272
[377] Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. 2024. QuRating: Selecting High-Quality Data for Training Language Models. In
Forty-first International Conference on Machine Learning . https://openreview.net/forum?id=GLGYYqPwjy
[378] Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference.
In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long Papers) . Association for Computational Linguistics, New Orleans, Louisiana, 1112‚Äì1122. https://doi.org/10.18653/v1/N18-1101
[379] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Empowering news recommendation with pre-trained language models. In
Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval . 1652‚Äì1656.
[380] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. 2024. LaMini-LM: A Diverse Herd of Distilled
Models from Large-Scale Instructions. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics
(Volume 1: Long Papers) , Yvette Graham and Matthew Purver (Eds.). Association for Computational Linguistics, St. Julian‚Äôs, Malta, 944‚Äì964.
https://aclanthology.org/2024.eacl-long.57
[381] Taiqiang Wu, Cheng Hou, Shanshan Lao, Jiayi Li, Ngai Wong, Zhe Zhao, and Yujiu Yang. 2024. Weight-Inherited Distillation for Task-Agnostic
BERT Compression. In Findings of the Association for Computational Linguistics: NAACL 2024 , Kevin Duh, Helena Gomez, and Steven Bethard (Eds.).
Association for Computational Linguistics, Mexico City, Mexico, 13‚Äì28. https://doi.org/10.18653/v1/2024.findings-naacl.2
[382] Xuansheng Wu, Huachi Zhou, Yucheng Shi, Wenlin Yao, Xiao Huang, and Ninghao Liu. 2024. Could Small Language Models Serve as Recommenders?
Towards Data-centric Cold-start Recommendation. In Proceedings of the ACM on Web Conference 2024 . 3566‚Äì3575.
[383] Zhuofeng Wu, He Bai, Aonan Zhang, Jiatao Gu, VG Vydiswaran, Navdeep Jaitly, and Yizhe Zhang. 2024. Divide-or-Conquer? Which Part Should
You Distill Your LLM? arXiv preprint arXiv:2402.15000 (2024).
[384] Nuwa Xi, Yuhan Chen, Sendong Zhao, Haochun Wang, Bing Qin, and Ting Liu. 2024. AS-ES Learning: Towards Efficient CoT Learning in Small
Models. arXiv preprint arXiv:2403.01969 (2024).
Models. arXiv preprint arXiv:2403.01969 (2024).
[385] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2024. Sheared LLaMA: Accelerating Language Model Pre-training via Structured
Pruning. In The Twelfth International Conference on Learning Representations . https://openreview.net/forum?id=09iOdaeOzp
[386] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. Smoothquant: Accurate and efficient post-training
quantization for large language models. In International Conference on Machine Learning . PMLR, 38087‚Äì38099.
[387] Tinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang, Udari Madhushani Sehwag, Kaixuan Huang, Luxi He, Boyi Wei, Dacheng Li, Ying Sheng, et al.
2024. Sorry-bench: Systematically evaluating large language model safety refusal behaviors. arXiv preprint arXiv:2406.14598 (2024).
[388] Tong Xie, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang, Qingyuan Linghu, Chunyu Kit, Clara Grazian, Wenjie Zhang, et al.
2023. Darwin series: Domain specific large language models for natural science. arXiv preprint arXiv:2308.13565 (2023).
[389] Weikai Xie, Li Zhang, Shihe Wang, Rongjie Yi, and Mengwei Xu. 2024. DroidCall: A Dataset for LLM-powered Android Intent Invocation. arXiv
preprint arXiv:2412.00402 (2024).
preprint arXiv:2412.00402 (2024).
[390] Xuan Xie, Jiayang Song, Zhehua Zhou, Yuheng Huang, Da Song, and Lei Ma. 2024. Online Safety Analysis for LLMs: a Benchmark, an Assessment,
and a Path Forward. arXiv preprint arXiv:2404.08517 (2024).
[391] Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. 2021. BERxiT: Early Exiting for BERT with Better Fine-Tuning and Extension to Regression.
In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , Paola Merlo, Jorg
Tiedemann, and Reut Tsarfaty (Eds.). Association for Computational Linguistics, Online, 91‚Äì104. https://doi.org/10.18653/v1/2021.eacl-main.8
[392] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large
language models to follow complex instructions. arXiv preprint arXiv:2304.12244 (2023).
Manuscript submitted to ACM
76 Fali Wang, et al.
[393] Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu, and Julian McAuley. 2023. Small models are valuable plug-ins for large
language models. arXiv preprint arXiv:2305.08848 (2023).
[394] Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei, Mengwei Xu, and Xuanzhe Liu. 2023. LLMCad: Fast and Scalable On-device Large
Language Model Inference. arXiv:2309.04255 [cs.NI] https://arxiv.org/abs/2309.04255
[395] Daliang Xu, Hao Zhang, Liming Yang, Ruiqi Liu, Gang Huang, Mengwei Xu, and Xuanzhe Liu. 2024. Empowering 1000 tokens/second on-device
llm prefilling with mllm-npu. arXiv preprint arXiv:2407.05858 (2024).
[396] Jiaming Xu, Peng Wang, Guanhua Tian, Bo Xu, Jun Zhao, Fangyuan Wang, and Hongwei Hao. 2015. Short Text Clustering via Convolutional Neural
Networks. In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing , Phil Blunsom, Shay Cohen, Paramveer
Dhillon, and Percy Liang (Eds.). Association for Computational Linguistics, Denver, Colorado, 62‚Äì69. https://doi.org/10.3115/v1/W15-1509
[397] Minrui Xu, Niyato Dusit, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, Dong In Kim, and Khaled B Letaief. 2024. When large language model
agents meet 6G networks: Perception, grounding, and alignment. arXiv preprint arXiv:2401.07764 (2024).
[398] Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, et al. 2024. A
survey of resource-efficient llm and multimodal foundation models. arXiv preprint arXiv:2401.08092 (2024).
[399] Weijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna Martindale, and Marine Carpuat. 2023. Understanding and Detecting Hallucinations in
Neural Machine Translation via Model Introspection. Transactions of the Association for Computational Linguistics 11 (2023), 546‚Äì564.
[400] Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, and Wanxiang Che. 2024. OneBit: Towards Extremely
Low-bit Large Language Models. arXiv preprint arXiv:2402.11295 (2024).
[401] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective retrieval augmented generation. arXiv preprint arXiv:2401.15884 (2024).
[402] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024.
Qwen2 technical report. arXiv preprint arXiv:2407.10671 (2024).
[403] Chuanpeng Yang, Wang Lu, Yao Zhu, Yidong Wang, Qian Chen, Chenlong Gao, Bingjie Yan, and Yiqiang Chen. 2024. Survey on Knowledge
Distillation for Large Language Models: Methods, Evaluation, and Application. arXiv preprint arXiv:2407.01885 (2024).
[404] Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Yuanlin Duan, Wenqi Jia, Miao Yin, Yu Cheng, and Bo Yuan. 2024. MoE-I2: Compressing
Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition. InFindings of the Association for Computational
Linguistics: EMNLP 2024 , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida,
USA, 10456‚Äì10466. https://doi.org/10.18653/v1/2024.findings-emnlp.612
[405] Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, and Yuandong Tian. 2024. RLCD: Reinforcement Learning from Contrastive Distillation for
LM Alignment. In The Twelfth International Conference on Learning Representations . https://openreview.net/forum?id=v3XXtxWKi6
[406] Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. 2024. MentaLLaMA: interpretable mental health
analysis on social media with large language models. In Proceedings of the ACM on Web Conference 2024 . 4489‚Äì4500.
[407] Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang. 2023. GLUE-X: Evaluating
Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective. In Findings of the Association for Computational
Linguistics: ACL 2023 . Association for Computational Linguistics, Toronto, Canada, 12731‚Äì12750. https://doi.org/10.18653/v1/2023.findings-acl.806
[408] Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, and Yue
Zhang. 2024. Supervised Knowledge Makes Large Language Models Better In-context Learners. In The Twelfth International Conference on Learning
Representations. https://openreview.net/forum?id=bAMPOUF227
[409] Runming Yang, Taiqiang Wu, Jiahao Wang, Pengfei Hu, Ngai Wong, and Yujiu Yang. 2024. LLM-Neo: Parameter Efficient Knowledge Distillation
for Large Language Models. arXiv preprint arXiv:2411.06839 (2024).
[410] Yifei Yang, Zouying Cao, and Hai Zhao. 2024. Laco: Large language model pruning via layer collapse. arXiv preprint arXiv:2402.11187 (2024).
[411] Yu Yang, Siddhartha Mishra, Jeffrey N Chiang, and Baharan Mirzasoleiman. 2024. SmallToLarge (S2L): Scalable Data Selection for Fine-tuning
Large Language Models by Summarizing Training Trajectories of Small Models. In The Thirty-eighth Annual Conference on Neural Information
Processing Systems . https://openreview.net/forum?id=K9IGlMQpif
[412] Yizhe Yang, Huashan Sun, Jiawei Li, Runheng Liu, Yinghao Li, Yuhang Liu, Heyan Huang, and Yang Gao. 2023. Mindllm: Pre-training lightweight
large language model from scratch, evaluations and domain applications. arXiv preprint arXiv:2310.15777 (2023).
[413] Zhou Yang, Zhaochun Ren, Wang Yufeng, Shizhong Peng, Haizhou Sun, Xiaofei Zhu, and Xiangwen Liao. 2024. Enhancing Empathetic Response
Generation by Augmenting LLMs with Small-scale Empathetic Models. arXiv preprint arXiv:2402.11801 (2024).
[414] Yunzhi Yao, Shaohan Huang, Wenhui Wang, Li Dong, and Furu Wei. 2021. Adapt-and-distill: Developing small, fast and effective pretrained
language models for domains. arXiv preprint arXiv:2106.13474 (2021), 460‚Äì470.
[415] Mert Yazan, Suzan Verberne, and Frederik Situmeang. 2024. The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small
LLMs. arXiv preprint arXiv:2406.10251 (2024).
[416] Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, and Mengwei Xu. 2023. EdgeMoE: Fast On-Device Inference of MoE-based Large
Language Models. arXiv:2308.14352 [cs.LG] https://arxiv.org/abs/2308.14352
[417] Rongjie Yi, Xiang Li, Weikai Xie, Zhenyan Lu, Chenghua Wang, Ao Zhou, Shangguang Wang, Xiwen Zhang, and Mengwei Xu. 2024. PhoneLM: an
Efficient and Capable Small Language Model Family through Principled Pre-training. arXiv preprint arXiv:2411.05046 (2024).
[418] Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-Wei Chang, and Jina Suh. 2016. The Value of Semantic Parse Labeling for Knowledge Base
Question Answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , Katrin Erk
Manuscript submitted to ACM
A Survey of Small Language Models 77
and Noah A. Smith (Eds.). Association for Computational Linguistics, Berlin, Germany, 201‚Äì206. https://doi.org/10.18653/v1/P16-2033
[419] Wangsong Yin, Mengwei Xu, Yuanchun Li, and Xuanzhe Liu. 2024. LLM as a System Service on Mobile Devices. arXiv:2403.11805 [cs.OS]
https://arxiv.org/abs/2403.11805
[420] Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2024.
MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. In The Twelfth International Conference on Learning
Representations. https://openreview.net/forum?id=N8N0hgNDRt
[421] Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and Bryan Catanzaro. 2024. Rankrag: Unifying context
ranking with retrieval-augmented generation in llms. arXiv preprint arXiv:2407.02485 (2024).
[422] Zhongzhi Yu, Zheng Wang, Yuhan Li, Haoran You, Ruijie Gao, Xiaoya Zhou, Sreenidhi Reedy Bommu, Yang Katie Zhao, and Yingyan Celine Lin.
2024. EDGE-LLM: Enabling Efficient Large Language Model Adaptation on Edge Devices via Layerwise Unified Compression and Adaptive Layer
Tuning and Voting. arXiv preprint arXiv:2406.15758 (2024).
[423] Jinliang Yuan, Chen Yang, Dongqi Cai, Shihe Wang, Xin Yuan, Zeling Zhang, Xiang Li, Dingge Zhang, Hanzi Mei, Xianqing Jia, et al. 2024. Mobile
Foundation Model as Firmware. In Proceedings of the 30th Annual International Conference on Mobile Computing and Networking . 279‚Äì295.
[424] Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. 2021. Wudaocorpora: A super
large-scale chinese corpora for pre-training language models. AI Open 2 (2021), 65‚Äì68.
[425] Xiaohan Yuan, Jinfeng Li, Dongxia Wang, Yuefeng Chen, Xiaofeng Mao, Longtao Huang, Hui Xue, Wenhai Wang, Kui Ren, and Jingyi Wang. 2024.
S-Eval: Automatic and Adaptive Test Generation for Benchmarking Safety Evaluation of Large Language Models. arXiv preprint arXiv:2405.14191
(2024).
[426] Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. 2024. GPT-4 Is Too Smart To Be Safe:
Stealthy Chat with LLMs via Cipher. In The Twelfth International Conference on Learning Representations . https://openreview.net/forum?id=
MbfAK4s61A
MbfAK4s61A
[427] Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Wei Lin, et al . 2023.
Disc-lawllm: Fine-tuning large language models for intelligent legal services. arXiv preprint arXiv:2309.11325 (2023).
[428] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. [n. d.]. Mammoth: Building math generalist
models through hybrid instruction tuning, 2023. URL https://arxiv. org/abs/2309.05653 ([n. d.]).
[429] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a Machine Really Finish Your Sentence?. In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . 4791‚Äì4800.
[430] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake
news. Advances in neural information processing systems 32 (2019).
[431] Biao Zhang and Rico Sennrich. 2019. Root mean square layer normalization. Advances in Neural Information Processing Systems 32 (2019).
[432] Cheng Zhang, Jianyi Cheng, George A Constantinides, and Yiren Zhao. 2024. LQER: Low-Rank Quantization Error Reconstruction for LLMs. arXiv
preprint arXiv:2402.02446 (2024).
[433] Collin Zhang, John X Morris, and Vitaly Shmatikov. 2024. Extracting Prompts by Inverting LLM Outputs. arXiv preprint arXiv:2405.15012 (2024).
[434] Chen Zhang, Dawei Song, Zheyu Ye, and Yan Gao. 2023. Towards the law of capacity gap in distilling language models.arXiv preprint arXiv:2311.07052
(2023).
[435] Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024. Sciglm: Training
scientific language models with self-reflective instruction annotation and tuning. arXiv preprint arXiv:2401.07950 (2024).
[436] Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, et al. 2024. Chemllm:
A chemical large language model. arXiv preprint arXiv:2402.06852 (2024).
[437] Kaiyan Zhang, Jianyu Wang, Ermo Hua, Biqing Qi, Ning Ding, and Bowen Zhou. 2024. Cogenesis: A framework collaborating large and small
language models for secure context-aware instruction following. arXiv preprint arXiv:2403.03129 (2024).
[438] Mingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, and Bohan Zhuang. 2023. Loraprune: Pruning meets low-rank
parameter-efficient fine-tuning. arXiv preprint arXiv:2305.18403 (2023).
[439] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. TinyLlama: An Open-Source Small Language Model. arXiv:2401.02385 [cs.CL]
https://arxiv.org/abs/2401.02385
https://arxiv.org/abs/2401.02385
[440] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022).
[441] Xinran Zhang, Xin Yuan, Yunwei Li, and Yanru Zhang. 2019. Cold-Start representation learning: A recommendation approach with bert4Movie
and movie2Vec. In Proceedings of the 27th ACM International Conference on Multimedia . 2612‚Äì2616.
[442] Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, and Carlo Vittorio Cannistraci. 2024. Plug-and-play: An efficient post-training pruning
method for large language models. In The Twelfth International Conference on Learning Representations .
[443] Yiming Zhang, Nicholas Carlini, and Daphne Ippolito. 2024. Effective Prompt Extraction from Language Models. arXiv:2307.06865 [cs.CL]
https://arxiv.org/abs/2307.06865
[444] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R√©, Clark Barrett,
et al. 2024. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems
36 (2024).
Manuscript submitted to ACM
78 Fali Wang, et al.
[445] Zhiwei Zhang, Fali Wang, Xiaomin Li, Zongyu Wu, Xianfeng Tang, Hui Liu, Qi He, Wenpeng Yin, and Suhang Wang. 2024. Does your LLM truly
unlearn? An embarrassingly simple approach to recover unlearned knowledge. arXiv preprint arXiv:2410.16454 (2024).
[446] Bowen Zhao, Hannaneh Hajishirzi, and Qingqing Cao. 2024. APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training
and Inference. In Forty-first International Conference on Machine Learning .
[447] Junchen Zhao, Yurun Song, Simeng Liu, Ian G. Harris, and Sangeetha Abdu Jyothi. 2023. LinguaLinked: A Distributed Large Language Model
Inference System for Mobile Devices. arXiv:2312.00388 [cs.LG] https://arxiv.org/abs/2312.00388
[448] Juntao Zhao, Borui Wan, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware
Partition and Adaptive Quantization. arXiv preprint arXiv:2403.01136 (2024).
[449] Kun Zhao, Bohao Yang, Chen Tang, Chenghua Lin, and Liang Zhan. 2024. SLIDE: A Framework Integrating Small and Large Language Models for
Open-Domain Dialogues Evaluation. arXiv preprint arXiv:2405.15924 (2024).
[450] Theodore Zhao, Mu Wei, J Samuel Preston, and Hoifung Poon. 2023. Automatic calibration and error correction for large language models via
pareto optimal self-supervision. arXiv preprint arXiv:2306.16564 (2023).
[451] Youpeng Zhao, Ming Lin, Huadong Tang, Qiang Wu, and Jun Wang. 2024. Merino: Entropy-driven Design for Generative Language Models on IoT
Devices. arXiv:2403.07921 [cs.LG] https://arxiv.org/abs/2403.07921
[452] Zhengyun Zhao, Qiao Jin, Fangyuan Chen, Tuorui Peng, and Sheng Yu. 2022. Pmc-patients: A large-scale dataset of patient summaries and relations
for benchmarking retrieval-based clinical decision support systems. arXiv preprint arXiv:2202.13876 (2022).
[453] Zhanhui Zhou, Zhixuan Liu, Jie Liu, Zhichen Dong, Chao Yang, and Yu Qiao. 2024. Weak-to-Strong Search: Align Large Language Models via
Searching over Small Language Models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems . https://openreview.net/
forum?id=dOJ6CqWDf1
[454] Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. TAT-QA: A Question
Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance. InProceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) . 3277‚Äì3287.
[455] Jiachen Zhu, Jianghao Lin, Xinyi Dai, Bo Chen, Rong Shan, Jieming Zhu, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024. Lifelong Personalized
Low-Rank Adaptation of Large Language Models for Recommendation. arXiv preprint arXiv:2408.03533 (2024).
[456] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong, et al. 2023.
Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528 (2023).
[457] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023. A survey on model compression for large language models. arXiv preprint
arXiv:2308.07633 (2023).
[458] Yun Zhu, Yinxiao Liu, Felix Stahlberg, Shankar Kumar, Yu hui Chen, Liangchen Luo, Lei Shu, Renjie Liu, Jindong Chen, and Lei Meng. 2023.
Towards an On-device Agent for Text Rewriting. arXiv:2308.11807 [cs.CL] https://arxiv.org/abs/2308.11807
[459] Yuanyuan Zhuang and Jaekyeong Kim. 2021. A bert-based multi-criteria recommender system for hotel promotion management. Sustainability 13,
14 (2021), 8039.
[460] Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023. Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and
toxicity. arXiv preprint arXiv:2301.12867 (2023).
[461] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on
aligned language models. arXiv preprint arXiv:2307.15043 (2023).
[462] Lixin Zou, Weixue Lu, Yiding Liu, Hengyi Cai, Xiaokai Chu, Dehong Ma, Daiting Shi, Yu Sun, Zhicong Cheng, Simiu Gu, et al. 2022. Pre-trained
language model-based retrieval and ranking for web search. ACM Transactions on the Web 17, 1 (2022), 1‚Äì36.
[463] Jingwei Zuo, Maksim Velikanov, Dhia Eddine Rhaiem, Ilyas Chahed, Younes Belkada, Guillaume Kunsch, and Hakim Hacid. 2024. Falcon mamba:
The first competitive attention-free 7b language model. arXiv preprint arXiv:2410.05355 (2024).
Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009
Manuscript submitted to ACM
